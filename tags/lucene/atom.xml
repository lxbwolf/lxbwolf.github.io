<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>lucene on Xiaobin&#39;s Notes</title>
        <link>https://lxb.wiki/tags/lucene/</link>
        <description>Recent content in lucene on Xiaobin&#39;s Notes</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sun, 03 Sep 2023 22:05:28 +0800</lastBuildDate><atom:link href="https://lxb.wiki/tags/lucene/atom.xml" rel="self" type="application/rss+xml" /><item>
        <title>Lucene 三: Lucene 的索引文件格式</title>
        <link>https://lxb.wiki/552b0416/</link>
        <pubDate>Sun, 03 Sep 2023 22:05:28 +0800</pubDate>
        
        <guid>https://lxb.wiki/552b0416/</guid>
        <description>&lt;p&gt;Lucene的索引里面存了些什么，如何存放的，也即Lucene的索引文件格式，是读懂Lucene源代码的一把钥匙。&lt;/p&gt;
&lt;p&gt;当我们真正进入到Lucene源代码之中的时候，我们会发现:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lucene的索引过程，就是按照全文检索的基本过程，将倒排表写成此文件格式的过程。&lt;/li&gt;
&lt;li&gt;Lucene的搜索过程，就是按照此文件格式将索引进去的信息读出来，然后计算每篇文档打分(score)的过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考官网 &lt;a class=&#34;link&#34; href=&#34;http://lucene.apache.org/java/2_9_0/fileformats.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Apache Lucene - Index File Formats&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;一基本概念&#34;&gt;&lt;strong&gt;一、基本概念&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;下图就是Lucene生成的索引的一个实例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402262111476.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Lucene的索引结构是有层次结构的，主要分以下几个层次：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引(Index)：
&lt;ul&gt;
&lt;li&gt;在Lucene中一个索引是放在一个文件夹中的。&lt;/li&gt;
&lt;li&gt;如上图，同一文件夹中的所有的文件构成一个Lucene索引。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;段(Segment)：
&lt;ul&gt;
&lt;li&gt;一个索引可以包含多个段，段与段之间是独立的，添加新文档可以生成新的段，不同的段可以合并。&lt;/li&gt;
&lt;li&gt;如上图，具有相同前缀文件的属同一个段，图中共两个段 &amp;ldquo;_0&amp;rdquo; 和 &amp;ldquo;_1&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;segments.gen和segments_5是段的元数据文件，也即它们保存了段的属性信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;文档(Document)：
&lt;ul&gt;
&lt;li&gt;文档是我们建索引的基本单位，不同的文档是保存在不同的段中的，一个段可以包含多篇文档。&lt;/li&gt;
&lt;li&gt;新添加的文档是单独保存在一个新生成的段中，随着段的合并，不同的文档合并到同一个段中。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;域(Field)：
&lt;ul&gt;
&lt;li&gt;一篇文档包含不同类型的信息，可以分开索引，比如标题，时间，正文，作者等，都可以保存在不同的域里。&lt;/li&gt;
&lt;li&gt;不同域的索引方式可以不同，在真正解析域的存储的时候，我们会详细解读。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;词(Term)：
&lt;ul&gt;
&lt;li&gt;词是索引的最小单位，是经过词法分析和语言处理后的字符串。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lucene的索引结构中，即保存了正向信息，也保存了反向信息。&lt;/p&gt;
&lt;p&gt;所谓正向信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;按层次保存了从索引，一直到词的包含关系：索引(Index) –&amp;gt; 段(segment) –&amp;gt; 文档(Document) –&amp;gt; 域(Field) –&amp;gt; 词(Term)&lt;/li&gt;
&lt;li&gt;也即此索引包含了那些段，每个段包含了那些文档，每个文档包含了那些域，每个域包含了那些词。&lt;/li&gt;
&lt;li&gt;既然是层次结构，则每个层次都保存了本层次的信息以及下一层次的元信息，也即属性信息，比如一本介绍中国地理的书，应该首先介绍中国地理的概况，以及中国包含多少个省，每个省介绍本省的基本概况及包含多少个市，每个市介绍本市的基本概况及包含多少个县，每个县具体介绍每个县的具体情况。&lt;/li&gt;
&lt;li&gt;如上图，包含正向信息的文件有：
&lt;ul&gt;
&lt;li&gt;segments_N保存了此索引包含多少个段，每个段包含多少篇文档。&lt;/li&gt;
&lt;li&gt;XXX.fnm保存了此段包含了多少个域，每个域的名称及索引方式。&lt;/li&gt;
&lt;li&gt;XXX.fdx，XXX.fdt保存了此段包含的所有文档，每篇文档包含了多少域，每个域保存了那些信息。&lt;/li&gt;
&lt;li&gt;XXX.tvx，XXX.tvd，XXX.tvf保存了此段包含多少文档，每篇文档包含了多少域，每个域包含了多少词，每个词的字符串，位置等信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所谓反向信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保存了词典到倒排表的映射：词(Term) –&amp;gt; 文档(Document)&lt;/li&gt;
&lt;li&gt;如上图，包含反向信息的文件有：
&lt;ul&gt;
&lt;li&gt;XXX.tis，XXX.tii保存了词典(Term Dictionary)，也即此段包含的所有的词按字典顺序的排序。&lt;/li&gt;
&lt;li&gt;XXX.frq保存了倒排表，也即包含每个词的文档ID列表。&lt;/li&gt;
&lt;li&gt;XXX.prx保存了倒排表中每个词在包含此词的文档中的位置。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在了解Lucene索引的详细结构之前，先看看Lucene索引中的基本数据类型。&lt;/p&gt;
&lt;h2 id=&#34;二基本类型&#34;&gt;&lt;strong&gt;二、基本类型&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Lucene索引文件中，用以下基本类型来保存信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Byte：是最基本的类型，长8位(bit)。&lt;/li&gt;
&lt;li&gt;UInt32：由4个Byte组成。&lt;/li&gt;
&lt;li&gt;UInt64：由8个Byte组成。&lt;/li&gt;
&lt;li&gt;VInt：
&lt;ul&gt;
&lt;li&gt;变长的整数类型，它可能包含多个Byte，对于每个Byte的8位，其中后7位表示数值，最高1位表示是否还有另一个Byte，0表示没有，1表示有。&lt;/li&gt;
&lt;li&gt;越前面的Byte表示数值的低位，越后面的Byte表示数值的高位。&lt;/li&gt;
&lt;li&gt;例如130化为二进制为 1000, 0010，总共需要8位，一个Byte表示不了，因而需要两个Byte来表示，第一个Byte表示后7位，并且在最高位置1来表示后面还有一个Byte，所以为(1) 0000010，第二个Byte表示第8位，并且最高位置0来表示后面没有其他的Byte了，所以为(0) 0000001。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402262118850.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chars：是UTF-8编码的一系列Byte。&lt;/li&gt;
&lt;li&gt;String：一个字符串首先是一个VInt来表示此字符串包含的字符的个数，接着便是UTF-8编码的字符序列Chars。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三基本规则&#34;&gt;&lt;strong&gt;三、基本规则&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Lucene为了使的信息的存储占用的空间更小，访问速度更快，采取了一些特殊的技巧，然而在看Lucene文件格式的时候，这些技巧却容易使我们感到困惑，所以有必要把这些特殊的技巧规则提取出来介绍一下。&lt;/p&gt;
&lt;h3 id=&#34;1-前缀后缀规则prefixsuffix&#34;&gt;&lt;strong&gt;1. 前缀后缀规则(Prefix+Suffix)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Lucene在反向索引中，要保存词典(Term Dictionary)的信息，所有的词(Term)在词典中是按照字典顺序进行排列的，然而词典中包含了文档中的几乎所有的词，并且有的词还是非常的长的，这样索引文件会非常的大，所谓前缀后缀规则，即当某个词和前一个词有共同的前缀的时候，后面的词仅仅保存前缀在词中的偏移(offset)，以及除前缀以外的字符串(称为后缀)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402262121854.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;比如要存储如下词:term，termagancy，termagant，terminal，&lt;/p&gt;
&lt;p&gt;如果按照正常方式来存储，需要的空间如下：&lt;/p&gt;
&lt;p&gt;[VInt = 4] [t][e][r][m]，&lt;/p&gt;
&lt;p&gt;[VInt = 10][t][e][r][m][a][g][a][n][c][y]，&lt;/p&gt;
&lt;p&gt;[VInt = 9][t][e][r][m][a][g][a][n][t]，&lt;/p&gt;
&lt;p&gt;[VInt = 8][t][e][r][m][i][n][a][l]&lt;/p&gt;
&lt;p&gt;共需要35个Byte.&lt;/p&gt;
&lt;p&gt;如果应用前缀后缀规则，需要的空间如下：&lt;/p&gt;
&lt;p&gt;[VInt = 4] [t][e][r][m]，&lt;/p&gt;
&lt;p&gt;[VInt = 4 (offset)][VInt = 6][a][g][a][n][c][y]，&lt;/p&gt;
&lt;p&gt;[VInt = 8 (offset)][VInt = 1][t]，&lt;/p&gt;
&lt;p&gt;[VInt = 4 (offset)][VInt = 4][i][n][a][l]&lt;/p&gt;
&lt;p&gt;共需要22个Byte。&lt;/p&gt;
&lt;p&gt;大大缩小了存储空间，尤其是在按字典顺序排序的情况下，前缀的重合率大大提高。&lt;/p&gt;
&lt;h3 id=&#34;2-差值规则delta&#34;&gt;&lt;strong&gt;2. 差值规则(Delta)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在Lucene的反向索引中，需要保存很多整型数字的信息，比如文档ID号，比如词(Term)在文档中的位置等等。&lt;/p&gt;
&lt;p&gt;由上面介绍，我们知道，整型数字是以VInt的格式存储的。随着数值的增大，每个数字占用的Byte的个数也逐渐的增多。所谓差值规则(Delta)就是先后保存两个整数的时候，后面的整数仅仅保存和前面整数的差即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402262129662.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;比如要存储如下整数：16386，16387，16388，16389&lt;/p&gt;
&lt;p&gt;如果按照正常方式来存储，需要的空间如下：&lt;/p&gt;
&lt;p&gt;[(1) 000, 0010][(1) 000, 0000][(0) 000, 0001]，&lt;/p&gt;
&lt;p&gt;[(1) 000, 0011][(1) 000, 0000][(0) 000, 0001]，&lt;/p&gt;
&lt;p&gt;[(1) 000, 0100][(1) 000, 0000][(0) 000, 0001]，&lt;/p&gt;
&lt;p&gt;[(1) 000, 0101][(1) 000, 0000][(0) 000, 0001]&lt;/p&gt;
&lt;p&gt;供需12个Byte。&lt;/p&gt;
&lt;p&gt;如果应用差值规则来存储，需要的空间如下：&lt;/p&gt;
&lt;p&gt;[(1) 000, 0010][(1) 000, 0000][(0) 000, 0001]，&lt;/p&gt;
&lt;p&gt;[(0) 000, 0001]，&lt;/p&gt;
&lt;p&gt;[(0) 000, 0001]，&lt;/p&gt;
&lt;p&gt;[(0) 000, 0001]&lt;/p&gt;
&lt;p&gt;共需6个Byte。&lt;/p&gt;
&lt;p&gt;大大缩小了存储空间，而且无论是文档ID，还是词在文档中的位置，都是按从小到大的顺序，逐渐增大的。&lt;/p&gt;
&lt;h3 id=&#34;3-或然跟随规则a-b&#34;&gt;&lt;strong&gt;3. 或然跟随规则(A, B?)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Lucene的索引结构中存在这样的情况，某个值A后面可能存在某个值B，也可能不存在，需要一个标志来表示后面是否跟随着B。&lt;/p&gt;
&lt;p&gt;一般的情况下，在A后面放置一个Byte，为0则后面不存在B，为1则后面存在B，或者0则后面存在B，1则后面不存在B。&lt;/p&gt;
&lt;p&gt;但这样要浪费一个Byte的空间，其实一个Bit就可以了。&lt;/p&gt;
&lt;p&gt;在Lucene中，采取以下的方式：A的值左移一位，空出最后一位，作为标志位，来表示后面是否跟随B，所以在这种情况下，A/2是真正的A原来的值。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402262134351.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如果去读Apache Lucene - Index File Formats这篇文章，会发现很多符合这种规则的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.frq文件中的DocDelta[, Freq?]，DocSkip,PayloadLength?&lt;/li&gt;
&lt;li&gt;.prx文件中的PositionDelta,Payload? (但不完全是，如下表分析)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然还有一些带?的但不属于此规则的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.frq文件中的SkipChildLevelPointer?，是多层跳跃表中，指向下一层表的指针，当然如果是最后一层，此值就不存在，也不需要标志。&lt;/li&gt;
&lt;li&gt;.tvf文件中的Positions?, Offsets?。
&lt;ul&gt;
&lt;li&gt;在此类情况下，带?的值是否存在，并不取决于前面的值的最后一位。&lt;/li&gt;
&lt;li&gt;而是取决于Lucene的某项配置，当然这些配置也是保存在Lucene索引文件中的。&lt;/li&gt;
&lt;li&gt;如Position和Offset是否存储，取决于.fnm文件中对于每个域的配置(TermVector.WITH_POSITIONS和TermVector.WITH_OFFSETS)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为什么会存在以上两种情况，其实是可以理解的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于符合或然跟随规则的，是因为对于每一个A，B是否存在都不相同，当这种情况大量存在的时候，从一个Byte到一个Bit如此8倍的空间节约还是很值得的。&lt;/li&gt;
&lt;li&gt;对于不符合或然跟随规则的，是因为某个值的是否存在的配置对于整个域(Field)甚至整个索引都是有效的，而非每次的情况都不相同，因而可以统一存放一个标志。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;文章中对如下格式的描述令人困惑：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Positions &amp;ndash;&amp;gt; &amp;lt;PositionDelta,Payload?&amp;gt; &lt;!-- raw HTML omitted --&gt;Freq&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Payload &amp;ndash;&amp;gt; &amp;lt;PayloadLength?,PayloadData&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;PositionDelta和Payload是否适用或然跟随规则呢？如何标识PayloadLength是否存在呢？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;其实PositionDelta和Payload并不符合或然跟随规则，Payload是否存在，是由.fnm文件中对于每个域的配置中有关Payload的配置决定的(FieldOption.STORES_PAYLOADS) 。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当Payload不存在时，PayloadDelta本身不遵从或然跟随原则。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当Payload存在时，格式应该变成如下：Positions &amp;ndash;&amp;gt; &amp;lt;PositionDelta,PayloadLength?,PayloadData&amp;gt; &lt;!-- raw HTML omitted --&gt;Freq&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;从而PositionDelta和PayloadLength一起适用或然跟随规则。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;4-跳跃表规则skip-list&#34;&gt;&lt;strong&gt;4. 跳跃表规则(Skip list)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;为了提高查找的性能，Lucene在很多地方采取的跳跃表的数据结构。&lt;/p&gt;
&lt;p&gt;跳跃表(Skip List)是如图的一种数据结构，有以下几个基本特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;元素是按顺序排列的，在Lucene中，或是按字典顺序排列，或是按从小到大顺序排列。&lt;/li&gt;
&lt;li&gt;跳跃是有间隔的(Interval)，也即每次跳跃的元素数，间隔是事先配置好的，如图跳跃表的间隔为3。&lt;/li&gt;
&lt;li&gt;跳跃表是由层次的(level)，每一层的每隔指定间隔的元素构成上一层，如图跳跃表共有2层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402262141098.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;需要注意一点的是，在很多数据结构或算法书中都会有跳跃表的描述，原理都是大致相同的，但是定义稍有差别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对间隔(Interval)的定义： 如图中，有的认为间隔为2，即两个上层元素之间的元素数，不包括两个上层元素；有的认为是3，即两个上层元素之间的差，包括后面上层元素，不包括前面的上层元素；有的认为是4，即除两个上层元素之间的元素外，既包括前面，也包括后面的上层元素。Lucene是采取的第二种定义。&lt;/li&gt;
&lt;li&gt;对层次(Level)的定义：如图中，有的认为应该包括原链表层，并从1开始计数，则总层次为3，为1，2，3层；有的认为应该包括原链表层，并从0计数，为0，1，2层；有的认为不应该包括原链表层，且从1开始计数，则为1，2层；有的认为不应该包括链表层，且从0开始计数，则为0，1层。Lucene采取的是最后一种定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;跳跃表比顺序查找，大大提高了查找速度，如查找元素72，原来要访问2，3，7，12，23，37，39，44，50，72总共10个元素，应用跳跃表后，只要首先访问第1层的50，发现72大于50，而第1层无下一个节点，然后访问第2层的94，发现94大于72，然后访问原链表的72，找到元素，共需要访问3个元素即可。&lt;/p&gt;
&lt;p&gt;然而Lucene在具体实现上，与理论又有所不同，在具体的格式中，会详细说明。&lt;/p&gt;
&lt;h2 id=&#34;四具体格式&#34;&gt;&lt;strong&gt;四、具体格式&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;上面曾经交代过，Lucene保存了从Index到Segment到Document到Field一直到Term的正向信息，也包括了从Term到Document映射的反向信息，还有其他一些Lucene特有的信息。下面对这三种信息一一介绍。&lt;/p&gt;
&lt;h3 id=&#34;41-正向信息&#34;&gt;&lt;strong&gt;4.1. 正向信息&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Index –&amp;gt; Segments (segments.gen, segments_N) –&amp;gt; Field(fnm, fdx, fdt) –&amp;gt; Term (tvx, tvd, tvf)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;上面的层次结构不是十分的准确，因为segments.gen和segments_N保存的是段(segment)的元数据信息(metadata)，其实是每个Index一个的，而段的真正的数据信息，是保存在域(Field)和词(Term)中的。&lt;/p&gt;
&lt;h4 id=&#34;411-段的元数据信息segments_n&#34;&gt;&lt;strong&gt;4.1.1. 段的元数据信息(segments_N)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;一个索引(Index)可以同时存在多个segments_N(至于如何存在多个segments_N，在描述完详细信息之后会举例说明)，然而当我们要打开一个索引的时候，我们必须要选择一个来打开，那如何选择哪个segments_N呢？&lt;/p&gt;
&lt;p&gt;Lucene采取以下过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;其一，在所有的segments_N中选择N最大的一个。基本逻辑参照SegmentInfos.getCurrentSegmentGeneration(File[] files)，其基本思路就是在所有以segments开头，并且不是segments.gen的文件中，选择N最大的一个作为genA。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其二，打开segments.gen，其中保存了当前的N值。其格式如下，读出版本号(Version)，然后再读出两个N，如果两者相等，则作为genB。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402272228421.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;IndexInput genInput &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; directory&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;openInput&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;IndexFileNames&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;SEGMENTS_GEN&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//&amp;#34;segments.gen&amp;#34;  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; version &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genInput&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;readInt&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//读出版本号  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;version &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; FORMAT_LOCKLESS&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//如果版本号正确  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; gen0 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genInput&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;readLong&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//读出第一个N  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; gen1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genInput&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;readLong&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//读出第二个N  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;gen0 &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; gen1&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//如果两者相等则为genB  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        genB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gen0&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其三，在上述得到的genA和genB中选择最大的那个作为当前的N，方才打开segments_N文件。其基本逻辑如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;genA &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; genB&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    gen &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genA&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    gen &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; genB&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下图是segments_N的具体格式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402272230529.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Format：
&lt;ul&gt;
&lt;li&gt;索引文件格式的版本号。&lt;/li&gt;
&lt;li&gt;由于Lucene是在不断开发过程中的，因而不同版本的Lucene，其索引文件格式也不尽相同，于是规定一个版本号。&lt;/li&gt;
&lt;li&gt;Lucene 2.1此值-3，Lucene 2.9时，此值为-9。&lt;/li&gt;
&lt;li&gt;当用某个版本号的IndexReader读取另一个版本号生成的索引的时候，会因为此值不同而报错。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Version：
&lt;ul&gt;
&lt;li&gt;索引的版本号，记录了IndexWriter将修改提交到索引文件中的次数。&lt;/li&gt;
&lt;li&gt;其初始值大多数情况下从索引文件里面读出，仅仅在索引开始创建的时候，被赋予当前的时间，已取得一个唯一值。&lt;/li&gt;
&lt;li&gt;其值改变在 &lt;code&gt;IndexWriter.commit-&amp;gt;IndexWriter.startCommit-&amp;gt;SegmentInfos.prepareCommit-&amp;gt;SegmentInfos.write-&amp;gt;writeLong(++version)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;其初始值之所最初取一个时间，是因为我们并不关心IndexWriter将修改提交到索引的具体次数，而更关心到底哪个是最新的。IndexReader中常比较自己的version和索引文件中的version是否相同来判断此IndexReader被打开后，还有没有被IndexWriter更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//在DirectoryReader中有一下函数。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;boolean&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;isCurrent&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;throws&lt;/span&gt; CorruptIndexException&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; IOException &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; SegmentInfos&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;readCurrentVersion&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;directory&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; segmentInfos&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getVersion&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;NameCount
&lt;ul&gt;
&lt;li&gt;是下一个新段(Segment)的段名。&lt;/li&gt;
&lt;li&gt;所有属于同一个段的索引文件都以段名作为文件名，一般为_0.xxx, _0.yyy,  _1.xxx, _1.yyy ……&lt;/li&gt;
&lt;li&gt;新生成的段的段名一般为原有最大段名加一。&lt;/li&gt;
&lt;li&gt;如同的索引，NameCount读出来是2，说明新的段为_2.xxx, _2.yyy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402272232711.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SegCount
&lt;ul&gt;
&lt;li&gt;段(Segment)的个数。&lt;/li&gt;
&lt;li&gt;如上图，此值为2。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SegCount个段的元数据信息：
&lt;ul&gt;
&lt;li&gt;SegName
&lt;ul&gt;
&lt;li&gt;段名，所有属于同一个段的文件都有以段名作为文件名。&lt;/li&gt;
&lt;li&gt;如上图，第一个段的段名为&amp;quot;_0&amp;quot;，第二个段的段名为&amp;quot;_1&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SegSize
&lt;ul&gt;
&lt;li&gt;此段中包含的文档数&lt;/li&gt;
&lt;li&gt;然而此文档数是包括已经删除，又没有optimize的文档的，因为在optimize之前，Lucene的段中包含了所有被索引过的文档，而被删除的文档是保存在.del文件中的，在搜索的过程中，是先从段中读到了被删除的文档，然后再用.del中的标志，将这篇文档过滤掉。&lt;/li&gt;
&lt;li&gt;如下的代码形成了上图的索引，可以看出索引了两篇文档形成了_0段，然后又删除了其中一篇，形成了_0_1.del，又索引了两篇文档形成_1段，然后又删除了其中一篇，形成_1_1.del。因而在两个段中，此值都是2。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;IndexWriter writer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; IndexWriter&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;FSDirectory&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;open&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;INDEX&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;\&lt;/span&gt;_DIR&lt;span style=&#34;color:#f92672&#34;&gt;),&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; StandardAnalyzer&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Version&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;LUCENE&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;\&lt;/span&gt;_CURRENT&lt;span style=&#34;color:#f92672&#34;&gt;),&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; IndexWriter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;MaxFieldLength&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;LIMITED&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;setUseCompoundFile&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;indexDocs&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; docDir&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//docDir中只有两篇文档
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//文档一为：Students should be allowed to go out with their friends, but not allowed to drink beer.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//文档二为：My friend Jerry went to school to see his students but found them drunk which is not allowed.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;commit&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//提交两篇文档，形成\_0段。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;deleteDocuments&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Term&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;contents&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;school&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;));&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//删除文档二  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;commit&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//提交删除，形成\_0\_1.del  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;indexDocs&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; docDir&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//再次索引两篇文档，Lucene不能判别文档与文档的不同，因而算两篇新的文档。  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;commit&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//提交两篇文档，形成\_1段  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;deleteDocuments&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Term&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;contents&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;school&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;));&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//删除第二次添加的文档二  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;close&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//提交删除，形成\_1\_1.del
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;DelGen
&lt;ul&gt;
&lt;li&gt;.del文件的版本号&lt;/li&gt;
&lt;li&gt;Lucene中，在optimize之前，删除的文档是保存在.del文件中的。&lt;/li&gt;
&lt;li&gt;在Lucene 2.9中，文档删除有以下几种方式：
&lt;ul&gt;
&lt;li&gt;IndexReader.deleteDocument(int docID)是用IndexReader按文档号删除。&lt;/li&gt;
&lt;li&gt;IndexReader.deleteDocuments(Term term)是用IndexReader删除包含此词(Term)的文档。&lt;/li&gt;
&lt;li&gt;IndexWriter.deleteDocuments(Term term)是用IndexWriter删除包含此词(Term)的文档。&lt;/li&gt;
&lt;li&gt;IndexWriter.deleteDocuments(Term[] terms)是用IndexWriter删除包含这些词(Term)的文档。&lt;/li&gt;
&lt;li&gt;IndexWriter.deleteDocuments(Query query)是用IndexWriter删除能满足此查询(Query)的文档。&lt;/li&gt;
&lt;li&gt;IndexWriter.deleteDocuments(Query[] queries)是用IndexWriter删除能满足这些查询(Query)的文档。&lt;/li&gt;
&lt;li&gt;原来的版本中Lucene的删除一直是由IndexReader来完成的，在Lucene 2.9中虽可以用IndexWriter来删除，但是其实真正的实现是在IndexWriter中，保存了readerpool，当IndexWriter向索引文件提交删除的时候，仍然是从readerpool中得到相应的IndexReader，并用IndexReader来进行删除的。下面的代码可以说明：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; IndexWriter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;applyDeletes&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; DocumentsWriter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;applyDeletes&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;SegmentInfos&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	  &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; reader&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;deleteDocument&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;doc&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;DelGen是每当IndexWriter向索引文件中提交删除操作的时候，加1，并生成新的.del文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;DocStoreOffset&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DocStoreSegment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DocStoreIsCompoundFile&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于域(Stored Field)和词向量(Term Vector)的存储可以有不同的方式，即可以每个段(Segment)单独存储自己的域和词向量信息，也可以多个段共享域和词向量，把它们存储到一个段中去。&lt;/li&gt;
&lt;li&gt;如果DocStoreOffset为-1，则此段单独存储自己的域和词向量，从存储文件上来看，如果此段段名为XXX，则此段有自己的XXX.fdt，XXX.fdx，XXX.tvf，XXX.tvd，XXX.tvx文件。DocStoreSegment和DocStoreIsCompoundFile在此处不被保存。&lt;/li&gt;
&lt;li&gt;如果DocStoreOffset不为-1，则DocStoreSegment保存了共享的段的名字，比如为YYY，DocStoreOffset则为此段的域及词向量信息在共享段中的偏移量。则此段没有自己的XXX.fdt，XXX.fdx，XXX.tvf，XXX.tvd，XXX.tvx文件，而是将信息存放在共享段的YYY.fdt，YYY.fdx，YYY.tvf，YYY.tvd，YYY.tvx文件中。&lt;/li&gt;
&lt;li&gt;DocumentsWriter中有两个成员变量：String segment是当前索引信息存放的段，String docStoreSegment是域和词向量信息存储的段。两者可以相同也可以不同，决定了域和词向量信息是存储在本段中，还是和其他的段共享。&lt;/li&gt;
&lt;li&gt;IndexWriter.flush(boolean triggerMerge, boolean flushDocStores, boolean flushDeletes)中第二个参数flushDocStores会影响到是否单独或是共享存储。其实最终影响的是DocumentsWriter.closeDocStore()。每当flushDocStores为false时，closeDocStore不被调用，说明下次添加到索引文件中的域和词向量信息是同此次共享一个段的。直到flushDocStores为true的时候，closeDocStore被调用，从而下次添加到索引文件中的域和词向量信息将被保存在一个新的段中，不同此次共享一个段(在这里需要指出的是Lucene的一个很奇怪的实现，虽然下次域和词向量信息是被保存到新的段中，然而段名却是这次被确定了的，在initSegmentName中当docStoreSegment == null时，被置为当前的segment，而非下一个新的segment，docStoreSegment = segment，于是会出现如下面的例子的现象)。&lt;/li&gt;
&lt;li&gt;好在共享域和词向量存储并不是经常被使用到，实现也或有缺陷，暂且解释到此。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IndexWriter writer = new IndexWriter(FSDirectory.open(INDEX_DIR), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);&lt;br&gt;
writer.setUseCompoundFile(false);&lt;/p&gt;
&lt;p&gt;indexDocs(writer, docDir);&lt;br&gt;
writer.flush();&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;//flush生成segment &amp;ldquo;_0&amp;rdquo;，并且flush函数中，flushDocStores设为false，也即下个段将同本段共享域和词向量信息，这时DocumentsWriter中的docStoreSegment= &amp;ldquo;_0&amp;rdquo;。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  indexDocs(writer, docDir);  
  writer.commit();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;//commit生成segment &amp;ldquo;_1&amp;rdquo;，由于上次flushDocStores设为false，于是段&amp;quot;_1&amp;quot;的域以及词向量信息是保存在&amp;quot;_0&amp;quot;中的，在这个时刻，段&amp;quot;_1&amp;quot;并不生成自己的&amp;quot;_1.fdx&amp;quot;和&amp;quot;_1.fdt&amp;quot;。然而在commit函数中，flushDocStores设为true，也即下个段将单独使用新的段来存储域和词向量信息。然而这时，DocumentsWriter中的docStoreSegment= &amp;ldquo;_1&amp;rdquo;，也即当段&amp;quot;_2&amp;quot;存储其域和词向量信息的时候，是存在&amp;quot;_1.fdx&amp;quot;和&amp;quot;_1.fdt&amp;quot;中的，而段&amp;quot;_1&amp;quot;的域和词向量信息却是存在&amp;quot;_0.fdt&amp;quot;和&amp;quot;_0.fdx&amp;quot;中的，这一点非常令人困惑。 如图writer.commit的时候，_1.fdt和_1.fdx并没有形成。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022114233.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  indexDocs(writer, docDir);  
  writer.flush();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;//段&amp;quot;_2&amp;quot;形成，由于上次flushDocStores设为true，其域和词向量信息是新创建一个段保存的，却是保存在_1.fdt和_1.fdx中的，这时候才产生了此二文件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022115773.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  indexDocs(writer, docDir);  
  writer.flush();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;//段&amp;quot;_3&amp;quot;形成，由于上次flushDocStores设为false，其域和词向量信息是共享一个段保存的，也是是保存在_1.fdt和_1.fdx中的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  indexDocs(writer, docDir);  
  writer.commit();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;//段&amp;quot;_4&amp;quot;形成，由于上次flushDocStores设为false，其域和词向量信息是共享一个段保存的，也是是保存在_1.fdt和_1.fdx中的。然而函数commit中flushDocStores设为true，也意味着下一个段将新创建一个段保存域和词向量信息，此时DocumentsWriter中docStoreSegment= &amp;ldquo;_4&amp;rdquo;，也表明了虽然段&amp;quot;_4&amp;quot;的域和词向量信息保存在了段&amp;quot;_1&amp;quot;中，将来的域和词向量信息却要保存在段&amp;quot;_4&amp;quot;中。此时&amp;quot;_4.fdx&amp;quot;和&amp;quot;_4.fdt&amp;quot;尚未产生。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022115627.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  indexDocs(writer, docDir);  
  writer.flush();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;//段&amp;quot;_5&amp;quot;形成，由于上次flushDocStores设为true，其域和词向量信息是新创建一个段保存的，却是保存在_4.fdt和_4.fdx中的，这时候才产生了此二文件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022116165.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  indexDocs(writer, docDir);  
  writer.commit();  
  writer.close();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;//段&amp;quot;_6&amp;quot;形成，由于上次flushDocStores设为false，其域和词向量信息是共享一个段保存的，也是是保存在_4.fdt和_4.fdx中的&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022116777.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;HasSingleNormFile
&lt;ul&gt;
&lt;li&gt;在搜索的过程中，标准化因子(Normalization Factor)会影响文档最后的评分。&lt;/li&gt;
&lt;li&gt;不同的文档重要性不同，不同的域重要性也不同。因而每个文档的每个域都可以有自己的标准化因子。&lt;/li&gt;
&lt;li&gt;如果HasSingleNormFile为1，则所有的标准化因子都是存在.nrm文件中的。&lt;/li&gt;
&lt;li&gt;如果HasSingleNormFile不是1，则每个域都有自己的标准化因子文件.fN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NumField
&lt;ul&gt;
&lt;li&gt;域的数量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NormGen
&lt;ul&gt;
&lt;li&gt;如果每个域有自己的标准化因子文件，则此数组描述了每个标准化因子文件的版本号，也即.fN的N。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IsCompoundFile
&lt;ul&gt;
&lt;li&gt;是否保存为复合文件，也即把同一个段中的文件按照一定格式，保存在一个文件当中，这样可以减少每次打开文件的个数。&lt;/li&gt;
&lt;li&gt;是否为复合文件，由接口IndexWriter.setUseCompoundFile(boolean)设定。&lt;/li&gt;
&lt;li&gt;非符合文件同符合文件的对比如下图：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;DeletionCount
&lt;ul&gt;
&lt;li&gt;记录了此段中删除的文档的数目。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HasProx
&lt;ul&gt;
&lt;li&gt;如果至少有一个段omitTf为false，也即词频(term freqency)需要被保存，则HasProx为1，否则为0。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Diagnostics
&lt;ul&gt;
&lt;li&gt;调试信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;User map data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保存了用户从字符串到字符串的映射Map&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CheckSum&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;此文件segment_N的校验和。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;读取此文件格式参考SegmentInfos.read(Directory directory, String segmentFileName):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;int format = input.readInt();&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;version = input.readLong(); // read version&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;counter = input.readInt(); // read counter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for (int i = input.readInt(); i &amp;gt; 0; i&amp;ndash;) // read segmentInfos&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;add(new SegmentInfo(directory, format, input));
&lt;ul&gt;
&lt;li&gt;name = input.readString();&lt;/li&gt;
&lt;li&gt;docCount = input.readInt();&lt;/li&gt;
&lt;li&gt;delGen = input.readLong();&lt;/li&gt;
&lt;li&gt;docStoreOffset = input.readInt();&lt;/li&gt;
&lt;li&gt;docStoreSegment = input.readString();&lt;/li&gt;
&lt;li&gt;docStoreIsCompoundFile = (1 == input.readByte());&lt;/li&gt;
&lt;li&gt;hasSingleNormFile = (1 == input.readByte());&lt;/li&gt;
&lt;li&gt;int numNormGen = input.readInt();&lt;/li&gt;
&lt;li&gt;normGen = new long[numNormGen];&lt;/li&gt;
&lt;li&gt;for(int j=0;j&lt;/li&gt;
&lt;li&gt;normGen[j] = input.readLong();&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;isCompoundFile = input.readByte();&lt;/li&gt;
&lt;li&gt;delCount = input.readInt();&lt;/li&gt;
&lt;li&gt;hasProx = input.readByte() == 1;&lt;/li&gt;
&lt;li&gt;diagnostics = input.readStringStringMap();&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;userData = input.readStringStringMap();&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;final long checksumNow = input.getChecksum();&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;final long checksumThen = input.readLong();&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;412-域field的元数据信息fnm&#34;&gt;&lt;strong&gt;4.1.2. 域(Field)的元数据信息(.fnm)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;一个段(Segment)包含多个域，每个域都有一些元数据信息，保存在.fnm文件中，.fnm文件的格式如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022117877.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FNMVersion
&lt;ul&gt;
&lt;li&gt;是fnm文件的版本号，对于Lucene 2.9为-2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FieldsCount
&lt;ul&gt;
&lt;li&gt;域的数目&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;一个数组的域(Fields)
&lt;ul&gt;
&lt;li&gt;FieldName：域名，如&amp;quot;title&amp;quot;，&amp;ldquo;modified&amp;rdquo;，&amp;ldquo;content&amp;quot;等。&lt;/li&gt;
&lt;li&gt;FieldBits:一系列标志位，表明对此域的索引方式
&lt;ul&gt;
&lt;li&gt;最低位：1表示此域被索引，0则不被索引。所谓被索引，也即放到倒排表中去。
&lt;ul&gt;
&lt;li&gt;仅仅被索引的域才能够被搜到。&lt;/li&gt;
&lt;li&gt;Field.Index.NO则表示不被索引。&lt;/li&gt;
&lt;li&gt;Field.Index.ANALYZED则表示不但被索引，而且被分词，比如索引&amp;quot;hello world&amp;quot;后，无论是搜&amp;quot;hello&amp;rdquo;，还是搜&amp;quot;world&amp;quot;都能够被搜到。&lt;/li&gt;
&lt;li&gt;Field.Index.NOT_ANALYZED表示虽然被索引，但是不分词，比如索引&amp;quot;hello world&amp;quot;后，仅当搜&amp;quot;hello world&amp;quot;时，能够搜到，搜&amp;quot;hello&amp;quot;和搜&amp;quot;world&amp;quot;都搜不到。&lt;/li&gt;
&lt;li&gt;一个域出了能够被索引，还能够被存储，仅仅被存储的域是搜索不到的，但是能通过文档号查到，多用于不想被搜索到，但是在通过其它域能够搜索到的情况下，能够随着文档号返回给用户的域。&lt;/li&gt;
&lt;li&gt;Field.Store.Yes则表示存储此域，Field.Store.NO则表示不存储此域。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;倒数第二位：1表示保存词向量，0为不保存词向量。
&lt;ul&gt;
&lt;li&gt;Field.TermVector.YES表示保存词向量。&lt;/li&gt;
&lt;li&gt;Field.TermVector.NO表示不保存词向量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;倒数第三位：1表示在词向量中保存位置信息。
&lt;ul&gt;
&lt;li&gt;Field.TermVector.WITH_POSITIONS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;倒数第四位：1表示在词向量中保存偏移量信息。
&lt;ul&gt;
&lt;li&gt;Field.TermVector.WITH_OFFSETS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;倒数第五位：1表示不保存标准化因子
&lt;ul&gt;
&lt;li&gt;Field.Index.ANALYZED_NO_NORMS&lt;/li&gt;
&lt;li&gt;Field.Index.NOT_ANALYZED_NO_NORMS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;倒数第六位：是否保存payload&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;要了解域的元数据信息，还要了解以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;位置(Position)和偏移量(Offset)的区别
&lt;ul&gt;
&lt;li&gt;位置是基于词Term的，偏移量是基于字母或汉字的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022117067.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引域(Indexed)和存储域(Stored)的区别
&lt;ul&gt;
&lt;li&gt;一个域为什么会被存储(store)而不被索引(Index)呢？在一个文档中的所有信息中，有这样一部分信息，可能不想被索引从而可以搜索到，但是当这个文档由于其他的信息被搜索到时，可以同其他信息一同返回。&lt;/li&gt;
&lt;li&gt;举个例子，读研究生时，您好不容易写了一篇论文交给您的导师，您的导师却要他所第一作者而您做第二作者，然而您导师不想别人在论文系统中搜索您的名字时找到这篇论文，于是在论文系统中，把第二作者这个Field的Indexed设为false，这样别人搜索您的名字，永远不知道您写过这篇论文，只有在别人搜索您导师的名字从而找到您的文章时，在一个角落表述着第二作者是您。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;payload的使用
&lt;ul&gt;
&lt;li&gt;我们知道，索引是以倒排表形式存储的，对于每一个词，都保存了包含这个词的一个链表，当然为了加快查询速度，此链表多用跳跃表进行存储。&lt;/li&gt;
&lt;li&gt;Payload信息就是存储在倒排表中的，同文档号一起存放，多用于存储与每篇文档相关的一些信息。当然这部分信息也可以存储域里(stored Field)，两者从功能上基本是一样的，然而当要存储的信息很多的时候，存放在倒排表里，利用跳跃表，有利于大大提高搜索速度。&lt;/li&gt;
&lt;li&gt;Payload的存储方式如下图：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022122309.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Payload主要有以下几种用法：
&lt;ul&gt;
&lt;li&gt;存储每个文档都有的信息：比如有的时候，我们想给每个文档赋一个我们自己的文档号，而不是用Lucene自己的文档号。于是我们可以声明一个特殊的域(Field)&amp;quot;_ID&amp;quot;和特殊的词(Term)&amp;quot;_ID&amp;quot;，使得每篇文档都包含词&amp;quot;_ID&amp;quot;，于是在词&amp;quot;_ID&amp;quot;的倒排表里面对于每篇文档又有一项，每一项都有一个payload，于是我们可以在payload里面保存我们自己的文档号。每当我们得到一个Lucene的文档号的时候，就能从跳跃表中查找到我们自己的文档号。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;//声明一个特殊的域和特殊的词&lt;/p&gt;
&lt;p&gt;public static final String ID_PAYLOAD_FIELD = &amp;ldquo;_ID&amp;rdquo;;&lt;/p&gt;
&lt;p&gt;public static final String ID_PAYLOAD_TERM = &amp;ldquo;_ID&amp;rdquo;;&lt;/p&gt;
&lt;p&gt;public static final Term ID_TERM = new Term(ID_PAYLOAD_TERM, ID_PAYLOAD_FIELD);&lt;/p&gt;
&lt;p&gt;//声明一个特殊的TokenStream，它只生成一个词(Term)，就是那个特殊的词，在特殊的域里面。&lt;/p&gt;
&lt;p&gt;static class SinglePayloadTokenStream extends TokenStream {&lt;br&gt;
private Token token;&lt;br&gt;
private boolean returnToken = false;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SinglePayloadTokenStream&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;String idPayloadTerm&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[]&lt;/span&gt; term &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idPayloadTerm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;toCharArray&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    token &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Token&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;term&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; term&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; term&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;setPayloadValue&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;byte&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[]&lt;/span&gt; value&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    token&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;setPayload&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Payload&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;value&lt;span style=&#34;color:#f92672&#34;&gt;));&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    returnToken &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; Token &lt;span style=&#34;color:#a6e22e&#34;&gt;next&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;throws&lt;/span&gt; IOException &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;returnToken&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        returnToken &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; token&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;}&lt;/p&gt;
&lt;p&gt;//对于每一篇文档，都让它包含这个特殊的词，在特殊的域里面&lt;/p&gt;
&lt;p&gt;SinglePayloadTokenStream singlePayloadTokenStream = new SinglePayloadTokenStream(ID_PAYLOAD_TERM);&lt;br&gt;
singlePayloadTokenStream.setPayloadValue(long2bytes(id));&lt;br&gt;
doc.add(new Field(ID_PAYLOAD_FIELD, singlePayloadTokenStream));&lt;/p&gt;
&lt;p&gt;//每当得到一个Lucene的文档号时，通过以下的方式得到payload里面的文档号&lt;/p&gt;
&lt;p&gt;long id = 0;&lt;br&gt;
TermPositions tp = reader.termPositions(ID_PAYLOAD_TERM);&lt;br&gt;
boolean ret = tp.skipTo(docID);&lt;br&gt;
tp.nextPosition();&lt;br&gt;
int payloadlength = tp.getPayloadLength();&lt;br&gt;
byte[] payloadBuffer = new byte[payloadlength];&lt;br&gt;
tp.getPayload(payloadBuffer, 0);&lt;br&gt;
id = bytes2long(payloadBuffer);&lt;br&gt;
tp.close();&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;影响词的评分
&lt;ul&gt;
&lt;li&gt;在Similarity抽象类中有函数public float scorePayload(byte [] payload, int offset, int length)  可以根据payload的值影响评分。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;读取域元数据信息的代码如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;FieldInfos.read(IndexInput, String)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;int firstInt = input.readVInt();&lt;/li&gt;
&lt;li&gt;size = input.readVInt();&lt;/li&gt;
&lt;li&gt;for (int i = 0; i &amp;lt; size; i++)
&lt;ul&gt;
&lt;li&gt;String name = input.readString();&lt;/li&gt;
&lt;li&gt;byte bits = input.readByte();&lt;/li&gt;
&lt;li&gt;boolean isIndexed = (bits &amp;amp; IS_INDEXED) != 0;&lt;/li&gt;
&lt;li&gt;boolean storeTermVector = (bits &amp;amp; STORE_TERMVECTOR) != 0;&lt;/li&gt;
&lt;li&gt;boolean storePositionsWithTermVector = (bits &amp;amp; STORE_POSITIONS_WITH_TERMVECTOR) != 0;&lt;/li&gt;
&lt;li&gt;boolean storeOffsetWithTermVector = (bits &amp;amp; STORE_OFFSET_WITH_TERMVECTOR) != 0;&lt;/li&gt;
&lt;li&gt;boolean omitNorms = (bits &amp;amp; OMIT_NORMS) != 0;&lt;/li&gt;
&lt;li&gt;boolean storePayloads = (bits &amp;amp; STORE_PAYLOADS) != 0;&lt;/li&gt;
&lt;li&gt;boolean omitTermFreqAndPositions = (bits &amp;amp; OMIT_TERM_FREQ_AND_POSITIONS) != 0;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;413-域field的数据信息fdtfdx&#34;&gt;&lt;strong&gt;4.1.3. 域(Field)的数据信息(.fdt，.fdx)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022125684.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;域数据文件(fdt):
&lt;ul&gt;
&lt;li&gt;真正保存存储域(stored field)信息的是fdt文件&lt;/li&gt;
&lt;li&gt;在一个段(segment)中总共有segment size篇文档，所以fdt文件中共有segment size个项，每一项保存一篇文档的域的信息&lt;/li&gt;
&lt;li&gt;对于每一篇文档，一开始是一个fieldcount，也即此文档包含的域的数目，接下来是fieldcount个项，每一项保存一个域的信息。&lt;/li&gt;
&lt;li&gt;对于每一个域，fieldnum是域号，接着是一个8位的byte，最低一位表示此域是否分词(tokenized)，倒数第二位表示此域是保存字符串数据还是二进制数据，倒数第三位表示此域是否被压缩，再接下来就是存储域的值，比如new Field(&amp;ldquo;title&amp;rdquo;, &amp;ldquo;lucene in action&amp;rdquo;, Field.Store.Yes, …)，则此处存放的就是&amp;quot;lucene in action&amp;quot;这个字符串。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;域索引文件(fdx)
&lt;ul&gt;
&lt;li&gt;由域数据文件格式我们知道，每篇文档包含的域的个数，每个存储域的值都是不一样的，因而域数据文件中segment size篇文档，每篇文档占用的大小也是不一样的，那么如何在fdt中辨别每一篇文档的起始地址和终止地址呢，如何能够更快的找到第n篇文档的存储域的信息呢？就是要借助域索引文件。&lt;/li&gt;
&lt;li&gt;域索引文件也总共有segment size个项，每篇文档都有一个项，每一项都是一个long，大小固定，每一项都是对应的文档在fdt文件中的起始地址的偏移量，这样如果我们想找到第n篇文档的存储域的信息，只要在fdx中找到第n项，然后按照取出的long作为偏移量，就可以在fdt文件中找到对应的存储域的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;读取域数据信息的代码如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Document FieldsReader.doc(int n, FieldSelector fieldSelector)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;long position = indexStream.readLong();//indexStream points to &amp;ldquo;.fdx&amp;rdquo;&lt;/li&gt;
&lt;li&gt;fieldsStream.seek(position);//fieldsStream points to &amp;ldquo;fdt&amp;rdquo;&lt;/li&gt;
&lt;li&gt;int numFields = fieldsStream.readVInt();&lt;/li&gt;
&lt;li&gt;for (int i = 0; i &amp;lt; numFields; i++)
&lt;ul&gt;
&lt;li&gt;int fieldNumber = fieldsStream.readVInt();&lt;/li&gt;
&lt;li&gt;byte bits = fieldsStream.readByte();&lt;/li&gt;
&lt;li&gt;boolean compressed = (bits &amp;amp; FieldsWriter.FIELD_IS_COMPRESSED) != 0;&lt;/li&gt;
&lt;li&gt;boolean tokenize = (bits &amp;amp; FieldsWriter.FIELD_IS_TOKENIZED) != 0;&lt;/li&gt;
&lt;li&gt;boolean binary = (bits &amp;amp; FieldsWriter.FIELD_IS_BINARY) != 0;&lt;/li&gt;
&lt;li&gt;if (binary)
&lt;ul&gt;
&lt;li&gt;int toRead = fieldsStream.readVInt();&lt;/li&gt;
&lt;li&gt;final byte[] b = new byte[toRead];&lt;/li&gt;
&lt;li&gt;fieldsStream.readBytes(b, 0, b.length);&lt;/li&gt;
&lt;li&gt;if (compressed)
&lt;ul&gt;
&lt;li&gt;int toRead = fieldsStream.readVInt();&lt;/li&gt;
&lt;li&gt;final byte[] b = new byte[toRead];&lt;/li&gt;
&lt;li&gt;fieldsStream.readBytes(b, 0, b.length);&lt;/li&gt;
&lt;li&gt;uncompress(b),&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;else
&lt;ul&gt;
&lt;li&gt;fieldsStream.readString()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;413-词向量term-vector的数据信息tvxtvdtvf&#34;&gt;&lt;strong&gt;4.1.3. 词向量(Term Vector)的数据信息(.tvx，.tvd，.tvf)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022135786.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;词向量信息是从索引(index)到文档(document)到域(field)到词(term)的正向信息，有了词向量信息，我们就可以得到一篇文档包含那些词的信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;词向量索引文件(tvx)
&lt;ul&gt;
&lt;li&gt;一个段(segment)包含N篇文档，此文件就有N项，每一项代表一篇文档。&lt;/li&gt;
&lt;li&gt;每一项包含两部分信息：第一部分是词向量文档文件(tvd)中此文档的偏移量，第二部分是词向量域文件(tvf)中此文档的第一个域的偏移量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;词向量文档文件(tvd)
&lt;ul&gt;
&lt;li&gt;一个段(segment)包含N篇文档，此文件就有N项，每一项包含了此文档的所有的域的信息。&lt;/li&gt;
&lt;li&gt;每一项首先是此文档包含的域的个数NumFields，然后是一个NumFields大小的数组，数组的每一项是域号。然后是一个(NumFields - 1)大小的数组，由前面我们知道，每篇文档的第一个域在tvf中的偏移量在tvx文件中保存，而其他(NumFields - 1)个域在tvf中的偏移量就是第一个域的偏移量加上这(NumFields - 1)个数组的每一项的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;词向量域文件(tvf)
&lt;ul&gt;
&lt;li&gt;此文件包含了此段中的所有的域，并不对文档做区分，到底第几个域到第几个域是属于那篇文档，是由tvx中的第一个域的偏移量以及tvd中的(NumFields - 1)个域的偏移量来决定的。&lt;/li&gt;
&lt;li&gt;对于每一个域，首先是此域包含的词的个数NumTerms，然后是一个8位的byte，最后一位是指定是否保存位置信息，倒数第二位是指定是否保存偏移量信息。然后是NumTerms个项的数组，每一项代表一个词(Term)，对于每一个词，由词的文本TermText，词频TermFreq(也即此词在此文档中出现的次数)，词的位置信息，词的偏移量信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;读取词向量数据信息的代码如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TermVectorsReader.get(int docNum, String field, TermVectorMapper)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;int fieldNumber = fieldInfos.fieldNumber(field);//通过field名字得到field号&lt;/li&gt;
&lt;li&gt;seekTvx(docNum);//在tvx文件中按docNum文档号找到相应文档的项&lt;/li&gt;
&lt;li&gt;long tvdPosition = tvx.readLong();//找到tvd文件中相应文档的偏移量&lt;/li&gt;
&lt;li&gt;tvd.seek(tvdPosition);//在tvd文件中按偏移量找到相应文档的项&lt;/li&gt;
&lt;li&gt;int fieldCount = tvd.readVInt();//此文档包含的域的个数。&lt;/li&gt;
&lt;li&gt;for (int i = 0; i &amp;lt; fieldCount; i++) //按域号查找域
&lt;ul&gt;
&lt;li&gt;number = tvd.readVInt();&lt;/li&gt;
&lt;li&gt;if (number == fieldNumber)
&lt;ul&gt;
&lt;li&gt;found = i;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;position = tvx.readLong();//在tvx中读出此文档的第一个域在tvf中的偏移量&lt;/li&gt;
&lt;li&gt;for (int i = 1; i &amp;lt;= found; i++)
&lt;ul&gt;
&lt;li&gt;position += tvd.readVLong();//加上所要找的域在tvf中的偏移量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tvf.seek(position);&lt;/li&gt;
&lt;li&gt;int numTerms = tvf.readVInt();&lt;/li&gt;
&lt;li&gt;byte bits = tvf.readByte();&lt;/li&gt;
&lt;li&gt;storePositions = (bits &amp;amp; STORE_POSITIONS_WITH_TERMVECTOR) != 0;&lt;/li&gt;
&lt;li&gt;storeOffsets = (bits &amp;amp; STORE_OFFSET_WITH_TERMVECTOR) != 0;&lt;/li&gt;
&lt;li&gt;for (int i = 0; i &amp;lt; numTerms; i++)
&lt;ul&gt;
&lt;li&gt;start = tvf.readVInt();&lt;/li&gt;
&lt;li&gt;deltaLength = tvf.readVInt();&lt;/li&gt;
&lt;li&gt;totalLength = start + deltaLength;&lt;/li&gt;
&lt;li&gt;tvf.readBytes(byteBuffer, start, deltaLength);&lt;/li&gt;
&lt;li&gt;term = new String(byteBuffer, 0, totalLength, &amp;ldquo;UTF-8&amp;rdquo;);&lt;/li&gt;
&lt;li&gt;if (storePositions)
&lt;ul&gt;
&lt;li&gt;positions = new int[freq];&lt;/li&gt;
&lt;li&gt;int prevPosition = 0;&lt;/li&gt;
&lt;li&gt;for (int j = 0; j &amp;lt; freq; j++)
&lt;ul&gt;
&lt;li&gt;positions[j] = prevPosition + tvf.readVInt();&lt;/li&gt;
&lt;li&gt;prevPosition = positions[j];&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if (storeOffsets)
&lt;ul&gt;
&lt;li&gt;offsets = new TermVectorOffsetInfo[freq];&lt;/li&gt;
&lt;li&gt;int prevOffset = 0;&lt;/li&gt;
&lt;li&gt;for (int j = 0; j &amp;lt; freq; j++)&lt;/li&gt;
&lt;li&gt;int startOffset = prevOffset + tvf.readVInt();&lt;/li&gt;
&lt;li&gt;int endOffset = startOffset + tvf.readVInt();&lt;/li&gt;
&lt;li&gt;offsets[j] = new TermVectorOffsetInfo(startOffset, endOffset);&lt;/li&gt;
&lt;li&gt;prevOffset = endOffset;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;42-反向信息&#34;&gt;&lt;strong&gt;4.2. 反向信息&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;反向信息是索引文件的核心，也即反向索引。&lt;/p&gt;
&lt;p&gt;反向索引包括两部分，左面是词典(Term Dictionary)，右面是倒排表(Posting List)。&lt;/p&gt;
&lt;p&gt;在Lucene中，这两部分是分文件存储的，词典是存储在tii，tis中的，倒排表又包括两部分，一部分是文档号及词频，保存在frq中，一部分是词的位置信息，保存在prx中。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Term Dictionary (tii, tis)
&lt;ul&gt;
&lt;li&gt;–&amp;gt; Frequencies (.frq)&lt;/li&gt;
&lt;li&gt;–&amp;gt; Positions (.prx)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;421-词典tis及词典索引tii信息&#34;&gt;&lt;strong&gt;4.2.1. 词典(tis)及词典索引(tii)信息&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022136378.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在词典中，所有的词是按照字典顺序排序的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;词典文件(tis)
&lt;ul&gt;
&lt;li&gt;TermCount：词典中包含的总的词数&lt;/li&gt;
&lt;li&gt;IndexInterval：为了加快对词的查找速度，也应用类似跳跃表的结构，假设IndexInterval为4，则在词典索引(tii)文件中保存第4个，第8个，第12个词，这样可以加快在词典文件中查找词的速度。&lt;/li&gt;
&lt;li&gt;SkipInterval：倒排表无论是文档号及词频，还是位置信息，都是以跳跃表的结构存在的，SkipInterval是跳跃的步数。&lt;/li&gt;
&lt;li&gt;MaxSkipLevels：跳跃表是多层的，这个值指的是跳跃表的最大层数。&lt;/li&gt;
&lt;li&gt;TermCount个项的数组，每一项代表一个词，对于每一个词，以前缀后缀规则存放词的文本信息(PrefixLength + Suffix)，词属于的域的域号(FieldNum)，有多少篇文档包含此词(DocFreq)，此词的倒排表在frq，prx中的偏移量(FreqDelta, ProxDelta)，此词的倒排表的跳跃表在frq中的偏移量(SkipDelta)，这里之所以用Delta，是应用差值规则。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;词典索引文件(tii)
&lt;ul&gt;
&lt;li&gt;词典索引文件是为了加快对词典文件中词的查找速度，保存每隔IndexInterval个词。&lt;/li&gt;
&lt;li&gt;词典索引文件是会被全部加载到内存中去的。&lt;/li&gt;
&lt;li&gt;IndexTermCount = TermCount / IndexInterval：词典索引文件中包含的词数。&lt;/li&gt;
&lt;li&gt;IndexInterval同词典文件中的IndexInterval。&lt;/li&gt;
&lt;li&gt;SkipInterval同词典文件中的SkipInterval。&lt;/li&gt;
&lt;li&gt;MaxSkipLevels同词典文件中的MaxSkipLevels。&lt;/li&gt;
&lt;li&gt;IndexTermCount个项的数组，每一项代表一个词，每一项包括两部分，第一部分是词本身(TermInfo)，第二部分是在词典文件中的偏移量(IndexDelta)。假设IndexInterval为4，此数组中保存第4个，第8个，第12个词。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;读取词典及词典索引文件的代码如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;origEnum = new SegmentTermEnum(directory.openInput(segment + &amp;ldquo;.&amp;rdquo; + IndexFileNames.TERMS_EXTENSION,readBufferSize), fieldInfos, false);//用于读取tis文件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;int firstInt = input.readInt();&lt;/li&gt;
&lt;li&gt;size = input.readLong();&lt;/li&gt;
&lt;li&gt;indexInterval = input.readInt();&lt;/li&gt;
&lt;li&gt;skipInterval = input.readInt();&lt;/li&gt;
&lt;li&gt;maxSkipLevels = input.readInt();&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openInput(segment + &amp;ldquo;.&amp;rdquo; + IndexFileNames.TERMS_INDEX_EXTENSION, readBufferSize), fieldInfos, true);//用于读取tii文件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;indexTerms = new Term[indexSize];&lt;/li&gt;
&lt;li&gt;indexInfos = new TermInfo[indexSize];&lt;/li&gt;
&lt;li&gt;indexPointers = new long[indexSize];&lt;/li&gt;
&lt;li&gt;for (int i = 0; indexEnum.next(); i++)
&lt;ul&gt;
&lt;li&gt;indexTerms[i] = indexEnum.term();&lt;/li&gt;
&lt;li&gt;indexInfos[i] = indexEnum.termInfo();&lt;/li&gt;
&lt;li&gt;indexPointers[i] = indexEnum.indexPointer;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;422-文档号及词频frq信息&#34;&gt;&lt;strong&gt;4.2.2. 文档号及词频(frq)信息&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022137895.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;文档号及词频文件里面保存的是倒排表，是以跳跃表形式存在的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;此文件包含TermCount个项，每一个词都有一项，因为每一个词都有自己的倒排表。&lt;/li&gt;
&lt;li&gt;对于每一个词的倒排表都包括两部分，一部分是倒排表本身，也即一个数组的文档号及词频，另一部分是跳跃表，为了更快的访问和定位倒排表中文档号及词频的位置。&lt;/li&gt;
&lt;li&gt;对于文档号和词频的存储应用的是差值规则和或然跟随规则，Lucene的文档本身有以下几句话，比较难以理解，在此解释一下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven, with omitTf false, would be the following sequence of VInts:&lt;/p&gt;
&lt;p&gt;15, 8, 3&lt;/p&gt;
&lt;p&gt;If omitTf were true it would be this sequence of VInts instead:&lt;/p&gt;
&lt;p&gt;7,4&lt;/p&gt;
&lt;p&gt;首先我们看omitTf=false的情况，也即我们在索引中会存储一个文档中term出现的次数。&lt;/p&gt;
&lt;p&gt;例子中说了，表示在文档7中出现1次，并且又在文档11中出现3次的文档用以下序列表示：15，8，3.&lt;/p&gt;
&lt;p&gt;那这三个数字是怎么计算出来的呢？&lt;/p&gt;
&lt;p&gt;首先，根据定义TermFreq &amp;ndash;&amp;gt; DocDelta[, Freq?]，一个TermFreq结构是由一个DocDelta后面或许跟着Freq组成，也即上面我们说的A+B？结构。&lt;/p&gt;
&lt;p&gt;DocDelta自然是想存储包含此Term的文档的ID号了，Freq是在此文档中出现的次数。&lt;/p&gt;
&lt;p&gt;所以根据例子，应该存储的完整信息为[DocID = 7, Freq = 1] [DocID = 11,  Freq = 3](见全文检索的基本原理章节)。&lt;/p&gt;
&lt;p&gt;然而为了节省空间，Lucene对编号此类的数据都是用差值来表示的，也即上面说的规则2，Delta规则，于是文档ID就不能按完整信息存了，就应该存放如下：&lt;/p&gt;
&lt;p&gt;[DocIDDelta = 7, Freq = 1][DocIDDelta = 4 (11-7), Freq = 3]&lt;/p&gt;
&lt;p&gt;然而Lucene对于A+B?这种或然跟随的结果，有其特殊的存储方式，见规则3，即A+B?规则，如果DocDelta后面跟随的Freq为1，则用DocDelta最后一位置1表示。&lt;/p&gt;
&lt;p&gt;如果DocDelta后面跟随的Freq大于1，则DocDelta得最后一位置0，然后后面跟随真正的值，从而对于第一个Term，由于Freq为1，于是放在DocDelta的最后一位表示，DocIDDelta = 7的二进制是000 0111，必须要左移一位，且最后一位置一，000 1111 = 15，对于第二个Term，由于Freq大于一，于是放在DocDelta的最后一位置零，DocIDDelta = 4的二进制是0000 0100，必须要左移一位，且最后一位置零，0000 1000 = 8，然后后面跟随真正的Freq = 3。&lt;/p&gt;
&lt;p&gt;于是得到序列：[DocDleta = 15][DocDelta = 8, Freq = 3]，也即序列，15，8，3。&lt;/p&gt;
&lt;p&gt;如果omitTf=true，也即我们不在索引中存储一个文档中Term出现的次数，则只存DocID就可以了，因而不存在A+B?规则的应用。&lt;/p&gt;
&lt;p&gt;[DocID = 7][DocID = 11]，然后应用规则2，Delta规则，于是得到序列[DocDelta = 7][DocDelta = 4 (11 - 7)]，也即序列，7，4.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于跳跃表的存储有以下几点需要解释一下：
&lt;ul&gt;
&lt;li&gt;跳跃表可根据倒排表本身的长度(DocFreq)和跳跃的幅度(SkipInterval)而分不同的层次，层次数为NumSkipLevels = Min(MaxSkipLevels, floor(log(DocFreq/log(SkipInterval)))).&lt;/li&gt;
&lt;li&gt;第Level层的节点数为DocFreq/(SkipInterval^(Level + 1))，level从零计数。&lt;/li&gt;
&lt;li&gt;除了最低层之外，其他层都有SkipLevelLength来表示此层的二进制长度(而非节点的个数)，方便读取某一层的跳跃表到缓存里面。&lt;/li&gt;
&lt;li&gt;高层在前，低层在后，当读完所有的高层后，剩下的就是最低一层，因而最后一层不需要SkipLevelLength。这也是为什么Lucene文档中的格式描述为 &lt;!-- raw HTML omitted --&gt;NumSkipLevels-1&lt;!-- raw HTML omitted --&gt;, SkipLevel，也即低NumSKipLevels-1层有SkipLevelLength，最后一层只有SkipLevel，没有SkipLevelLength。&lt;/li&gt;
&lt;li&gt;除最低层以外，其他层都有SkipChildLevelPointer来指向下一层相应的节点。&lt;/li&gt;
&lt;li&gt;每一个跳跃节点包含以下信息：文档号，payload的长度，文档号对应的倒排表中的节点在frq中的偏移量，文档号对应的倒排表中的节点在prx中的偏移量。&lt;/li&gt;
&lt;li&gt;虽然Lucene的文档中有以下的描述，然而实验的结果却不是完全准确的：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example: SkipInterval = 4, MaxSkipLevels = 2, DocFreq = 35. Then skip level 0 has 8 SkipData entries, containing the 3&lt;!-- raw HTML omitted --&gt;rd&lt;!-- raw HTML omitted --&gt;, 7&lt;!-- raw HTML omitted --&gt;th&lt;!-- raw HTML omitted --&gt;, 11&lt;!-- raw HTML omitted --&gt;th&lt;!-- raw HTML omitted --&gt;, 15&lt;!-- raw HTML omitted --&gt;th&lt;!-- raw HTML omitted --&gt;, 19&lt;!-- raw HTML omitted --&gt;th&lt;!-- raw HTML omitted --&gt;, 23&lt;!-- raw HTML omitted --&gt;rd&lt;!-- raw HTML omitted --&gt;, 27&lt;!-- raw HTML omitted --&gt;th&lt;!-- raw HTML omitted --&gt;, and 31&lt;!-- raw HTML omitted --&gt;st&lt;!-- raw HTML omitted --&gt; document numbers in TermFreqs. Skip level 1 has 2 SkipData entries, containing the 15&lt;!-- raw HTML omitted --&gt;th&lt;!-- raw HTML omitted --&gt; and 31&lt;!-- raw HTML omitted --&gt;st&lt;!-- raw HTML omitted --&gt; document numbers in TermFreqs.&lt;/p&gt;
&lt;p&gt;按照描述，当SkipInterval为4，且有35篇文档的时候，Skip level = 0应该包括第3，第7，第11，第15，第19，第23，第27，第31篇文档，Skip level = 1应该包括第15，第31篇文档。&lt;/p&gt;
&lt;p&gt;然而真正的实现中，跳跃表节点的时候，却向前偏移了，偏移的原因在于下面的代码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FormatPostingsDocsWriter.addDoc(int docID, int termDocFreq)
&lt;ul&gt;
&lt;li&gt;final int delta = docID - lastDocID;&lt;/li&gt;
&lt;li&gt;if ((++df % skipInterval) == 0)
&lt;ul&gt;
&lt;li&gt;skipListWriter.setSkipData(lastDocID, storePayloads, posWriter.lastPayloadLength);&lt;/li&gt;
&lt;li&gt;skipListWriter.bufferSkip(df);&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从代码中，我们可以看出，当SkipInterval为4的时候，当docID = 0时，++df为1，1%4不为0，不是跳跃节点，当docID = 3时，++df=4，4%4为0，为跳跃节点，然而skipData里面保存的却是lastDocID为2。&lt;/p&gt;
&lt;p&gt;所以真正的倒排表和跳跃表中保存一下的信息：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022137901.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;423-词位置prx信息&#34;&gt;&lt;strong&gt;4.2.3. 词位置(prx)信息&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022138998.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;词位置信息也是倒排表，也是以跳跃表形式存在的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;此文件包含TermCount个项，每一个词都有一项，因为每一个词都有自己的词位置倒排表。&lt;/li&gt;
&lt;li&gt;对于每一个词的都有一个DocFreq大小的数组，每项代表一篇文档，记录此文档中此词出现的位置。这个文档数组也是和frq文件中的跳跃表有关系的，从上面我们知道，在frq的跳跃表节点中有ProxSkip，当SkipInterval为3的时候，frq的跳跃表节点指向prx文件中的此数组中的第1，第4，第7，第10，第13，第16篇文档。&lt;/li&gt;
&lt;li&gt;对于每一篇文档，可能包含一个词多次，因而有一个Freq大小的数组，每一项代表此词在此文档中出现一次，则有一个位置信息。&lt;/li&gt;
&lt;li&gt;每一个位置信息包含：PositionDelta(采用差值规则)，还可以保存payload，应用或然跟随规则。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;43-其他信息&#34;&gt;&lt;strong&gt;4.3. 其他信息&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;431-标准化因子文件nrm&#34;&gt;&lt;strong&gt;4.3.1. 标准化因子文件(nrm)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;为什么会有标准化因子呢？从第一章中的描述，我们知道，在搜索过程中，搜索出的文档要按与查询语句的相关性排序，相关性大的打分(score)高，从而排在前面。相关性打分(score)使用向量空间模型(Vector Space Model)，在计算相关性之前，要计算Term Weight，也即某Term相对于某Document的重要性。在计算Term Weight时，主要有两个影响因素，一个是此Term在此文档中出现的次数，一个是此Term的普通程度。显然此Term在此文档中出现的次数越多，此Term在此文档中越重要。&lt;/p&gt;
&lt;p&gt;这种Term Weight的计算方法是最普通的，然而存在以下几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同的文档重要性不同。有的文档重要些，有的文档相对不重要，比如对于做软件的，在索引书籍的时候，我想让计算机方面的书更容易搜到，而文学方面的书籍搜索时排名靠后。&lt;/li&gt;
&lt;li&gt;不同的域重要性不同。有的域重要一些，如关键字，如标题，有的域不重要一些，如附件等。同样一个词(Term)，出现在关键字中应该比出现在附件中打分要高。&lt;/li&gt;
&lt;li&gt;根据词(Term)在文档中出现的绝对次数来决定此词对文档的重要性，有不合理的地方。比如长的文档词在文档中出现的次数相对较多，这样短的文档比较吃亏。比如一个词在一本砖头书中出现了10次，在另外一篇不足100字的文章中出现了9次，就说明砖头书应该排在前面码？不应该，显然此词在不足100字的文章中能出现9次，可见其对此文章的重要性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于以上原因，Lucene在计算Term Weight时，都会乘上一个标准化因子(Normalization Factor)，来减少上面三个问题的影响。&lt;/p&gt;
&lt;p&gt;标准化因子(Normalization Factor)是会影响随后打分(score)的计算的，Lucene的打分计算一部分发生在索引过程中，一般是与查询语句无关的参数如标准化因子，大部分发生在搜索过程中，会在搜索过程的代码分析中详述。&lt;/p&gt;
&lt;p&gt;标准化因子(Normalization Factor)在索引过程总的计算如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022139227.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;它包括三个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Document boost：此值越大，说明此文档越重要。&lt;/li&gt;
&lt;li&gt;Field boost：此域越大，说明此域越重要。&lt;/li&gt;
&lt;li&gt;lengthNorm(field) = (1.0 / Math.sqrt(numTerms))：一个域中包含的Term总数越多，也即文档越长，此值越小，文档越短，此值越大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从上面的公式，我们知道，一个词(Term)出现在不同的文档或不同的域中，标准化因子不同。比如有两个文档，每个文档有两个域，如果不考虑文档长度，就有四种排列组合，在重要文档的重要域中，在重要文档的非重要域中，在非重要文档的重要域中，在非重要文档的非重要域中，四种组合，每种有不同的标准化因子。&lt;/p&gt;
&lt;p&gt;于是在Lucene中，标准化因子共保存了(文档数目乘以域数目)个，格式如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022139297.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标准化因子文件(Normalization Factor File: nrm)：
&lt;ul&gt;
&lt;li&gt;NormsHeader：字符串“NRM”外加Version，依Lucene的版本的不同而不同。&lt;/li&gt;
&lt;li&gt;接着是一个数组，大小为NumFields，每个Field一项，每一项为一个Norms。&lt;/li&gt;
&lt;li&gt;Norms也是一个数组，大小为SegSize，即此段中文档的数量，每一项为一个Byte，表示一个浮点数，其中0~2为尾数，3~8为指数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;432-删除文档文件del&#34;&gt;&lt;strong&gt;4.3.2. 删除文档文件(del)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022139282.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;被删除文档文件(Deleted Document File: .del)
&lt;ul&gt;
&lt;li&gt;Format：在此文件中，Bits和DGaps只能保存其中之一，-1表示保存DGaps，非负值表示保存Bits。&lt;/li&gt;
&lt;li&gt;ByteCount：此段中有多少文档，就有多少个bit被保存，但是以byte形式计数，也即Bits的大小应该是byte的倍数。&lt;/li&gt;
&lt;li&gt;BitCount：Bits中有多少位被至1，表示此文档已经被删除。&lt;/li&gt;
&lt;li&gt;Bits：一个数组的byte，大小为ByteCount，应用时被认为是byte*8个bit。&lt;/li&gt;
&lt;li&gt;DGaps：如果删除的文档数量很小，则Bits大部分位为0，很浪费空间。DGaps采用以下的方式来保存稀疏数组：比如第十，十二，三十二个文档被删除，于是第十，十二，三十二位设为1，DGaps也是以byte为单位的，仅保存不为0的byte，如第1个byte，第4个byte，第1个byte十进制为20，第4个byte十进制为1。于是保存成DGaps，第1个byte，位置1用不定长正整数保存，值为20用二进制保存，第2个byte，位置4用不定长正整数保存，用差值为3，值为1用二进制保存，二进制数据不用差值表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;五总体结构&#34;&gt;&lt;strong&gt;五、总体结构&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202403022139069.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图示为Lucene索引文件的整体结构：
&lt;ul&gt;
&lt;li&gt;属于整个索引(Index)的segment.gen，segment_N，其保存的是段(segment)的元数据信息，然后分多个segment保存数据信息，同一个segment有相同的前缀文件名。&lt;/li&gt;
&lt;li&gt;对于每一个段，包含域信息，词信息，以及其他信息(标准化因子，删除文档)&lt;/li&gt;
&lt;li&gt;域信息也包括域的元数据信息，在fnm中，域的数据信息，在fdx，fdt中。&lt;/li&gt;
&lt;li&gt;词信息是反向信息，包括词典(tis, tii)，文档号及词频倒排表(frq)，词位置倒排表(prx)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大家可以通过看源代码，相应的Reader和Writer来了解文件结构，将更为透彻。&lt;/p&gt;
&lt;p&gt;参考资料&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623597.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623597.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623599.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623599.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Lucene 二: Lucene 的总体架构</title>
        <link>https://lxb.wiki/1c70c347/</link>
        <pubDate>Sat, 02 Sep 2023 21:59:04 +0800</pubDate>
        
        <guid>https://lxb.wiki/1c70c347/</guid>
        <description>&lt;p&gt;Lucene总的来说是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个高效的，可扩展的，全文检索库。&lt;/li&gt;
&lt;li&gt;全部用Java实现，无须配置。&lt;/li&gt;
&lt;li&gt;仅支持纯文本文件的索引(Indexing)和搜索(Search)。&lt;/li&gt;
&lt;li&gt;不负责由其他格式的文件抽取纯文本文件，或从网络中抓取文件的过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在Lucene in action中，Lucene 的架构和过程如下图，&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252200930.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;说明Lucene是有索引和搜索的两个过程，包含索引创建，索引，搜索三个要点。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;让我们更细一些看Lucene的各组件：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252202412.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;被索引的文档用Document对象表示。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IndexWriter通过函数addDocument将文档添加到索引中，实现创建索引的过程。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucene的索引是应用反向索引。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;当用户有请求时，Query代表用户的查询语句。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IndexSearcher通过函数search搜索Lucene Index。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IndexSearcher计算term weight和score并且将结果返回给用户。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;返回给用户的文档集合用TopDocsCollector表示。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么如何应用这些组件呢？&lt;/p&gt;
&lt;p&gt;让我们再详细到对Lucene API 的调用实现索引和搜索过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252203512.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引过程如下：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;创建一个IndexWriter用来写索引文件，它有几个参数，INDEX_DIR就是索引文件所存放的位置，Analyzer便是用来对文档进行词法分析和语言处理的。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建一个Document代表我们要索引的文档。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;将不同的Field加入到文档中。我们知道，一篇文档有多种信息，如题目，作者，修改时间，内容等。不同类型的信息用不同的Field来表示，在本例子中，一共有两类信息进行了索引，一个是文件路径，一个是文件内容。其中FileReader的SRC_FILE就表示要索引的源文件。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IndexWriter调用函数addDocument将索引写到索引文件夹中。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;搜索过程如下：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IndexReader将磁盘上的索引信息读入到内存，INDEX_DIR就是索引文件存放的位置。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建IndexSearcher准备进行搜索。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建Analyer用来对查询语句进行词法分析和语言处理。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建QueryParser用来对查询语句进行语法分析。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;QueryParser调用parser进行语法分析，形成查询语法树，放到Query中。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IndexSearcher调用search对查询语法树Query进行搜索，得到结果TopScoreDocCollector。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上便是Lucene API函数的简单调用。&lt;/p&gt;
&lt;p&gt;然而当进入Lucene的源代码后，发现Lucene有很多包，关系错综复杂。&lt;/p&gt;
&lt;p&gt;然而通过下图，我们不难发现，Lucene的各源码模块，都是对普通索引和搜索过程的一种实现。&lt;/p&gt;
&lt;p&gt;此图是上一节介绍的全文检索的流程对应的Lucene实现的包结构。(参照http://www.lucene.com.cn/about.htm中文章《开放源代码的全文检索引擎Lucene》)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252206421.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lucene的analysis模块主要负责词法分析及语言处理而形成Term。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucene的index模块主要负责索引的创建，里面有IndexWriter。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucene的store模块主要负责索引的读写。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucene的QueryParser主要负责语法分析。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucene的search模块主要负责对索引的搜索。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucene的similarity模块主要负责对相关性打分的实现。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Lucene 一: 全文检索的基本原理</title>
        <link>https://lxb.wiki/5e159a30/</link>
        <pubDate>Fri, 01 Sep 2023 21:20:39 +0800</pubDate>
        
        <guid>https://lxb.wiki/5e159a30/</guid>
        <description>&lt;h2 id=&#34;一总论&#34;&gt;一、总论&lt;/h2&gt;
&lt;p&gt;根据&lt;a class=&#34;link&#34; href=&#34;http://lucene.apache.org/java/docs/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://lucene.apache.org/java/docs/index.html&lt;/a&gt;定义：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lucene 是一个高效的，基于Java 的全文检索库。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以在了解Lucene之前要费一番工夫了解一下全文检索。&lt;/p&gt;
&lt;p&gt;那么什么叫做全文检索呢？这要从我们生活中的数据说起。&lt;/p&gt;
&lt;p&gt;我们生活中的数据总体分为两种：&lt;strong&gt;结构化数据&lt;/strong&gt;和&lt;strong&gt;非结构化数据&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;结构化数据：&lt;/strong&gt; 指具有固定格式或有限长度的数据，如数据库，元数据等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非结构化数据：&lt;/strong&gt; 指不定长或无固定格式的数据，如邮件，word文档等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然有的地方还会提到第三种，半结构化数据，如XML，HTML等，当根据需要可按结构化数据来处理，也可抽取出纯文本按非结构化数据来处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;非结构化数据又一种叫法叫全文数据。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按照数据的分类，搜索也分为两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对结构化数据的搜索&lt;/strong&gt;：如对数据库的搜索，用SQL语句。再如对元数据的搜索，如利用windows搜索对文件名，类型，修改时间进行搜索等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对非结构化数据的搜索&lt;/strong&gt;：如利用windows的搜索也可以搜索文件内容，Linux下的grep命令，再如用Google和百度可以搜索大量内容数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对非结构化数据也即对全文数据的搜索主要有两种方法：&lt;/p&gt;
&lt;p&gt;一种是&lt;strong&gt;顺序扫描法(Serial Scanning)：&lt;/strong&gt; 所谓顺序扫描，比如要找内容包含某一个字符串的文件，就是一个文档一个文档的看，对于每一个文档，从头看到尾，如果此文档包含此字符串，则此文档为我们要找的文件，接着看下一个文件，直到扫描完所有的文件。如利用windows的搜索也可以搜索文件内容，只是相当的慢。如果你有一个80G硬盘，如果想在上面找到一个内容包含某字符串的文件，不花他几个小时，怕是做不到。Linux下的grep命令也是这一种方式。大家可能觉得这种方法比较原始，但对于小数据量的文件，这种方法还是最直接，最方便的。但是对于大量的文件，这种方法就很慢了。&lt;/p&gt;
&lt;p&gt;有人可能会说，对非结构化数据顺序扫描很慢，对结构化数据的搜索却相对较快（由于结构化数据有一定的结构可以采取一定的搜索算法加快速度），那么把我们的非结构化数据想办法弄得有一定结构不就行了吗？&lt;/p&gt;
&lt;p&gt;这种想法很天然，却构成了全文检索的基本思路，也即将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。&lt;/p&gt;
&lt;p&gt;这部分从非结构化数据中提取出的然后重新组织的信息，我们称之&lt;strong&gt;索引&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这种说法比较抽象，举几个例子就很容易明白，比如字典，字典的拼音表和部首检字表就相当于字典的索引，对每一个字的解释是非结构化的，如果字典没有音节表和部首检字表，在茫茫辞海中找一个字只能顺序扫描。然而字的某些信息可以提取出来进行结构化处理，比如读音，就比较结构化，分声母和韵母，分别只有几种可以一一列举，于是将读音拿出来按一定的顺序排列，每一项读音都指向此字的详细解释的页数。我们搜索时按结构化的拼音搜到读音，然后按其指向的页数，便可找到我们的非结构化数据——也即对字的解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这种先建立索引，再对索引进行搜索的过程就叫全文检索(Full-text Search)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面这幅图来自《Lucene in action》，但却不仅仅描述了Lucene的检索过程，而是描述了全文检索的一般过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252124676.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;全文检索大体分两个过程，&lt;strong&gt;索引创建(Indexing)&lt;strong&gt;和&lt;/strong&gt;搜索索引(Search)&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引创建：将现实世界中所有的结构化和非结构化数据提取信息，创建索引的过程。&lt;/li&gt;
&lt;li&gt;搜索索引：就是得到用户的查询请求，搜索创建的索引，然后返回结果的过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;于是全文检索就存在三个重要问题：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; &lt;strong&gt;索引里面究竟存些什么？(Index)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; &lt;strong&gt;如何创建索引？(Indexing)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; &lt;strong&gt;如何对索引进行搜索？(Search)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面我们顺序对每个个问题进行研究。&lt;/p&gt;
&lt;h2 id=&#34;二索引里面究竟存些什么&#34;&gt;二、索引里面究竟存些什么&lt;/h2&gt;
&lt;p&gt;索引里面究竟需要存些什么呢？&lt;/p&gt;
&lt;p&gt;首先我们来看为什么顺序扫描的速度慢：&lt;/p&gt;
&lt;p&gt;其实是由于我们想要搜索的信息和非结构化数据中所存储的信息不一致造成的。&lt;/p&gt;
&lt;p&gt;非结构化数据中所存储的信息是每个文件包含哪些字符串，也即已知文件，欲求字符串相对容易，也即是从文件到字符串的映射。而我们想搜索的信息是哪些文件包含此字符串，也即已知字符串，欲求文件，也即从字符串到文件的映射。两者恰恰相反。于是如果索引总能够保存从字符串到文件的映射，则会大大提高搜索速度。&lt;/p&gt;
&lt;p&gt;由于从字符串到文件的映射是文件到字符串映射的反向过程，于是保存这种信息的索引称为&lt;strong&gt;反向索引&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;反向索引的所保存的信息一般如下：&lt;/p&gt;
&lt;p&gt;假设我的文档集合里面有100篇文档，为了方便表示，我们为文档编号从1到100，得到下面的结构&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252127189.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;左边保存的是一系列字符串，称为&lt;strong&gt;词典&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;每个字符串都指向包含此字符串的文档(Document)链表，此文档链表称为&lt;strong&gt;倒排表&lt;/strong&gt;(Posting List)。&lt;/p&gt;
&lt;p&gt;有了索引，便使保存的信息和要搜索的信息一致，可以大大加快搜索的速度。&lt;/p&gt;
&lt;p&gt;比如说，我们要寻找既包含字符串“lucene”又包含字符串“solr”的文档，我们只需要以下几步：&lt;/p&gt;
&lt;p&gt;1. 取出包含字符串“lucene”的文档链表。&lt;/p&gt;
&lt;p&gt;2. 取出包含字符串“solr”的文档链表。&lt;/p&gt;
&lt;p&gt;3. 通过合并链表，找出既包含“lucene”又包含“solr”的文件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252128442.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;看到这个地方，有人可能会说，全文检索的确加快了搜索的速度，但是多了索引的过程，两者加起来不一定比顺序扫描快多少。的确，加上索引的过程，全文检索不一定比顺序扫描快，尤其是在数据量小的时候更是如此。而对一个很大量的数据创建索引也是一个很慢的过程。&lt;/p&gt;
&lt;p&gt;然而两者还是有区别的，顺序扫描是每次都要扫描，而创建索引的过程仅仅需要一次，以后便是一劳永逸的了，每次搜索，创建索引的过程不必经过，仅仅搜索创建好的索引就可以了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这也是全文搜索相对于顺序扫描的优势之一：一次索引，多次使用。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;三如何创建索引&#34;&gt;三、如何创建索引&lt;/h2&gt;
&lt;p&gt;全文检索的索引创建过程一般有以下几步：&lt;/p&gt;
&lt;h3 id=&#34;第一步一些要索引的原文档document&#34;&gt;&lt;strong&gt;第一步：一些要索引的原文档(Document)。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;为了方便说明索引创建过程，这里特意用两个文件为例：&lt;/p&gt;
&lt;p&gt;文件一：Students should be allowed to go out with their friends, but not allowed to drink beer.&lt;/p&gt;
&lt;p&gt;文件二：My friend Jerry went to school to see his students but found them drunk which is not allowed.&lt;/p&gt;
&lt;h3 id=&#34;第二步将原文档传给分词组件tokenizer&#34;&gt;&lt;strong&gt;第二步：将原文档传给分词组件(Tokenizer)。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;分词组件(Tokenizer)会做以下几件事情(此过程称为Tokenize)：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; &lt;strong&gt;将文档分成一个一个单独的单词。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; &lt;strong&gt;去除标点符号。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; &lt;strong&gt;去除停词(Stop word)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所谓停词(Stop word)就是一种语言中最普通的一些单词，由于没有特别的意义，因而大多数情况下不能成为搜索的关键词，因而创建索引时，这种词会被去掉而减少索引的大小。&lt;/p&gt;
&lt;p&gt;英语中挺词(Stop word)如：“the”,“a”，“this”等。&lt;/p&gt;
&lt;p&gt;对于每一种语言的分词组件(Tokenizer)，都有一个停词(stop word)集合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;经过分词(Tokenizer)后得到的结果称为词元(Token)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在我们的例子中，便得到以下词元(Token)：&lt;/p&gt;
&lt;p&gt;“Students”，“allowed”，“go”，“their”，“friends”，“allowed”，“drink”，“beer”，“My”，“friend”，“Jerry”，“went”，“school”，“see”，“his”，“students”，“found”，“them”，“drunk”，“allowed”。&lt;/p&gt;
&lt;h3 id=&#34;第三步将得到的词元token传给语言处理组件linguistic-processor&#34;&gt;&lt;strong&gt;第三步：将得到的词元(Token)传给语言处理组件(Linguistic Processor)。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;语言处理组件(linguistic processor)主要是对得到的词元(Token)做一些同语言相关的处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;对于英语，语言处理组件(Linguistic Processor)一般做以下几点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; &lt;strong&gt;变为小写(Lowercase)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; &lt;strong&gt;将单词缩减为词根形式，如“cars”到“car”等。这种操作称为：stemming。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; &lt;strong&gt;将单词转变为词根形式，如“drove”到“drive”等。这种操作称为：lemmatization。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stemming 和 lemmatization的异同：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相同之处：Stemming和lemmatization都要使词汇成为词根形式。&lt;/li&gt;
&lt;li&gt;两者的方式不同：
&lt;ul&gt;
&lt;li&gt;Stemming采用的是“缩减”的方式：“cars”到“car”，“driving”到“drive”。&lt;/li&gt;
&lt;li&gt;Lemmatization采用的是“转变”的方式：“drove”到“drove”，“driving”到“drive”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;两者的算法不同：
&lt;ul&gt;
&lt;li&gt;Stemming主要是采取某种固定的算法来做这种缩减，如去除“s”，去除“ing”加“e”，将“ational”变为“ate”，将“tional”变为“tion”。&lt;/li&gt;
&lt;li&gt;Lemmatization主要是采用保存某种字典的方式做这种转变。比如字典中有“driving”到“drive”，“drove”到“drive”，“am, is, are”到“be”的映射，做转变时，只要查字典就可以了。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stemming和lemmatization不是互斥关系，是有交集的，有的词利用这两种方式都能达到相同的转换。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;语言处理组件(linguistic processor)的结果称为词(Term)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在我们的例子中，经过语言处理，得到的词(Term)如下：&lt;/p&gt;
&lt;p&gt;“student”，“allow”，“go”，“their”，“friend”，“allow”，“drink”，“beer”，“my”，“friend”，“jerry”，“go”，“school”，“see”，“his”，“student”，“find”，“them”，“drink”，“allow”。&lt;/p&gt;
&lt;p&gt;也正是因为有语言处理的步骤，才能使搜索drove，而drive也能被搜索出来。&lt;/p&gt;
&lt;h3 id=&#34;第四步将得到的词term传给索引组件indexer&#34;&gt;&lt;strong&gt;第四步：将得到的词(Term)传给索引组件(Indexer)。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;索引组件(Indexer)主要做以下几件事情：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 利用得到的词(Term)创建一个字典。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在我们的例子中字典如下：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; &lt;strong&gt;对字典按字母顺序进行排序。&lt;/strong&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; &lt;strong&gt;合并相同的词(Term)成为文档倒排(Posting List)链表。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252135690.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在此表中，有几个定义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Document Frequency 即文档频次，表示总共有多少文件包含此词(Term)。&lt;/li&gt;
&lt;li&gt;Frequency 即词频率，表示此文件中包含了几个此词(Term)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以对词(Term) “allow”来讲，总共有两篇文档包含此词(Term)，从而词(Term)后面的文档链表总共有两项，第一项表示包含“allow”的第一篇文档，即1号文档，此文档中，“allow”出现了2次，第二项表示包含“allow”的第二个文档，是2号文档，此文档中，“allow”出现了1次。&lt;/p&gt;
&lt;p&gt;到此为止，索引已经创建好了，我们可以通过它很快的找到我们想要的文档。&lt;/p&gt;
&lt;p&gt;而且在此过程中，我们惊喜地发现，搜索“drive”，“driving”，“drove”，“driven”也能够被搜到。因为在我们的索引中，“driving”，“drove”，“driven”都会经过语言处理而变成“drive”，在搜索时，如果您输入“driving”，输入的查询语句同样经过我们这里的一到三步，从而变为查询“drive”，从而可以搜索到想要的文档。&lt;/p&gt;
&lt;h2 id=&#34;三如何对索引进行搜索&#34;&gt;三、如何对索引进行搜索？&lt;/h2&gt;
&lt;p&gt;到这里似乎我们可以宣布“我们找到想要的文档了”。&lt;/p&gt;
&lt;p&gt;然而事情并没有结束，找到了仅仅是全文检索的一个方面。不是吗？如果仅仅只有一个或十个文档包含我们查询的字符串，我们的确找到了。然而如果结果有一千个，甚至成千上万个呢？那个又是您最想要的文件呢？&lt;/p&gt;
&lt;p&gt;打开Google吧，比如说您想在微软找份工作，于是您输入“Microsoft job”，您却发现总共有22600000个结果返回。好大的数字呀，突然发现找不到是一个问题，找到的太多也是一个问题。在如此多的结果中，如何将最相关的放在最前面呢？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252137104.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;当然Google做的很不错，您一下就找到了jobs at Microsoft。想象一下，如果前几个全部是“Microsoft does a good job at software industry…”将是多么可怕的事情呀。&lt;/p&gt;
&lt;p&gt;如何像Google一样，在成千上万的搜索结果中，找到和查询语句最相关的呢？&lt;/p&gt;
&lt;p&gt;如何判断搜索出的文档和查询语句的相关性呢？&lt;/p&gt;
&lt;p&gt;这要回到我们第三个问题：如何对索引进行搜索？&lt;/p&gt;
&lt;p&gt;搜索主要分为以下几步：&lt;/p&gt;
&lt;h3 id=&#34;第一步用户输入查询语句&#34;&gt;&lt;strong&gt;第一步：用户输入查询语句。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;查询语句同我们普通的语言一样，也是有一定语法的。&lt;/p&gt;
&lt;p&gt;不同的查询语句有不同的语法，如SQL语句就有一定的语法。&lt;/p&gt;
&lt;p&gt;查询语句的语法根据全文检索系统的实现而不同。最基本的有比如：AND, OR, NOT等。&lt;/p&gt;
&lt;p&gt;举个例子，用户输入语句：lucene AND learned NOT hadoop。&lt;/p&gt;
&lt;p&gt;说明用户想找一个包含lucene和learned然而不包括hadoop的文档。&lt;/p&gt;
&lt;h3 id=&#34;第二步对查询语句进行词法分析语法分析及语言处理&#34;&gt;&lt;strong&gt;第二步：对查询语句进行词法分析，语法分析，及语言处理。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;由于查询语句有语法，因而也要进行语法分析，语法分析及语言处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 词法分析主要用来识别单词和关键字。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如上述例子中，经过词法分析，得到单词有lucene，learned，hadoop, 关键字有AND, NOT。&lt;/p&gt;
&lt;p&gt;如果在词法分析中发现不合法的关键字，则会出现错误。如lucene AMD learned，其中由于AND拼错，导致AMD作为一个普通的单词参与查询。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 语法分析主要是根据查询语句的语法规则来形成一棵语法树。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果发现查询语句不满足语法规则，则会报错。如lucene NOT AND learned，则会出错。&lt;/p&gt;
&lt;p&gt;如上述例子，lucene AND learned NOT hadoop形成的语法树如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252139825.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. 语言处理同索引过程中的语言处理几乎相同。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如learned变成learn等。&lt;/p&gt;
&lt;p&gt;经过第二步，我们得到一棵经过语言处理的语法树。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252139377.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;第三步搜索索引得到符合语法树的文档&#34;&gt;&lt;strong&gt;第三步：搜索索引，得到符合语法树的文档。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;此步骤有分几小步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先，在反向索引表中，分别找出包含lucene，learn，hadoop的文档链表。&lt;/li&gt;
&lt;li&gt;其次，对包含lucene，learn的链表进行合并操作，得到既包含lucene又包含learn的文档链表。&lt;/li&gt;
&lt;li&gt;然后，将此链表与hadoop的文档链表进行差操作，去除包含hadoop的文档，从而得到既包含lucene又包含learn而且不包含hadoop的文档链表。&lt;/li&gt;
&lt;li&gt;此文档链表就是我们要找的文档。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;第四步根据得到的文档和查询语句的相关性对结果进行排序&#34;&gt;&lt;strong&gt;第四步：根据得到的文档和查询语句的相关性，对结果进行排序。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;虽然在上一步，我们得到了想要的文档，然而对于查询结果应该按照与查询语句的相关性进行排序，越相关者越靠前。&lt;/p&gt;
&lt;p&gt;如何计算文档和查询语句的相关性呢？&lt;/p&gt;
&lt;p&gt;不如我们把查询语句看作一片短小的文档，对文档与文档之间的相关性(relevance)进行打分(scoring)，分数高的相关性好，就应该排在前面。&lt;/p&gt;
&lt;p&gt;那么又怎么对文档之间的关系进行打分呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这可不是一件容易的事情，首先我们看一看判断人之间的关系吧。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先&lt;/strong&gt;看一个人，往往有很多&lt;strong&gt;要素&lt;/strong&gt;，如性格，信仰，爱好，衣着，高矮，胖瘦等等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其次&lt;/strong&gt;对于人与人之间的关系，&lt;strong&gt;不同的要素重要性不同&lt;/strong&gt;，性格，信仰，爱好可能重要些，衣着，高矮，胖瘦可能就不那么重要了，所以具有相同或相似性格，信仰，爱好的人比较容易成为好的朋友，然而衣着，高矮，胖瘦不同的人，也可以成为好的朋友。&lt;/p&gt;
&lt;p&gt;因而判断人与人之间的关系，&lt;strong&gt;首先要找出哪些要素对人与人之间的关系最重要&lt;/strong&gt;，比如性格，信仰，爱好。&lt;strong&gt;其次要判断两个人的这些要素之间的关系&lt;/strong&gt;，比如一个人性格开朗，另一个人性格外向，一个人信仰佛教，另一个信仰上帝，一个人爱好打篮球，另一个爱好踢足球。我们发现，两个人在性格方面都很积极，信仰方面都很善良，爱好方面都爱运动，因而两个人关系应该会很好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我们再来看看公司之间的关系吧。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先&lt;/strong&gt;看一个公司，有很多人组成，如总经理，经理，首席技术官，普通员工，保安，门卫等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其次对于公司与公司之间的关系，不同的人重要性不同&lt;/strong&gt;，总经理，经理，首席技术官可能更重要一些，普通员工，保安，门卫可能较不重要一点。所以如果两个公司总经理，经理，首席技术官之间关系比较好，两个公司容易有比较好的关系。然而一位普通员工就算与另一家公司的一位普通员工有血海深仇，怕也难影响两个公司之间的关系。&lt;/p&gt;
&lt;p&gt;因而判断公司与公司之间的关系，&lt;strong&gt;首先要找出哪些人对公司与公司之间的关系最重要&lt;/strong&gt;，比如总经理，经理，首席技术官。&lt;strong&gt;其次要判断这些人之间的关系&lt;/strong&gt;，不如两家公司的总经理曾经是同学，经理是老乡，首席技术官曾是创业伙伴。我们发现，两家公司无论总经理，经理，首席技术官，关系都很好，因而两家公司关系应该会很好。&lt;/p&gt;
&lt;p&gt;分析了两种关系，下面看一下&lt;strong&gt;如何判断文档之间的关系&lt;/strong&gt;了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先，一个文档有很多词(Term)组成&lt;/strong&gt;，如search, lucene, full-text, this, a, what等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其次对于文档之间的关系，不同的Term重要性不同&lt;/strong&gt;，比如对于本篇文档，search, Lucene, full-text就相对重要一些，this, a , what可能相对不重要一些。所以如果两篇文档都包含search, Lucene，fulltext，这两篇文档的相关性好一些，然而就算一篇文档包含this, a, what，另一篇文档不包含this, a, what，也不能影响两篇文档的相关性。&lt;/p&gt;
&lt;p&gt;因而判断文档之间的关系，首先找出哪些词(Term)对文档之间的关系最重要，如search, Lucene, fulltext。然后判断这些词(Term)之间的关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;找出词(Term)对文档的重要性的过程称为计算词的权重(Term weight)的过程。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;计算词的权重(term weight)有两个参数，第一个是词(Term)，第二个是文档(Document)。&lt;/p&gt;
&lt;p&gt;词的权重(Term weight)表示此词(Term)在此文档中的重要程度，越重要的词(Term)有越大的权重(Term weight)，因而在计算文档之间的相关性中将发挥更大的作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;判断词(Term)之间的关系从而得到文档相关性的过程应用一种叫做向量空间模型的算法(Vector Space Model)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面仔细分析一下这两个过程：&lt;/p&gt;
&lt;h4 id=&#34;1-计算权重term-weight的过程&#34;&gt;&lt;strong&gt;1. 计算权重(Term weight)的过程。&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;影响一个词(Term)在一篇文档中的重要性主要有两个因素：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Term Frequency (tf)：即此Term在此文档中出现了多少次。tf 越大说明越重要。&lt;/li&gt;
&lt;li&gt;Document Frequency (df)：即有多少文档包含次Term。df 越大说明越不重要。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;容易理解吗？词(Term)在文档中出现的次数越多，说明此词(Term)对该文档越重要，如“搜索”这个词，在本文档中出现的次数很多，说明本文档主要就是讲这方面的事的。然而在一篇英语文档中，this出现的次数更多，就说明越重要吗？不是的，这是由第二个因素进行调整，第二个因素说明，有越多的文档包含此词(Term), 说明此词(Term)太普通，不足以区分这些文档，因而重要性越低。&lt;/p&gt;
&lt;p&gt;这也如我们程序员所学的技术，对于程序员本身来说，这项技术掌握越深越好（掌握越深说明花时间看的越多，tf越大），找工作时越有竞争力。然而对于所有程序员来说，这项技术懂得的人越少越好（懂得的人少df小），找工作越有竞争力。人的价值在于不可替代性就是这个道理。&lt;/p&gt;
&lt;p&gt;道理明白了，我们来看看公式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252144198.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252145148.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这仅仅只term weight计算公式的简单典型实现。实现全文检索系统的人会有自己的实现，Lucene就与此稍有不同。&lt;/p&gt;
&lt;h4 id=&#34;2-判断term之间的关系从而得到文档相关性的过程也即向量空间模型的算法vsm&#34;&gt;&lt;strong&gt;2. 判断Term之间的关系从而得到文档相关性的过程，也即向量空间模型的算法(VSM)。&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;我们把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。&lt;/p&gt;
&lt;p&gt;于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。&lt;/p&gt;
&lt;p&gt;Document = {term1, term2, …… ,term N}&lt;/p&gt;
&lt;p&gt;Document Vector = {weight1, weight2, …… ,weight N}&lt;/p&gt;
&lt;p&gt;同样我们把查询语句看作一个简单的文档，也用向量来表示。&lt;/p&gt;
&lt;p&gt;Query = {term1, term 2, …… , term N}&lt;/p&gt;
&lt;p&gt;Query Vector = {weight1, weight2, …… , weight N}&lt;/p&gt;
&lt;p&gt;我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。&lt;/p&gt;
&lt;p&gt;如图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252148225.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;我们认为两个向量之间的夹角越小，相关性越大。&lt;/p&gt;
&lt;p&gt;所以我们计算夹角的余弦值作为相关性的打分，夹角越小，余弦值越大，打分越高，相关性越大。&lt;/p&gt;
&lt;p&gt;有人可能会问，查询语句一般是很短的，包含的词(Term)是很少的，因而查询向量的维数很小，而文档很长，包含词(Term)很多，文档向量维数很大。你的图中两者维数怎么都是N呢？&lt;/p&gt;
&lt;p&gt;在这里，既然要放到相同的向量空间，自然维数是相同的，不同时，取二者的并集，如果不含某个词(Term)时，则权重(Term Weight)为0。&lt;/p&gt;
&lt;p&gt;相关性打分公式如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252151747.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;举个例子，查询语句有11个Term，共有三篇文档搜索出来。其中各自的权重(Term weight)，如下表格。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;于是计算，三篇文档同查询语句的相关性打分分别为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252152826.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252152014.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252153180.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;于是文档二相关性最高，先返回，其次是文档一，最后是文档三。&lt;/p&gt;
&lt;p&gt;到此为止，我们可以找到我们最想要的文档了。&lt;/p&gt;
&lt;p&gt;说了这么多，其实还没有进入到Lucene，而仅仅是信息检索技术(Information retrieval)中的基本理论，然而当我们看过Lucene后我们会发现，Lucene是对这种基本理论的一种基本的的实践。所以在以后分析Lucene的文章中，会常常看到以上理论在Lucene中的应用。&lt;/p&gt;
&lt;p&gt;在进入Lucene之前，对上述索引创建和搜索过程所一个总结，如图：&lt;/p&gt;
&lt;p&gt;此图参照&lt;a class=&#34;link&#34; href=&#34;http://www.lucene.com.cn/about.htm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://www.lucene.com.cn/about.htm&lt;/a&gt;中文章《开放源代码的全文检索引擎Lucene》&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252155722.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 索引过程：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; &lt;strong&gt;有一系列被索引文件&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2)&lt;/strong&gt; &lt;strong&gt;被索引文件经过语法分析和语言处理形成一系列词(Term)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3)&lt;/strong&gt; &lt;strong&gt;经过索引创建形成词典和反向索引表。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4)&lt;/strong&gt; &lt;strong&gt;通过索引存储将索引写入硬盘。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 搜索过程：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;a)&lt;/strong&gt; &lt;strong&gt;用户输入查询语句。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;b)&lt;/strong&gt; &lt;strong&gt;对查询语句经过语法分析和语言分析得到一系列词(Term)。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;c)&lt;/strong&gt; &lt;strong&gt;通过语法分析得到一个查询树。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;d)&lt;/strong&gt; &lt;strong&gt;通过索引存储将索引读入到内存。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;e)&lt;/strong&gt; &lt;strong&gt;利用查询树搜索索引，从而得到每个词(Term)的文档链表，对文档链表进行交，差，并得到结果文档。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;f)&lt;/strong&gt; &lt;strong&gt;将搜索到的结果文档对查询的相关性进行排序。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;g)&lt;/strong&gt; &lt;strong&gt;返回查询结果给用户。&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>【译】Lucene 查询语法</title>
        <link>https://lxb.wiki/f8737bfa/</link>
        <pubDate>Wed, 28 Jun 2023 23:02:47 +0800</pubDate>
        
        <guid>https://lxb.wiki/f8737bfa/</guid>
        <description>&lt;p&gt;原文：&lt;a class=&#34;link&#34; href=&#34;http://lucene.apache.org/core/8_2_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package.description&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Query Parser Syntax&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;概览&#34;&gt;概览&lt;/h2&gt;
&lt;p&gt;Lucene 除了提供 API 方便开发者创建查询请求，还通过一个查询解析器（词法分析器，使用 JavaCC 将字符串翻译成 Lucene 查询语句）提供一种功能丰富的查询语言。&lt;/p&gt;
&lt;p&gt;一般来说，查询解析器支持的语法在不同发布版本之间可能会有变化。本文档描述的是当前版本的语法。如果你正在使用一个不同版本的 Lucene，请参考该版本自带的 &lt;code&gt;docs/queryparsersyntax.html&lt;/code&gt; 文档。&lt;/p&gt;
&lt;p&gt;在选择使用这个查询解析器之前，请考虑以下 3 点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果你准备以编程的方式生成一个查询字符串，然后使用查询解析器来解析它。那么，你应该认真考虑一下是否应该直接使用查询 API 来构建查询。换句话说，查询解析器专门用于人类输入的文本，而不是程序生成的文本。&lt;/li&gt;
&lt;li&gt;没有被识别为token的域最好直接添加到查询中，而不是通过查询解析器来解析。如果一个域的值是通过应用自动生成的，那么应该为这个域自动生成查询子句。查询解析器所使用的分析器是专门用于将人类输入的文本转换成词（terms）的。由程序生成的值，如日期、关键字等等，也应该由程序添加到查询中。&lt;/li&gt;
&lt;li&gt;从查询形式来看，如果域的值是普通文本，则应该使用查询解析器。所有其它值类型，比如：日期范围、关键字等等，最好通过查询 API 直接添加。如果一个域的值仅限于一个有限的集合（可以通过一个下拉菜单指定），则不应该添加到查询字符串（后续会被解析）中，而是应该作为一个 TermQuery 子句添加到查询中。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;词terms&#34;&gt;词（Terms）&lt;/h2&gt;
&lt;p&gt;一个查询语句可以拆解成 词（terms） 和 操作符（operators）。词又分为两种：单个词（single Terms）和短语（Phrases）。&lt;/p&gt;
&lt;p&gt;单个词是指 ”test“ 或 ”Hello“ 这类单词。&lt;/p&gt;
&lt;p&gt;短语是指以双引号包围起来的一组单词，比如：”hello dolly“。&lt;/p&gt;
&lt;p&gt;多个词（Multiple terms）可以使用布尔操作符组合在一起，实现一个更加复杂的查询（如下文所示）。&lt;/p&gt;
&lt;p&gt;备注：用于创建索引的解析器也会用于解析查询字符串中的词和短语。因此，选择合适的解析器很重要，否则解析器可能会被查询字符串中的词干扰。&lt;/p&gt;
&lt;h2 id=&#34;域fields&#34;&gt;域（Fields）&lt;/h2&gt;
&lt;p&gt;Lucene 支持分多个字段/域的数据。搜索时，可以指定一个域，也可以使用默认域。域的名称以及默认域与具体实现相关。&lt;/p&gt;
&lt;p&gt;输入域的名称，后跟一个冒号（:），再跟目标搜索词，即可对任意一个域进行搜索。&lt;/p&gt;
&lt;p&gt;举例来说，假设一个 Lucene 索引包含 2 个域：title 和 text，text 是默认域。若想查找标题为 ”The Right Way“ 且文本内容包含 ”don&amp;rsquo;t go this way“ 的文档，可以输入：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;title:&amp;#34;The Right Way&amp;#34; AND text:go
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;title:&amp;#34;The Right Way&amp;#34; AND go
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为 text 是默认域，所以域标识符可以省略。&lt;/p&gt;
&lt;p&gt;注意：指定的域仅对紧跟其后的词生效，因此，如下查询：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;title:The Right Way
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将对 title 域仅查找 ”The“，并对默认域（当前这个例子中是指 text 域）查找 ”Right“ 和 ”Way“。&lt;/p&gt;
&lt;h2 id=&#34;词修饰语term-modifiers&#34;&gt;词修饰语（Term Modifiers）&lt;/h2&gt;
&lt;p&gt;Lucene 支持修饰查询词（modifying query terms）来提供多种搜索方式。&lt;/p&gt;
&lt;h3 id=&#34;通配符搜索&#34;&gt;&lt;em&gt;通配符搜索&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Lucene 支持对单个词(single terms)（不是短语查询 phrase queries）进行单个字符和多个字符的通配搜索。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;?&lt;/code&gt; 符号进行单个字符的通配搜索。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;*&lt;/code&gt; 符号进行多个字符的通配搜索。&lt;/p&gt;
&lt;p&gt;单字符通配搜索用于查找替换单个字符即可匹配的词。举例来说，若要搜索 ”text“ 或 ”test“，可以如下查询：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;te?t
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;多字符通配搜索用于查找替换0个或多个字符即可匹配的词。举例来说，若要搜索 ”test“、”tests“ 或 ”tester“，可以如下查询：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;test*
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;也可以对词的中间部分进行通配搜索：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;te*t
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;备注： &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;?&lt;/code&gt; 符号不能用作搜索语句的第一个字符。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;正则表达式搜索&#34;&gt;&lt;em&gt;正则表达式搜索&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Lucene 支持正则表达式搜索，匹配斜杠（&lt;code&gt;/&lt;/code&gt;） 之间的模式。正则表达式的语法在不同的发布版本之间可能会有差异，目前支持的语法在 &lt;a class=&#34;link&#34; href=&#34;http://lucene.apache.org/core/8_2_0/core/org/apache/lucene/util/automaton/RegExp.html?is-external=true&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RegExp&lt;/a&gt; 类文档中有说明。举例来说，查找包含 ”moat“ 或 ”boat“ 的文档：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/[mb]oat/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;模糊搜索&#34;&gt;&lt;em&gt;模糊搜索&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Lucene 支持基于 Damerau-Levenshtein 编辑距离的模糊搜索。在单个词的最后添加波浪符（~）即可进行模糊搜索。举例来说，使用模糊搜索查找在拼写上与 ”roam“ 近似的词：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;roam~
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个查询语句会找到 foam 和 roams 这类词。&lt;/p&gt;
&lt;p&gt;模糊搜索可以通过一个额外（可选）的参数来指定允许的最大编辑次数。这个参数值界于 0 和 2 之间，例如：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;roam~1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果未指定该参数，则默认使用 2 个编辑距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;以前，这里还允许使用浮点数。现在这个语法已被考虑弃用，将于 Lucene 5.0 中移除。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;邻近搜索&#34;&gt;&lt;em&gt;邻近搜索&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Lucene 支持查找指定距离的邻近词。在短语的最后添加拨浪符（~）即可进行邻近搜索。举例来说，在文档中搜索 ”apache“ 和 ”jakarta“ 相距 10 个词的模式：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;jakarta apache&amp;#34;~10
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;范围搜索&#34;&gt;&lt;em&gt;范围搜索&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;范围查询可以匹配到域的值在范围查询语句指定的上下界之间的所有文档。对于上下界的值，范围查询可以包含也可以不包含。排序是按照字典序进行的。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mod_date:[20020101 TO 20030101]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个查询语句会查找 mod_date 域的值在 20020101 和 20030101 （包含上下界） 之间的文档。注意：范围查询对非日期的域也可以使用：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;title:{Aida TO Carmen}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个查询语句能查找到 title 域的值在 Aida 和 Carmen （不包含上下界）之间的所有文档。&lt;/p&gt;
&lt;p&gt;中括号表示范围查询包含上下界，花括号表示范围查询不包含上下界。&lt;/p&gt;
&lt;h3 id=&#34;相关性查询boosting-a-term&#34;&gt;&lt;em&gt;相关性查询（Boosting a term）&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Lucene 会基于文档中找到的词对匹配到的文档设置相关性的级别。可以在目标搜索词之后紧接一个脱字符 “^”，后跟一个加权系数（一个数字）来提升该搜索词的相关性权重。加权系数越高，查询命中的文档与该词的相关性越强。&lt;/p&gt;
&lt;p&gt;你可以通过对某词进行加权来控制文档的相关性。例如，假设你正在搜索：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;jakarta apache
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后希望搜索结果和词 ”jakarta“ 的相关性更强一些，则可以使用 ”^“ 符号后跟一个加权系数对这个词进行加权，即如下这样查询：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;jakarta^4 apache
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这会使得查找到的文档和词 ”jakarta“ 看起来相关性更强一些。你也可以对短语进行加权，如下所示：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;jakarta apache&amp;#34;^4 &amp;#34;Apache Lucene&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;默认加权系数是 1。加权系统可以小于 1（比如：0.2），但必须大于 0。&lt;/p&gt;
&lt;h2 id=&#34;布尔操作符&#34;&gt;布尔操作符&lt;/h2&gt;
&lt;p&gt;布尔操作符允许使用逻辑操作符组合多个词。Lucene 支持的布尔操作符包含 &lt;code&gt;AND&lt;/code&gt;、&lt;code&gt;+&lt;/code&gt;、&lt;code&gt;OR&lt;/code&gt;、&lt;code&gt;NOT&lt;/code&gt; 及 &lt;code&gt;-&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（备注：布尔操作符必须全部是大写字母）。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;or&#34;&gt;&lt;em&gt;OR&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;“OR” 操作符是默认的连接操作符。这意味着如果两个词之间没有布尔操作符，则lucene会使用 “OR” 操作符。OR 操作符链接两个词，并匹配包含其中任意一个词的文档。这相当于集合的并集操作。“||” 符合可用于替代单词 “OR”。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索包含 “jakarta apache” 或仅是 “jakarta” 的文档：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;jakarta apache&amp;#34; jakarta
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;jakarta apache&amp;#34; OR jakarta
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;and&#34;&gt;AND&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;AND&amp;rdquo; 操作符会匹配文本内容中同时存在两个要查询的词（因为 AND 是二元操作符）的文档。这相当于集合的交集操作。“&amp;amp;&amp;amp;” 符号可用于替代单词 “AND”。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索包含 “jakarta apache” 和 “Apache Lucene” 的文档：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;jakarta apache&amp;#34; AND “Apache Lucene”
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;heading&#34;&gt;+&lt;/h3&gt;
&lt;p&gt;“+”（必需）操作符要求文档的某个域中包含 “+” 符号之后的词。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索（必须）包含 “jakarta” 以及可能包含 “lucene”（包不包含都可以）的文档：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;+jakarta lucene
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;not&#34;&gt;NOT&lt;/h3&gt;
&lt;p&gt;”NOT“ 操作符会排除包含”NOT“之后的词的文档。这相当于集合的差集操作。也可以用”!“ 符号代替 ”NOT“。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句搜索包含 ”jakarta apache“ 但不包含 ”Apache Lucene“ 的文档”：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;jakarta apache&amp;#34; NOT &amp;#34;Apache Lucene&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;备注：“NOT” 操作符不可以用于单个词。例如，如下搜索不会返回任何结果：&lt;/strong&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;NOT &amp;#34;jakarta apache&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;-&#34;&gt;-&lt;/h3&gt;
&lt;p&gt;”-“（禁止）操作符会排除包含”-“符号之后的词的文档。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来查询包含 ”jakarta apache“ 但不包含 ”Apache Lucene“ 的文档：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;jakarta apache&amp;#34; -&amp;#34;Apache Lucene&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;分组&#34;&gt;分组&lt;/h2&gt;
&lt;p&gt;Lucene 支持使用圆括号对子句进行分组，构成子查询。这对于控制一个查询语句的布尔逻辑非常有用。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索包含 “jakarta” 或 “apache”，同时包含 “website” 的文档：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(jakarta OR apache) AND website
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样查询语句就没有了歧义：必须包含 ”website“，同时包含“jakarta” 或 ”apache“其中之一。&lt;/p&gt;
&lt;h2 id=&#34;域分组&#34;&gt;域分组&lt;/h2&gt;
&lt;p&gt;Lucene 支持使用圆括号对单个域的多个子句进行分组。&lt;/p&gt;
&lt;p&gt;例如，若想搜索一个 title 中包含单词“return”同时包含短语“pink panther”，可以使用如下查询：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;title:(+return +&amp;#34;pink panther&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;特殊字符转义&#34;&gt;特殊字符转义&lt;/h2&gt;
&lt;p&gt;Lucene 支持对查询语法使用的特殊字符进行转义。目前这些特殊字符如下列表所示：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;+ - &amp;amp;&amp;amp; || ! ( ) { } [ ] ^ &amp;#34; ~ * ? : \ /
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在特殊字符之前加 &lt;code&gt;\&lt;/code&gt; 来转义。例如，使用如下查询语句来搜索 &lt;code&gt;(1+1):2&lt;/code&gt;：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;\(1\+1\)\:2
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>Lucene 概述</title>
        <link>https://lxb.wiki/6ddc4d41/</link>
        <pubDate>Tue, 25 Apr 2023 20:54:13 +0800</pubDate>
        
        <guid>https://lxb.wiki/6ddc4d41/</guid>
        <description>&lt;p&gt;Lucene是apache软件基金会4 jakarta项目组的一个子项目，是一个开放源代码的全文检索引擎工具包，但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎（英文与德文两种西方语言）。Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎&lt;/p&gt;
&lt;h2 id=&#34;全文检索概述&#34;&gt;全文检索概述&lt;/h2&gt;
&lt;p&gt;比如，我们一个文件夹中，或者一个磁盘中有很多的文件，记事本、world、Excel、pdf，我们想根据其中的关键词搜索包含的文件。例如，我们输入Lucene，所有内容含有Lucene的文件就会被检查出来。这就是所谓的全文检索。&lt;/p&gt;
&lt;p&gt;因此，很容易的我们想到，应该建立一个关键字与文件的相关映射，盗用ppt中的一张图，很明白的解释了这种映射如何实现。&lt;/p&gt;
&lt;h3 id=&#34;倒排索引&#34;&gt;倒排索引&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252056096.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;有了这种映射关系，我们就来看看Lucene的架构设计。 下面是Lucene的资料必出现的一张图，但也是其精髓的概括。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxbwolf/blog_source_image/main/202402252056599.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;可以看到，Lucene的使用主要体现在两个步骤：&lt;/p&gt;
&lt;p&gt;1 创建索引，通过IndexWriter对不同的文件进行索引的创建，并将其保存在索引相关&lt;a class=&#34;link&#34; href=&#34;https://cloud.tencent.com/product/cfs?from_column=20065&amp;amp;from=20065&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;文件存储&lt;/a&gt;的位置中。&lt;/p&gt;
&lt;p&gt;2 通过索引查寻关键字相关文档。&lt;/p&gt;
&lt;p&gt;在Lucene中，就是使用这种“倒排索引”的技术，来实现相关映射。&lt;/p&gt;
&lt;h2 id=&#34;lucene数学模型&#34;&gt;Lucene数学模型&lt;/h2&gt;
&lt;p&gt;文档、域、词元&lt;/p&gt;
&lt;p&gt;文档是Lucene搜索和索引的原子单位，文档为包含一个或者多个域的&lt;a class=&#34;link&#34; href=&#34;https://cloud.tencent.com/product/tke?from_column=20065&amp;amp;from=20065&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;容器&lt;/a&gt;，而域则是依次包含“真正的”被搜索的内容，域值通过分词技术处理，得到多个词元。&lt;/p&gt;
&lt;p&gt;For Example，一篇小说（斗破苍穹）信息可以称为一个文档，小说信息又包含多个域，例如：标题（斗破苍穹）、作者、简介、最后更新时间等等，对标题这个域采用分词技术又可以得到一个或者多个词元（斗、破、苍、穹）。&lt;/p&gt;
&lt;h2 id=&#34;lucene文件结构&#34;&gt;Lucene文件结构&lt;/h2&gt;
&lt;h3 id=&#34;层次结构&#34;&gt;层次结构&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;index&lt;/strong&gt; 一个索引存放在一个目录中&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;segment&lt;/strong&gt; 一个索引中可以有多个段，段与段之间是独立的，添加新的文档可能产生新段，不同的段可以合并成一个新段&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;document&lt;/strong&gt; 文档是创建索引的基本单位，不同的文档保存在不同的段中，一个段可以包含多个文档&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;field&lt;/strong&gt; 域，一个文档包含不同类型的信息，可以拆分开索引&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;term&lt;/strong&gt; 词，索引的最小单位，是经过词法分析和语言处理后的数据。&lt;/p&gt;
&lt;h3 id=&#34;正向信息&#34;&gt;正向信息&lt;/h3&gt;
&lt;p&gt;按照层次依次保存了从索引到词的包含关系：index–&amp;gt;segment–&amp;gt;document–&amp;gt;field–&amp;gt;term。&lt;/p&gt;
&lt;h3 id=&#34;反向信息&#34;&gt;反向信息&lt;/h3&gt;
&lt;p&gt;反向信息保存了词典的倒排表映射：term–&amp;gt;document&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IndexWriter&lt;/strong&gt; lucene中最重要的的类之一，它主要是用来将文档加入索引，同时控制索引过程中的一些参数使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Analyzer&lt;/strong&gt; 分析器,主要用于分析搜索引擎遇到的各种文本。常用的有StandardAnalyzer分析器,StopAnalyzer分析器,WhitespaceAnalyzer分析器等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Directory&lt;/strong&gt; 索引存放的位置;lucene提供了两种索引存放的位置，一种是磁盘，一种是内存。一般情况将索引放在磁盘上；相应地lucene提供了FSDirectory和RAMDirectory两个类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Document&lt;/strong&gt; 文档;Document相当于一个要进行索引的单元，任何可以想要被索引的文件都必须转化为Document对象才能进行索引。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Field&lt;/strong&gt; 字段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IndexSearcher&lt;/strong&gt; 是lucene中最基本的检索工具，所有的检索都会用到IndexSearcher工具;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt; 查询，lucene中支持模糊查询，语义查询，短语查询，组合查询等等,如有TermQuery,BooleanQuery,RangeQuery,WildcardQuery等一些类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;QueryParser&lt;/strong&gt; 是一个解析用户输入的工具，可以通过扫描用户输入的字符串，生成Query对象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hits&lt;/strong&gt; 在搜索完成之后，需要把搜索结果返回并显示给用户，只有这样才算是完成搜索的目的。在lucene中，搜索的结果的集合是用Hits类的实例来表示的。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
