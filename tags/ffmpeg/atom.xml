<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>ffmpeg on Xiaobin&#39;s Notes</title>
        <link>https://lxb.wiki/tags/ffmpeg/</link>
        <description>Recent content in ffmpeg on Xiaobin&#39;s Notes</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Tue, 27 Mar 2018 14:41:52 +0000</lastBuildDate><atom:link href="https://lxb.wiki/tags/ffmpeg/atom.xml" rel="self" type="application/rss+xml" /><item>
        <title>ffmpeg 视音频同步</title>
        <link>https://lxb.wiki/aeb01c06/</link>
        <pubDate>Tue, 27 Mar 2018 14:41:52 +0000</pubDate>
        
        <guid>https://lxb.wiki/aeb01c06/</guid>
        <description>&lt;p&gt;原文地址: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/nonmarking/article/details/50522413&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/nonmarking/article/details/50522413&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;对于直播流来说, 只考虑发送端的同步问题, 原理如下: 1. 解析视音频, 讲视频流和音频流的时间戳用同样的时间基准表示 2. 比较转换后的两个时间戳, 找出较小值, 对应发送偏慢的流 3. 读取, 转码, 发送相应的流, 同时, 若该流的转码时间很快, 超前于wall clock, 则还需要进行相应的延时 4. 重复以上过程&lt;/p&gt;
&lt;p&gt;下文包括两部分, 一是音频转码部分, 二是视音频同步&lt;/p&gt;
&lt;h6 id=&#34;音频转码基本流程&#34;&gt;音频转码基本流程&lt;/h6&gt;
&lt;p&gt;首先是一些音频输入输出的基本设置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//Set own audio device&#39;s name
    if (avformat_open_input(&amp;amp;ifmt_ctx_a, device_name_a, ifmt, &amp;amp;device_param) != 0){

        printf(&amp;quot;Couldn&#39;t open input audio stream.（无法打开输入流）\n&amp;quot;);
        return -1;
    }
……
//input audio initialize
    if (avformat_find_stream_info(ifmt_ctx_a, NULL) &amp;lt; 0)
    {
        printf(&amp;quot;Couldn&#39;t find audio stream information.（无法获取流信息）\n&amp;quot;);
        return -1;
    }
    audioindex = -1;
    for (i = 0; i &amp;lt; ifmt_ctx_a-&amp;gt;nb_streams; i++)
    if (ifmt_ctx_a-&amp;gt;streams[i]-&amp;gt;codec-&amp;gt;codec_type == AVMEDIA_TYPE_AUDIO)
    {
        audioindex = i;
        break;
    }
    if (audioindex == -1)
    {
        printf(&amp;quot;Couldn&#39;t find a audio stream.（没有找到视频流）\n&amp;quot;);
        return -1;
    }
    if (avcodec_open2(ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec, avcodec_find_decoder(ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec-&amp;gt;codec_id), NULL) &amp;lt; 0)
    {
        printf(&amp;quot;Could not open audio codec.（无法打开解码器）\n&amp;quot;);
        return -1;
    }
……
 //output audio encoder initialize
    pCodec_a = avcodec_find_encoder(AV_CODEC_ID_AAC);
    if (!pCodec_a){
        printf(&amp;quot;Can not find output audio encoder! (没有找到合适的编码器！)\n&amp;quot;);
        return -1;
    }
    pCodecCtx_a = avcodec_alloc_context3(pCodec_a);
    pCodecCtx_a-&amp;gt;channels = 2;
    pCodecCtx_a-&amp;gt;channel_layout = av_get_default_channel_layout(2);
    pCodecCtx_a-&amp;gt;sample_rate = ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec-&amp;gt;sample_rate;
    pCodecCtx_a-&amp;gt;sample_fmt = pCodec_a-&amp;gt;sample_fmts[0];
    pCodecCtx_a-&amp;gt;bit_rate = 32000;
    pCodecCtx_a-&amp;gt;time_base.num = 1;
    pCodecCtx_a-&amp;gt;time_base.den = pCodecCtx_a-&amp;gt;sample_rate;
    /** Allow the use of the experimental AAC encoder */
    pCodecCtx_a-&amp;gt;strict_std_compliance = FF_COMPLIANCE_EXPERIMENTAL;
    /* Some formats want stream headers to be separate. */
    if (ofmt_ctx-&amp;gt;oformat-&amp;gt;flags &amp;amp; AVFMT_GLOBALHEADER)
        pCodecCtx_a-&amp;gt;flags |= CODEC_FLAG_GLOBAL_HEADER;
    if (avcodec_open2(pCodecCtx_a, pCodec_a, NULL) &amp;lt; 0){
        printf(&amp;quot;Failed to open ouput audio encoder! (编码器打开失败！)\n&amp;quot;);
        return -1;
    }

    //Add a new stream to output,should be called by the user before avformat_write_header() for muxing
    audio_st = avformat_new_stream(ofmt_ctx, pCodec_a);
    if (audio_st == NULL){
        return -1;
    }
    audio_st-&amp;gt;time_base.num = 1;
    audio_st-&amp;gt;time_base.den = pCodecCtx_a-&amp;gt;sample_rate;
    audio_st-&amp;gt;codec = pCodecCtx_a;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接下来, 考虑到输入音频的sample format 可能需要进行转换, 需要用到swresample库的功能 先做好相应的初始化&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Initialize the resampler to be able to convert audio sample formats
    aud_convert_ctx = swr_alloc_set_opts(NULL,
        av_get_default_channel_layout(pCodecCtx_a-&amp;gt;channels),
        pCodecCtx_a-&amp;gt;sample_fmt,
        pCodecCtx_a-&amp;gt;sample_rate,
        av_get_default_channel_layout(ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec-&amp;gt;channels),
        ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec-&amp;gt;sample_fmt,
        ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec-&amp;gt;sample_rate,
        0, NULL);
swr_init(aud_convert_ctx);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此外, 参照transcode_aac.c的做法, 使用FIFO buffer存储从输入端解码得到的音频采样数据, 这些数据在后续将转换sample format并进行编码, 由此即完成了一个音频转码功.&lt;/p&gt;
&lt;p&gt;此外, 还需要另外的一个buffer来存储转换合适之后的音频数据&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//Initialize the FIFO buffer to store audio samples to be encoded. 
    AVAudioFifo *fifo = NULL;
    fifo = av_audio_fifo_alloc(pCodecCtx_a-&amp;gt;sample_fmt, pCodecCtx_a-&amp;gt;channels, 1);

    //Initialize the buffer to store converted samples to be encoded.
    uint8_t **converted_input_samples = NULL;
    /**
    * Allocate as many pointers as there are audio channels.
    * Each pointer will later point to the audio samples of the corresponding
    * channels (although it may be NULL for interleaved formats).
    */
    if (!(converted_input_samples = (uint8_t**)calloc(pCodecCtx_a-&amp;gt;channels,
        sizeof(**converted_input_samples)))) {
        printf(&amp;quot;Could not allocate converted input sample pointers\n&amp;quot;);
        return AVERROR(ENOMEM);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;至此, 一些基本的初始化工作完成.&lt;/p&gt;
&lt;p&gt;音频计算pts的方法和视频类似. 即先通过sample rate算出每两个音频sample之间的时间间隔, 再通过计数当前已编码的音频sample总数(nb_samples变量的作用) 来算出当前编码音频帧的时间戳. 如果和视频的流程做类比, 大概为: framerate 相当于sample rate, framecnt相当于nb_samples.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//audio trancoding here
        const int output_frame_size = pCodecCtx_a-&amp;gt;frame_size;

        /**
        * Make sure that there is one frame worth of samples in the FIFO
        * buffer so that the encoder can do its work.
        * Since the decoder&#39;s and the encoder&#39;s frame size may differ, we
        * need to FIFO buffer to store as many frames worth of input samples
        * that they make up at least one frame worth of output samples.
        */
        while (av_audio_fifo_size(fifo) &amp;lt; output_frame_size) {
            /**
            * Decode one frame worth of audio samples, convert it to the
            * output sample format and put it into the FIFO buffer.
            */
            AVFrame *input_frame = av_frame_alloc();
            if (!input_frame)
            {
                ret = AVERROR(ENOMEM);
                return ret;
            }           

            /** Decode one frame worth of audio samples. */
            /** Packet used for temporary storage. */
            AVPacket input_packet;
            av_init_packet(&amp;amp;input_packet);
            input_packet.data = NULL;
            input_packet.size = 0;

            /** Read one audio frame from the input file into a temporary packet. */
            if ((ret = av_read_frame(ifmt_ctx_a, &amp;amp;input_packet)) &amp;lt; 0) {
                /** If we are at the end of the file, flush the decoder below. */
                if (ret == AVERROR_EOF)
                {
                    encode_audio = 0;
                }
                else
                {
                    printf(&amp;quot;Could not read audio frame\n&amp;quot;);
                    return ret;
                }                   
            }

            /**
            * Decode the audio frame stored in the temporary packet.
            * The input audio stream decoder is used to do this.
            * If we are at the end of the file, pass an empty packet to the decoder
            * to flush it.
            */
            if ((ret = avcodec_decode_audio4(ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec, input_frame,
                &amp;amp;dec_got_frame_a, &amp;amp;input_packet)) &amp;lt; 0) {
                printf(&amp;quot;Could not decode audio frame\n&amp;quot;);
                return ret;
            }
            av_packet_unref(&amp;amp;input_packet);
            /** If there is decoded data, convert and store it */
            if (dec_got_frame_a) {
                /**
                * Allocate memory for the samples of all channels in one consecutive
                * block for convenience.
                */
                if ((ret = av_samples_alloc(converted_input_samples, NULL,
                    pCodecCtx_a-&amp;gt;channels,
                    input_frame-&amp;gt;nb_samples,
                    pCodecCtx_a-&amp;gt;sample_fmt, 0)) &amp;lt; 0) {
                    printf(&amp;quot;Could not allocate converted input samples\n&amp;quot;);
                    av_freep(&amp;amp;(*converted_input_samples)[0]);
                    free(*converted_input_samples);
                    return ret;
                }

                /**
                * Convert the input samples to the desired output sample format.
                * This requires a temporary storage provided by converted_input_samples.
                */
                /** Convert the samples using the resampler. */
                if ((ret = swr_convert(aud_convert_ctx,
                    converted_input_samples, input_frame-&amp;gt;nb_samples,
                    (const uint8_t**)input_frame-&amp;gt;extended_data, input_frame-&amp;gt;nb_samples)) &amp;lt; 0) {
                    printf(&amp;quot;Could not convert input samples\n&amp;quot;);
                    return ret;
                }

                /** Add the converted input samples to the FIFO buffer for later processing. */
                /**
                * Make the FIFO as large as it needs to be to hold both,
                * the old and the new samples.
                */
                if ((ret = av_audio_fifo_realloc(fifo, av_audio_fifo_size(fifo) + input_frame-&amp;gt;nb_samples)) &amp;lt; 0) {
                    printf(&amp;quot;Could not reallocate FIFO\n&amp;quot;);
                    return ret;
                }

                /** Store the new samples in the FIFO buffer. */
                if (av_audio_fifo_write(fifo, (void **)converted_input_samples,
                    input_frame-&amp;gt;nb_samples) &amp;lt; input_frame-&amp;gt;nb_samples) {
                    printf(&amp;quot;Could not write data to FIFO\n&amp;quot;);
                    return AVERROR_EXIT;
                }               
            }
        }

        /**
        * If we have enough samples for the encoder, we encode them.
        * At the end of the file, we pass the remaining samples to
        * the encoder.
        */
        if (av_audio_fifo_size(fifo) &amp;gt;= output_frame_size)
            /**
            * Take one frame worth of audio samples from the FIFO buffer,
            * encode it and write it to the output file.
            */
        {
            /** Temporary storage of the output samples of the frame written to the file. */
            AVFrame *output_frame=av_frame_alloc();
            if (!output_frame)
            {
                ret = AVERROR(ENOMEM);
                return ret;
            }
            /**
            * Use the maximum number of possible samples per frame.
            * If there is less than the maximum possible frame size in the FIFO
            * buffer use this number. Otherwise, use the maximum possible frame size
            */
            const int frame_size = FFMIN(av_audio_fifo_size(fifo),
                pCodecCtx_a-&amp;gt;frame_size);

            /** Initialize temporary storage for one output frame. */
            /**
            * Set the frame&#39;s parameters, especially its size and format.
            * av_frame_get_buffer needs this to allocate memory for the
            * audio samples of the frame.
            * Default channel layouts based on the number of channels
            * are assumed for simplicity.
            */
            output_frame-&amp;gt;nb_samples = frame_size;
            output_frame-&amp;gt;channel_layout = pCodecCtx_a-&amp;gt;channel_layout;
            output_frame-&amp;gt;format = pCodecCtx_a-&amp;gt;sample_fmt;
            output_frame-&amp;gt;sample_rate = pCodecCtx_a-&amp;gt;sample_rate;

            /**
            * Allocate the samples of the created frame. This call will make
            * sure that the audio frame can hold as many samples as specified.
            */
            if ((ret = av_frame_get_buffer(output_frame, 0)) &amp;lt; 0) {
                printf(&amp;quot;Could not allocate output frame samples\n&amp;quot;);
                av_frame_free(&amp;amp;output_frame);
                return ret;
            }

            /**
            * Read as many samples from the FIFO buffer as required to fill the frame.
            * The samples are stored in the frame temporarily.
            */
            if (av_audio_fifo_read(fifo, (void **)output_frame-&amp;gt;data, frame_size) &amp;lt; frame_size) {
                printf(&amp;quot;Could not read data from FIFO\n&amp;quot;);
                return AVERROR_EXIT;
            }

            /** Encode one frame worth of audio samples. */
            /** Packet used for temporary storage. */
            AVPacket output_packet;
            av_init_packet(&amp;amp;output_packet);
            output_packet.data = NULL;
            output_packet.size = 0;

            /** Set a timestamp based on the sample rate for the container. */
            if (output_frame) {
                nb_samples += output_frame-&amp;gt;nb_samples;
            }

            /**
            * Encode the audio frame and store it in the temporary packet.
            * The output audio stream encoder is used to do this.
            */
            if ((ret = avcodec_encode_audio2(pCodecCtx_a, &amp;amp;output_packet,
                output_frame, &amp;amp;enc_got_frame_a)) &amp;lt; 0) {
                printf(&amp;quot;Could not encode frame\n&amp;quot;);
                av_packet_unref(&amp;amp;output_packet);
                return ret;
            }

            /** Write one audio frame from the temporary packet to the output file. */
            if (enc_got_frame_a) {

                output_packet.stream_index = 1;

                AVRational time_base = ofmt_ctx-&amp;gt;streams[1]-&amp;gt;time_base;
                AVRational r_framerate1 = { ifmt_ctx_a-&amp;gt;streams[audioindex]-&amp;gt;codec-&amp;gt;sample_rate, 1 };// { 44100, 1};  
                int64_t calc_duration = (double)(AV_TIME_BASE)*(1 / av_q2d(r_framerate1));  //内部时间戳  

                output_packet.pts = av_rescale_q(nb_samples*calc_duration, time_base_q, time_base);
                output_packet.dts = output_packet.pts;
                output_packet.duration = output_frame-&amp;gt;nb_samples;

                //printf(&amp;quot;audio pts : %d\n&amp;quot;, output_packet.pts);
                aud_next_pts = nb_samples*calc_duration;

                int64_t pts_time = av_rescale_q(output_packet.pts, time_base, time_base_q);
                int64_t now_time = av_gettime() - start_time;

                if ((pts_time &amp;gt; now_time) &amp;amp;&amp;amp; ((aud_next_pts + pts_time - now_time)&amp;lt;vid_next_pts))
                    av_usleep(pts_time - now_time);

                if ((ret = av_interleaved_write_frame(ofmt_ctx, &amp;amp;output_packet)) &amp;lt; 0) {
                    printf(&amp;quot;Could not write frame\n&amp;quot;);
                    av_packet_unref(&amp;amp;output_packet);
                    return ret;
                }

                av_packet_unref(&amp;amp;output_packet);
            }           
            av_frame_free(&amp;amp;output_frame);       
        }     
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;视音频同步&#34;&gt;视音频同步&lt;/h6&gt;
&lt;p&gt;首先定义几个变量&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    int aud_next_pts = 0;//视频流目前的pts,可以理解为目前的进度
    int vid_next_pts = 0;//音频流目前的pts
    int encode_video = 1, encode_audio = 1;//是否要编码视频、音频
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;则相应的视音频同步方法如下: 1. 确定视频, 音频二者中至少有一个是需要进行转码的 2. 比较两个流的进度, 使用av_compare_ts函数, 注意：此时的vid_next_pts和aud_next_pts的time base都是ffmpeg内部基准，即&lt;code&gt;AVRational time_base_q = { 1, AV_TIME_BASE };&lt;/code&gt; 3. 对进度落后的流进行转码, 并相应地对进度进行更新. 对于视频，有 vid_next_pts=framecnt_calc_duration;，对于音频，有 aud_next_pts = nb_samples_calc_duration;这里framecnt和nb_samples都相当于计数器，而calc_duration是对应流每两个frame或sample之间的时间间隔，也是以ffmpeg内部时间基准为单位的 4. 若转码进度很快完成, 则不能急于写入输出流, 而是需要先进行延时, 但是也要保证延时后的时间不会超过另一个流的进度&lt;/p&gt;
&lt;p&gt;综上, 流程如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; //start decode and encode
    int64_t start_time = av_gettime();
    while (encode_video || encode_audio)
    {
        if (encode_video &amp;amp;&amp;amp;
            (!encode_audio || av_compare_ts(vid_next_pts, time_base_q,
            aud_next_pts, time_base_q) &amp;lt;= 0))
        {
              进行视频转码；
              转码完成后；
              vid_next_pts=framecnt*calc_duration; //general timebase

                        //Delay
                        int64_t pts_time = av_rescale_q(enc_pkt.pts, time_base, time_base_q);
                        int64_t now_time = av_gettime() - start_time;                       
                        if ((pts_time &amp;gt; now_time) &amp;amp;&amp;amp; ((vid_next_pts + pts_time - now_time)&amp;lt;aud_next_pts))
                            av_usleep(pts_time - now_time);
              写入流；
}
else
{
              进行音频转码；
              转码完成后；
          aud_next_pts = nb_samples*calc_duration;

                int64_t pts_time = av_rescale_q(output_packet.pts, time_base, time_base_q);
                int64_t now_time = av_gettime() - start_time;
                if ((pts_time &amp;gt; now_time) &amp;amp;&amp;amp; ((aud_next_pts + pts_time - now_time)&amp;lt;vid_next_pts))
                    av_usleep(pts_time - now_time);
              写入流；
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;至此, 视音频同步完成. 最后再完成一些flush_encoder的工作即可.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ffmpeg 推流报错</title>
        <link>https://lxb.wiki/eaddfbfe/</link>
        <pubDate>Tue, 27 Mar 2018 14:39:47 +0000</pubDate>
        
        <guid>https://lxb.wiki/eaddfbfe/</guid>
        <description>&lt;p&gt;在使用dshow设备推流时，经常会报出real time buffer too full dropping frames的错误信息，其原因在&lt;a class=&#34;link&#34; href=&#34;https://trac.ffmpeg.org/wiki/DirectShow&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这篇文章&lt;/a&gt;里有写到，可以通过添加rtbufsize参数来解决，码率越高对应的rtbufsize就需要越高，但过高的rtbufsize会带来视频的延时，若要保持同步，可能就需要对音频人为增加一定的延时。而根据我的测试，即使不添加rtbufszie参数，虽然会报出错误信息，但并不影响直播流的观看或录制，而且可以保持同步。这就是一个trade off的问题了。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>基于FFmpeg的摄像头直播(推流)</title>
        <link>https://lxb.wiki/ae1aac27/</link>
        <pubDate>Sat, 13 Jan 2018 20:50:43 +0000</pubDate>
        
        <guid>https://lxb.wiki/ae1aac27/</guid>
        <description>&lt;p&gt;原文地址: &lt;a class=&#34;link&#34; href=&#34;http://blog.csdn.net/wh8_2011/article/details/73506154&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://blog.csdn.net/wh8_2011/article/details/73506154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文实现: 读取PC摄像头视频数据并以RTMP协议发送为直播流. 示例包含 1. FFmpeg的libavdevice的使用 2. 视频编码, 解码, 推流的基本流程&lt;/p&gt;
&lt;p&gt;要使用libavdevice的相关函数, 首先需要注册相关组件 &lt;code&gt;avdevice_register_all()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;列出电脑中可用的DShow设备&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;AVFormatContext *pFmtCtx = avformat_alloc_context();  
    AVDeviceInfoList *device_info = NULL;  
    AVDictionary* options = NULL;  
    av_dict_set(&amp;amp;options, &amp;quot;list_devices&amp;quot;, &amp;quot;true&amp;quot;, 0);  
    AVInputFormat *iformat = av_find_input_format(&amp;quot;dshow&amp;quot;);  
    printf(&amp;quot;Device Info=============\n&amp;quot;);  
    avformat_open_input(&amp;amp;pFmtCtx, &amp;quot;video=dummy&amp;quot;, iformat, &amp;amp;options);  
    printf(&amp;quot;========================\n&amp;quot;); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;也可以直接使用FFmpeg的工具 &lt;code&gt;ffmpeg -list_devices true -f dshow -i dummy&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;PS: avdevice有一个avdevice_list_devices函数可以枚举系统的采集设备, 包括设备名和设备描述, 可以让用户选择要使用的设备, 但是不支持DShow设备.&lt;/p&gt;
&lt;p&gt;像打开普通文件一样将上面的具体设备名作为输入打开, 并进行相应的初始化设置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;av_register_all();  
    //Register Device  
    avdevice_register_all();  
    avformat_network_init();  

    //Show Dshow Device    
    show_dshow_device();  

    printf(&amp;quot;\nChoose capture device: &amp;quot;);  
    if (gets(capture_name) == 0)  
    {  
        printf(&amp;quot;Error in gets()\n&amp;quot;);  
        return -1;  
    }  
    sprintf(device_name, &amp;quot;video=%s&amp;quot;, capture_name);  

    ifmt=av_find_input_format(&amp;quot;dshow&amp;quot;);  

    //Set own video device&#39;s name  
    if (avformat_open_input(&amp;amp;ifmt_ctx, device_name, ifmt, NULL) != 0){  
        printf(&amp;quot;Couldn&#39;t open input stream.（无法打开输入流）\n&amp;quot;);  
        return -1;  
    }  
    //input initialize  
    if (avformat_find_stream_info(ifmt_ctx, NULL)&amp;lt;0)  
    {  
        printf(&amp;quot;Couldn&#39;t find stream information.（无法获取流信息）\n&amp;quot;);  
        return -1;  
    }  
    videoindex = -1;  
    for (i = 0; i&amp;lt;ifmt_ctx-&amp;gt;nb_streams; i++)  
        if (ifmt_ctx-&amp;gt;streams[i]-&amp;gt;codec-&amp;gt;codec_type == AVMEDIA_TYPE_VIDEO)  
        {  
            videoindex = i;  
            break;  
        }  
    if (videoindex == -1)  
    {  
        printf(&amp;quot;Couldn&#39;t find a video stream.（没有找到视频流）\n&amp;quot;);  
        return -1;  
    }  
    if (avcodec_open2(ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;codec, avcodec_find_decoder(ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;codec-&amp;gt;codec_id), NULL)&amp;lt;0)  
    {  
        printf(&amp;quot;Could not open codec.（无法打开解码器）\n&amp;quot;);  
        return -1;  
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输入设备初始化后, 需要对输出做相应的初始化. FFmpeg将网络协议和文件同等看待, 同时因为使用RTMP协议进行传输, 因此制定输出为flv格式, 编码器使用H.264&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//output initialize  
    avformat_alloc_output_context2(&amp;amp;ofmt_ctx, NULL, &amp;quot;flv&amp;quot;, out_path);  
    //output encoder initialize  
    pCodec = avcodec_find_encoder(AV_CODEC_ID_H264);  
    if (!pCodec){  
        printf(&amp;quot;Can not find encoder! (没有找到合适的编码器！)\n&amp;quot;);  
        return -1;  
    }  
    pCodecCtx=avcodec_alloc_context3(pCodec);  
    pCodecCtx-&amp;gt;pix_fmt = PIX_FMT_YUV420P;  
    pCodecCtx-&amp;gt;width = ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;codec-&amp;gt;width;  
    pCodecCtx-&amp;gt;height = ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;codec-&amp;gt;height;  
    pCodecCtx-&amp;gt;time_base.num = 1;  
    pCodecCtx-&amp;gt;time_base.den = 25;  
    pCodecCtx-&amp;gt;bit_rate = 400000;  
    pCodecCtx-&amp;gt;gop_size = 250;  
    /* Some formats,for example,flv, want stream headers to be separate. */  
    if (ofmt_ctx-&amp;gt;oformat-&amp;gt;flags &amp;amp; AVFMT_GLOBALHEADER)  
        pCodecCtx-&amp;gt;flags |= CODEC_FLAG_GLOBAL_HEADER;  

    //H264 codec param  
    //pCodecCtx-&amp;gt;me_range = 16;  
    //pCodecCtx-&amp;gt;max_qdiff = 4;  
    //pCodecCtx-&amp;gt;qcompress = 0.6;  
    pCodecCtx-&amp;gt;qmin = 10;  
    pCodecCtx-&amp;gt;qmax = 51;  
    //Optional Param  
    pCodecCtx-&amp;gt;max_b_frames = 3;  
    // Set H264 preset and tune  
    AVDictionary *param = 0;  
    av_dict_set(&amp;amp;param, &amp;quot;preset&amp;quot;, &amp;quot;fast&amp;quot;, 0);  
    av_dict_set(&amp;amp;param, &amp;quot;tune&amp;quot;, &amp;quot;zerolatency&amp;quot;, 0);  

    if (avcodec_open2(pCodecCtx, pCodec,&amp;amp;param) &amp;lt; 0){  
        printf(&amp;quot;Failed to open encoder! (编码器打开失败！)\n&amp;quot;);  
        return -1;  
    }  

    //Add a new stream to output,should be called by the user before avformat_write_header() for muxing  
    video_st = avformat_new_stream(ofmt_ctx, pCodec);  
    if (video_st == NULL){  
        return -1;  
    }  
    video_st-&amp;gt;time_base.num = 1;  
    video_st-&amp;gt;time_base.den = 25;  
    video_st-&amp;gt;codec = pCodecCtx;  

    //Open output URL,set before avformat_write_header() for muxing  
    if (avio_open(&amp;amp;ofmt_ctx-&amp;gt;pb,out_path, AVIO_FLAG_READ_WRITE) &amp;lt; 0){  
    printf(&amp;quot;Failed to open output file! (输出文件打开失败！)\n&amp;quot;);  
    return -1;  
    }  

    //Show some Information  
    av_dump_format(ofmt_ctx, 0, out_path, 1);  

    //Write File Header  
    avformat_write_header(ofmt_ctx,NULL); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;完成输入和输出的初始化后, 就可以正式开始解码和编码并推流的流程了. 需要注意的是, 摄像头数据往往是RGB格式的, 需要将其转换为YUV420P格式, 才能推流, 因此要先做如下的准备工作&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//prepare before decode and encode  
    dec_pkt = (AVPacket *)av_malloc(sizeof(AVPacket));  
    //enc_pkt = (AVPacket *)av_malloc(sizeof(AVPacket));  
    //camera data has a pix fmt of RGB,convert it to YUV420  
    img_convert_ctx = sws_getContext(ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;codec-&amp;gt;width, ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;codec-&amp;gt;height,   
        ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;codec-&amp;gt;pix_fmt, pCodecCtx-&amp;gt;width, pCodecCtx-&amp;gt;height, PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL);  
    pFrameYUV = avcodec_alloc_frame();  
    uint8_t *out_buffer = (uint8_t *)av_malloc(avpicture_get_size(PIX_FMT_YUV420P, pCodecCtx-&amp;gt;width, pCodecCtx-&amp;gt;height));  
    avpicture_fill((AVPicture *)pFrameYUV, out_buffer, PIX_FMT_YUV420P, pCodecCtx-&amp;gt;width, pCodecCtx-&amp;gt;height);  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在, 就可以正式开始解码, 编码 和推流了&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//start decode and encode  
    int64_t start_time=av_gettime();  
    while (av_read_frame(ifmt_ctx, dec_pkt) &amp;gt;= 0){     
        if (exit_thread)  
            break;  
        av_log(NULL, AV_LOG_DEBUG, &amp;quot;Going to reencode the frame\n&amp;quot;);  
        pframe = av_frame_alloc();  
        if (!pframe) {  
            ret = AVERROR(ENOMEM);  
            return -1;  
        }  
        //av_packet_rescale_ts(dec_pkt, ifmt_ctx-&amp;gt;streams[dec_pkt-&amp;gt;stream_index]-&amp;gt;time_base,  
        //  ifmt_ctx-&amp;gt;streams[dec_pkt-&amp;gt;stream_index]-&amp;gt;codec-&amp;gt;time_base);  
        ret = avcodec_decode_video2(ifmt_ctx-&amp;gt;streams[dec_pkt-&amp;gt;stream_index]-&amp;gt;codec, pframe,  
            &amp;amp;dec_got_frame, dec_pkt);  
        if (ret &amp;lt; 0) {  
            av_frame_free(&amp;amp;pframe);  
            av_log(NULL, AV_LOG_ERROR, &amp;quot;Decoding failed\n&amp;quot;);  
            break;  
        }  
        if (dec_got_frame){  
            sws_scale(img_convert_ctx, (const uint8_t* const*)pframe-&amp;gt;data, pframe-&amp;gt;linesize, 0, pCodecCtx-&amp;gt;height, pFrameYUV-&amp;gt;data, pFrameYUV-&amp;gt;linesize);     

            enc_pkt.data = NULL;  
            enc_pkt.size = 0;  
            av_init_packet(&amp;amp;enc_pkt);  
            ret = avcodec_encode_video2(pCodecCtx, &amp;amp;enc_pkt, pFrameYUV, &amp;amp;enc_got_frame);  
            av_frame_free(&amp;amp;pframe);  
            if (enc_got_frame == 1){  
                //printf(&amp;quot;Succeed to encode frame: %5d\tsize:%5d\n&amp;quot;, framecnt, enc_pkt.size);  
                framecnt++;   
                enc_pkt.stream_index = video_st-&amp;gt;index;  

                //Write PTS  
                AVRational time_base = ofmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;time_base;//{ 1, 1000 };  
                AVRational r_framerate1 = ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;r_frame_rate;// { 50, 2 };  
                AVRational time_base_q = { 1, AV_TIME_BASE };  
                //Duration between 2 frames (us)  
                int64_t calc_duration = (double)(AV_TIME_BASE)*(1 / av_q2d(r_framerate1));  //内部时间戳  
                //Parameters  
                //enc_pkt.pts = (double)(framecnt*calc_duration)*(double)(av_q2d(time_base_q)) / (double)(av_q2d(time_base));  
                enc_pkt.pts = av_rescale_q(framecnt*calc_duration, time_base_q, time_base);  
                enc_pkt.dts = enc_pkt.pts;  
                enc_pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base); //(double)(calc_duration)*(double)(av_q2d(time_base_q)) / (double)(av_q2d(time_base));  
                enc_pkt.pos = -1;  

                //Delay  
                int64_t pts_time = av_rescale_q(enc_pkt.dts, time_base, time_base_q);  
                int64_t now_time = av_gettime() - start_time;  
                if (pts_time &amp;gt; now_time)  
                    av_usleep(pts_time - now_time);  

                ret = av_interleaved_write_frame(ofmt_ctx, &amp;amp;enc_pkt);  
                av_free_packet(&amp;amp;enc_pkt);  
            }  
        }  
        else {  
            av_frame_free(&amp;amp;pframe);  
        }  
        av_free_packet(dec_pkt);  
    }  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解码比较简单, 编码部分需要自己计算PTS, DTS, 比较复杂 这里通过帧率计算PTS和DTS, 首先通过帧率计算两帧之间的时间间隔, 但是要换算&lt;/p&gt;
</description>
        </item>
        <item>
        <title>基于FFmpeg的推送文件到RTMP服务器</title>
        <link>https://lxb.wiki/5722b57a/</link>
        <pubDate>Sat, 13 Jan 2018 11:51:28 +0000</pubDate>
        
        <guid>https://lxb.wiki/5722b57a/</guid>
        <description>&lt;p&gt;原文地址: &lt;a class=&#34;link&#34; href=&#34;http://blog.csdn.net/leixiaohua1020/article/details/39803457&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://blog.csdn.net/leixiaohua1020/article/details/39803457&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;将本地的MOV/AVI/MKV/MP4/FLV等格式的媒体文件， 通过流媒体协议(RTMP, HTTP, UDP, TCP, RTP等)以直播流的形式推送出去.&lt;/p&gt;
&lt;p&gt;在这个推流器的基础上, 可以进行以下几种方式的修改, 实现各式各样的推流器. 例如: * 将输入文件改为网络流URL, 可以显示转流器 * 将输入文件改为回调函数(内存读取)的形式, 可以推送内存中的视频数据 * 将输入文件改为系统设备(通过libavdevice), 同时加上编码的功能, 可以实现实时推流器(现场直播)&lt;/p&gt;
&lt;h3 id=&#34;需要注意的地方&#34;&gt;需要注意的地方&lt;/h3&gt;
&lt;h4 id=&#34;封装格式&#34;&gt;封装格式&lt;/h4&gt;
&lt;p&gt;RTMP采用的封装格式FLV, 因此在指定输出流媒体的时候需要制定其封装格式为&amp;quot;flv&amp;quot;. 同理, 其他流媒体协议也需要指定其封装格式. 例如采用UDP推送流媒体的时候, 可以指定其封装格式为&amp;quot;mpegts&amp;quot;.&lt;/p&gt;
&lt;h4 id=&#34;延时&#34;&gt;延时&lt;/h4&gt;
&lt;p&gt;发送流媒体的数据的时候需要延时. 否则, FFmpeg处理数据速度很快, 瞬间就能把所有的数据发送出去, 流媒体服务器是承受不了的. 因此需要按照视频实际的帧率发送数据. 本文的推流器在视频帧与帧之间采用av_usleep()函数休眠的方式来延迟发送. 这样就可以按照视频的帧率发送数据了, 代码如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//…  
int64_t start_time=av_gettime();  
while (1) {  
//…  
    //Important:Delay  
    if(pkt.stream_index==videoindex){  
        AVRational time_base=ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;time_base;  
        AVRational time_base_q={1,AV_TIME_BASE};  
        int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);  
        int64_t now_time = av_gettime() - start_time;  
        if (pts_time &amp;gt; now_time)  
            av_usleep(pts_time - now_time);  
    }  
//…  
}  
//… 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;ptsdts问题&#34;&gt;PTS/DTS问题&lt;/h4&gt;
&lt;p&gt;没有封装格式的裸流(例如H.264裸流)是不包含PTS, DTS这些参数的. 在发送这种数据的时候, 需要自己计算并写入AVPacket的PTS, DTS, duration等参数.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//FIX：No PTS (Example: Raw H.264)  
//Simple Write PTS  
if(pkt.pts==AV_NOPTS_VALUE){  
    //Write PTS  
    AVRational time_base1=ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;time_base;  
    //Duration between 2 frames (us)  
    int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;r_frame_rate);  
    //Parameters  
    pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE);  
    pkt.dts=pkt.pts;  
    pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE);  
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;sequence&#34;&gt;sequence&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20180113202019583?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbHhid29sZg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;这里写图片描述&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;代码&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/** 
 * 最简单的基于FFmpeg的推流器（推送RTMP） 
 * Simplest FFmpeg Streamer (Send RTMP) 
 *  
 * 雷霄骅 Lei Xiaohua 
 * leixiaohua1020@126.com 
 * 中国传媒大学/数字电视技术 
 * Communication University of China / Digital TV Technology 
 * http://blog.csdn.net/leixiaohua1020 
 *  
 * 本例子实现了推送本地视频至流媒体服务器（以RTMP为例）。 
 * 是使用FFmpeg进行流媒体推送最简单的教程。 
 * 
 * This example stream local media files to streaming media  
 * server (Use RTMP as example).  
 * It&#39;s the simplest FFmpeg streamer. 
 *  
 */  

#include &amp;lt;stdio.h&amp;gt;  

#define __STDC_CONSTANT_MACROS  

#ifdef _WIN32  
//Windows  
extern &amp;quot;C&amp;quot;  
{  
#include &amp;quot;libavformat/avformat.h&amp;quot;  
#include &amp;quot;libavutil/mathematics.h&amp;quot;  
#include &amp;quot;libavutil/time.h&amp;quot;  
};  
#else  
//Linux...  
#ifdef __cplusplus  
extern &amp;quot;C&amp;quot;  
{  
#endif  
#include &amp;lt;libavformat/avformat.h&amp;gt;  
#include &amp;lt;libavutil/mathematics.h&amp;gt;  
#include &amp;lt;libavutil/time.h&amp;gt;  
#ifdef __cplusplus  
};  
#endif  
#endif  

int main(int argc, char* argv[])  
{  
    AVOutputFormat *ofmt = NULL;  
    //输入对应一个AVFormatContext，输出对应一个AVFormatContext  
    //（Input AVFormatContext and Output AVFormatContext）  
    AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL;  
    AVPacket pkt;  
    const char *in_filename, *out_filename;  
    int ret, i;  
    int videoindex=-1;  
    int frame_index=0;  
    int64_t start_time=0;  
    //in_filename  = &amp;quot;cuc_ieschool.mov&amp;quot;;  
    //in_filename  = &amp;quot;cuc_ieschool.mkv&amp;quot;;  
    //in_filename  = &amp;quot;cuc_ieschool.ts&amp;quot;;  
    //in_filename  = &amp;quot;cuc_ieschool.mp4&amp;quot;;  
    //in_filename  = &amp;quot;cuc_ieschool.h264&amp;quot;;  
    in_filename  = &amp;quot;cuc_ieschool.flv&amp;quot;;//输入URL（Input file URL）  
    //in_filename  = &amp;quot;shanghai03_p.h264&amp;quot;;  

    out_filename = &amp;quot;rtmp://localhost/publishlive/livestream&amp;quot;;//输出 URL（Output URL）[RTMP]  
    //out_filename = &amp;quot;rtp://233.233.233.233:6666&amp;quot;;//输出 URL（Output URL）[UDP]  

    av_register_all();  
    //Network  
    avformat_network_init();  
    //输入（Input）  
    if ((ret = avformat_open_input(&amp;amp;ifmt_ctx, in_filename, 0, 0)) &amp;lt; 0) {  
        printf( &amp;quot;Could not open input file.&amp;quot;);  
        goto end;  
    }  
    if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) &amp;lt; 0) {  
        printf( &amp;quot;Failed to retrieve input stream information&amp;quot;);  
        goto end;  
    }  

    for(i=0; i&amp;lt;ifmt_ctx-&amp;gt;nb_streams; i++)   
        if(ifmt_ctx-&amp;gt;streams[i]-&amp;gt;codec-&amp;gt;codec_type==AVMEDIA_TYPE_VIDEO){  
            videoindex=i;  
            break;  
        }  

    av_dump_format(ifmt_ctx, 0, in_filename, 0);  

    //输出（Output）  

    avformat_alloc_output_context2(&amp;amp;ofmt_ctx, NULL, &amp;quot;flv&amp;quot;, out_filename); //RTMP  
    //avformat_alloc_output_context2(&amp;amp;ofmt_ctx, NULL, &amp;quot;mpegts&amp;quot;, out_filename);//UDP  

    if (!ofmt_ctx) {  
        printf( &amp;quot;Could not create output context\n&amp;quot;);  
        ret = AVERROR_UNKNOWN;  
        goto end;  
    }  
    ofmt = ofmt_ctx-&amp;gt;oformat;  
    for (i = 0; i &amp;lt; ifmt_ctx-&amp;gt;nb_streams; i++) {  
        //根据输入流创建输出流（Create output AVStream according to input AVStream）  
        AVStream *in_stream = ifmt_ctx-&amp;gt;streams[i];  
        AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream-&amp;gt;codec-&amp;gt;codec);  
        if (!out_stream) {  
            printf( &amp;quot;Failed allocating output stream\n&amp;quot;);  
            ret = AVERROR_UNKNOWN;  
            goto end;  
        }  
        //复制AVCodecContext的设置（Copy the settings of AVCodecContext）  
        ret = avcodec_copy_context(out_stream-&amp;gt;codec, in_stream-&amp;gt;codec);  
        if (ret &amp;lt; 0) {  
            printf( &amp;quot;Failed to copy context from input to output stream codec context\n&amp;quot;);  
            goto end;  
        }  
        out_stream-&amp;gt;codec-&amp;gt;codec_tag = 0;  
        if (ofmt_ctx-&amp;gt;oformat-&amp;gt;flags &amp;amp; AVFMT_GLOBALHEADER)  
            out_stream-&amp;gt;codec-&amp;gt;flags |= CODEC_FLAG_GLOBAL_HEADER;  
    }  
    //Dump Format------------------  
    av_dump_format(ofmt_ctx, 0, out_filename, 1);  
    //打开输出URL（Open output URL）  
    if (!(ofmt-&amp;gt;flags &amp;amp; AVFMT_NOFILE)) {  
        ret = avio_open(&amp;amp;ofmt_ctx-&amp;gt;pb, out_filename, AVIO_FLAG_WRITE);  
        if (ret &amp;lt; 0) {  
            printf( &amp;quot;Could not open output URL &#39;%s&#39;&amp;quot;, out_filename);  
            goto end;  
        }  
    }  
    //写文件头（Write file header）  
    ret = avformat_write_header(ofmt_ctx, NULL);  
    if (ret &amp;lt; 0) {  
        printf( &amp;quot;Error occurred when opening output URL\n&amp;quot;);  
        goto end;  
    }  

    start_time=av_gettime();  
    while (1) {  
        AVStream *in_stream, *out_stream;  
        //获取一个AVPacket（Get an AVPacket）  
        ret = av_read_frame(ifmt_ctx, &amp;amp;pkt);  
        if (ret &amp;lt; 0)  
            break;  
        //FIX：No PTS (Example: Raw H.264)  
        //Simple Write PTS  
        if(pkt.pts==AV_NOPTS_VALUE){  
            //Write PTS  
            AVRational time_base1=ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;time_base;  
            //Duration between 2 frames (us)  
            int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;r_frame_rate);  
            //Parameters  
            pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE);  
            pkt.dts=pkt.pts;  
            pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE);  
        }  
        //Important:Delay  
        if(pkt.stream_index==videoindex){  
            AVRational time_base=ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;time_base;  
            AVRational time_base_q={1,AV_TIME_BASE};  
            int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);  
            int64_t now_time = av_gettime() - start_time;  
            if (pts_time &amp;gt; now_time)  
                av_usleep(pts_time - now_time);  

        }  

        in_stream  = ifmt_ctx-&amp;gt;streams[pkt.stream_index];  
        out_stream = ofmt_ctx-&amp;gt;streams[pkt.stream_index];  
        /* copy packet */  
        //转换PTS/DTS（Convert PTS/DTS）  
        pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
        pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
        pkt.duration = av_rescale_q(pkt.duration, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base);  
        pkt.pos = -1;  
        //Print to Screen  
        if(pkt.stream_index==videoindex){  
            printf(&amp;quot;Send %8d video frames to output URL\n&amp;quot;,frame_index);  
            frame_index++;  
        }  
        //ret = av_write_frame(ofmt_ctx, &amp;amp;pkt);  
        ret = av_interleaved_write_frame(ofmt_ctx, &amp;amp;pkt);  

        if (ret &amp;lt; 0) {  
            printf( &amp;quot;Error muxing packet\n&amp;quot;);  
            break;  
        }  

        av_free_packet(&amp;amp;pkt);  

    }  
    //写文件尾（Write file trailer）  
    av_write_trailer(ofmt_ctx);  
end:  
    avformat_close_input(&amp;amp;ifmt_ctx);  
    /* close output */  
    if (ofmt_ctx &amp;amp;&amp;amp; !(ofmt-&amp;gt;flags &amp;amp; AVFMT_NOFILE))  
        avio_close(ofmt_ctx-&amp;gt;pb);  
    avformat_free_context(ofmt_ctx);  
    if (ret &amp;lt; 0 &amp;amp;&amp;amp; ret != AVERROR_EOF) {  
        printf( &amp;quot;Error occurred.\n&amp;quot;);  
        return -1;  
    }  
    return 0;  
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>ffmpeg 推流工具</title>
        <link>https://lxb.wiki/b3ed22dc/</link>
        <pubDate>Fri, 12 Jan 2018 19:52:44 +0000</pubDate>
        
        <guid>https://lxb.wiki/b3ed22dc/</guid>
        <description>&lt;p&gt;SRR测试网址 &lt;code&gt;http://www.ossrs.net/srs.release/trunk/research/players/srs_player.html&lt;/code&gt; 获取 &lt;code&gt;git clone https://github.com/ossrs/srs.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;configure make &lt;code&gt;cd srs/trunk&lt;/code&gt; &lt;code&gt;./configure &amp;amp;&amp;amp; make&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;开启服务器 &lt;code&gt;./objs/srs -c conf/srs.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;列出设备 &lt;code&gt;./ffmpeg.exe -list_devices true -f dshow -i dummy&lt;/code&gt; ffmpeg采集摄像头推流 &lt;code&gt;ffmpeg.exe -f dshow -i video=&amp;quot;EasyCamera&amp;quot; -q 4 -s 640*480 -aspect 4:3 -r 10 -vcodec flv -ar 22050 -ab 64k -ac 1 -acodec libmp3lame -threads 4 -f flv rtmp://192.168.1.102/RTMP/RtmpVideo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ffmpeg采集摄像头和麦克风推流 &lt;code&gt;ffmpeg -f dshow -i video=&amp;quot;USB2.0 PC CAMERA&amp;quot; -f dshow -i audio=&amp;quot;麦克风 (2- USB2.0 MIC)&amp;quot; -b:a 600k -ab 128k -f flv rtmp://192.168.1.102/RTMP/RtmpVideo&lt;/code&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
