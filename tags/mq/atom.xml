<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>mq on Xiaobin&#39;s Notes</title>
        <link>https://lxb.wiki/tags/mq/</link>
        <description>Recent content in mq on Xiaobin&#39;s Notes</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sun, 26 Sep 2021 22:01:43 +0000</lastBuildDate><atom:link href="https://lxb.wiki/tags/mq/atom.xml" rel="self" type="application/rss+xml" /><item>
        <title>Kafka 主从同步</title>
        <link>https://lxb.wiki/4c6cb38f/</link>
        <pubDate>Sun, 26 Sep 2021 22:01:43 +0000</pubDate>
        
        <guid>https://lxb.wiki/4c6cb38f/</guid>
        <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Kafka允许topic的分区拥有若干副本，这个数量是可以配置的，你可以为每个topci配置副本的数量。Kafka会自动在每个个副本上备份数据，所以当一个节点down掉时数据依然是可用的。&lt;/p&gt;
&lt;p&gt;Kafka的副本功能不是必须的，你可以配置只有一个副本，这样其实就相当于只有一份数据。&lt;/p&gt;
&lt;p&gt;创建副本的单位是topic的分区，每个分区都有一个leader和零或多个followers.所有的读写操作都由leader处理，一般分区的数量都比broker的数量多的多，各分区的leader均匀的分布在brokers中。所有的followers都复制leader的日志，日志中的消息和顺序都和leader中的一致。flowers向普通的consumer那样从leader那里拉取消息并保存在自己的日志文件中。
许多分布式的消息系统自动的处理失败的请求，它们对一个节点是否
着（alive）”有着清晰的定义。Kafka判断一个节点是否活着有两个条件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接。&lt;/li&gt;
&lt;li&gt;如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;符合以上条件的节点准确的说应该是“同步中的（in sync）”，而不是模糊的说是“活着的”或是“失败的”。Leader会追踪所有“同步中”的节点，一旦一个down掉了，或是卡住了，或是延时太久，leader就会把它移除。至于延时多久算是“太久”，是由参数replica.lag.max.messages决定的，怎样算是卡住了，怎是由参数replica.lag.time.max.ms决定的。
只有当消息被所有的副本加入到日志中时，才算是“committed”，只有committed的消息才会发送给consumer，这样就不用担心一旦leader down掉了消息会丢失。Producer也可以选择是否等待消息被提交的通知，这个是由参数request.required.acks决定的。&lt;/p&gt;
&lt;p&gt;Kafka保证只要有一个“同步中”的节点，“committed”的消息就不会丢失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leader的选择&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kafka的核心是日志文件，日志文件在集群中的同步是分布式数据系统最基础的要素。&lt;/p&gt;
&lt;p&gt;如果leaders永远不会down的话我们就不需要followers了！一旦leader down掉了，需要在followers中选择一个新的leader.但是followers本身有可能延时太久或者crash，所以必须选择高质量的follower作为leader.必须保证，一旦一个消息被提交了，但是leader down掉了，新选出的leader必须可以提供这条消息。大部分的分布式系统采用了多数投票法则选择新的leader,对于多数投票法则，就是根据所有副本节点的状况动态的选择最适合的作为leader.Kafka并不是使用这种方法。&lt;/p&gt;
&lt;p&gt;Kafaka动态维护了一个同步状态的副本的集合（a set of in-sync replicas），简称ISR，在这个集合中的节点都是和leader保持高度一致的，任何一条消息必须被这个集合中的每个节点读取并追加到日志中了，才回通知外部这个消息已经被提交了。因此这个集合中的任何一个节点随时都可以被选为leader.ISR在ZooKeeper中维护。ISR中有f+1个节点，就可以允许在f个节点down掉的情况下不会丢失消息并正常提供服。ISR的成员是动态的，如果一个节点被淘汰了，当它重新达到“同步中”的状态时，他可以重新加入ISR.这种leader的选择方式是非常快速的，适合kafka的应用场景。&lt;/p&gt;
&lt;p&gt;一个邪恶的想法：如果所有节点都down掉了怎么办？Kafka对于数据不会丢失的保证，是基于至少一个节点是存活的，一旦所有节点都down了，这个就不能保证了。
实际应用中，当所有的副本都down掉时，必须及时作出反应。可以有以下两种选择:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;等待ISR中的任何一个节点恢复并担任leader。&lt;/li&gt;
&lt;li&gt;选择所有节点中（不只是ISR）第一个恢复的节点作为leader.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这是一个在可用性和连续性之间的权衡。如果等待ISR中的节点恢复，一旦ISR中的节点起不起来或者数据都是了，那集群就永远恢复不了了。如果等待ISR意外的节点恢复，这个节点的数据就会被作为线上数据，有可能和真实的数据有所出入，因为有些数据它可能还没同步到。Kafka目前选择了第二种策略，在未来的版本中将使这个策略的选择可配置，可以根据场景灵活的选择。&lt;/p&gt;
&lt;p&gt;这种窘境不只Kafka会遇到，几乎所有的分布式数据系统都会遇到。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;副本管理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以上仅仅以一个topic一个分区为例子进行了讨论，但实际上一个Kafka将会管理成千上万的topic分区.Kafka尽量的使所有分区均匀的分布到集群所有的节点上而不是集中在某些节点上，另外主从关系也尽量均衡这样每个几点都会担任一定比例的分区的leader.&lt;/p&gt;
&lt;p&gt;优化leader的选择过程也是很重要的，它决定了系统发生故障时的空窗期有多久。Kafka选择一个节点作为“controller”,当发现有节点down掉的时候它负责在游泳分区的所有节点中选择新的leader,这使得Kafka可以批量的高效的管理所有分区节点的主从关系。如果controller down掉了，活着的节点中的一个会备切换为新的controller.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Kafka 架构</title>
        <link>https://lxb.wiki/44075289/</link>
        <pubDate>Sat, 25 Sep 2021 21:55:19 +0000</pubDate>
        
        <guid>https://lxb.wiki/44075289/</guid>
        <description>&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;一简介&#34;&gt;一、简介&lt;/h2&gt;
&lt;h3 id=&#34;11-概述&#34;&gt;1.1 概述&lt;/h3&gt;
&lt;p&gt;Kafka主要设计目标如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。&lt;/li&gt;
&lt;li&gt;高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。&lt;/li&gt;
&lt;li&gt;支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。&lt;/li&gt;
&lt;li&gt;同时支持离线数据处理和实时数据处理。&lt;/li&gt;
&lt;li&gt;支持在线水平扩展&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;12-消息系统介绍&#34;&gt;1.2 消息系统介绍&lt;/h3&gt;
&lt;p&gt;一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。有两种主要的消息传递模式：&lt;strong&gt;点对点传递模式、发布-订阅模式&lt;/strong&gt;。大部分的消息系统选用发布-订阅模式。&lt;strong&gt;Kafka就是一种发布-订阅模式&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;13-点对点消息传递&#34;&gt;1.3 点对点消息传递&lt;/h3&gt;
&lt;p&gt;在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数据。但是一条消息只能被消费一次。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。这种架构描述示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220305215705.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;生产者发送一条消息到queue，只有一个消费者能收到&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;14-发布-订阅消息传递&#34;&gt;1.4 发布-订阅消息传递&lt;/h3&gt;
&lt;p&gt;在发布-订阅消息系统中，消息被持久化到一个topic中。与点对点消息系统不同的是，消费者可以订阅一个或多个topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。该模式的示例图如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220305215731.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;15-kafka的优点&#34;&gt;1.5 Kafka的优点&lt;/h3&gt;
&lt;p&gt;1）解耦：&lt;/p&gt;
&lt;p&gt;在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。&lt;/p&gt;
&lt;p&gt;2）冗余：（副本）&lt;/p&gt;
&lt;p&gt;有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的&amp;quot;插入-获取-删除&amp;quot;范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。&lt;/p&gt;
&lt;p&gt;3）扩展性&lt;/p&gt;
&lt;p&gt;因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。&lt;/p&gt;
&lt;p&gt;4）灵活性&amp;amp;峰值处理能力&lt;/p&gt;
&lt;p&gt;在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。&lt;/p&gt;
&lt;p&gt;5）可恢复性&lt;/p&gt;
&lt;p&gt;系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。&lt;/p&gt;
&lt;p&gt;6）顺序保证&lt;/p&gt;
&lt;p&gt;在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。&lt;/p&gt;
&lt;p&gt;7）缓冲&lt;/p&gt;
&lt;p&gt;在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。&lt;/p&gt;
&lt;p&gt;8）异步通信&lt;/p&gt;
&lt;p&gt;很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。&lt;/p&gt;
&lt;h3 id=&#34;16-常用mq对比&#34;&gt;1.6 常用MQ对比&lt;/h3&gt;
&lt;p&gt;1）RabbitMQ&lt;/p&gt;
&lt;p&gt;RabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正因如此，它非常重量级，更适合于企业级的开发。同时实现了Broker构架，这意味着消息在发送给客户端时先在中心队列排队。对路由，负载均衡或者数据持久化都有很好的支持。&lt;/p&gt;
&lt;p&gt;2）Redis&lt;/p&gt;
&lt;p&gt;Redis是一个基于Key-Value对的NoSQL数据库，开发维护很活跃。虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明：入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。&lt;/p&gt;
&lt;p&gt;3）ZeroMQ&lt;/p&gt;
&lt;p&gt;ZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZeroMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对这MQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序将扮演这个服务器角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但是ZeroMQ仅提供非持久性的队列，也就是说如果宕机，数据将会丢失。其中，Twitter的Storm 0.9.0以前的版本中默认使用ZeroMQ作为数据流的传输（Storm从0.9版本开始同时支持ZeroMQ和Netty作为传输模块）。&lt;/p&gt;
&lt;p&gt;4）ActiveMQ&lt;/p&gt;
&lt;p&gt;ActiveMQ是Apache下的一个子项目。 类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。&lt;/p&gt;
&lt;p&gt;5）Kafka/Jafka&lt;/p&gt;
&lt;p&gt;Kafka是Apache下的一个子项目，是一个高性能跨语言分布式发布/订阅消息队列系统，而Jafka是在Kafka之上孵化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；支持Hadoop数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka通过Hadoop的并行加载机制统一了在线和离线的消息处理。Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。&lt;/p&gt;
&lt;h3 id=&#34;17-kafka中的术语解释&#34;&gt;1.7 Kafka中的术语解释&lt;/h3&gt;
&lt;h4 id=&#34;概述&#34;&gt;概述&lt;/h4&gt;
&lt;p&gt;在深入理解Kafka之前，先介绍一下Kafka中的术语。下图展示了Kafka的相关术语以及之间的关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220305215821.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。&lt;/p&gt;
&lt;p&gt;如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。&lt;/p&gt;
&lt;h4 id=&#34;1-broker&#34;&gt;1 broker&lt;/h4&gt;
&lt;p&gt;Kafka 集群包含一个或多个服务器，服务器节点称为broker。&lt;/p&gt;
&lt;p&gt;broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。&lt;/p&gt;
&lt;p&gt;如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。&lt;/p&gt;
&lt;p&gt;如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。&lt;/p&gt;
&lt;h4 id=&#34;2-topic&#34;&gt;2 Topic&lt;/h4&gt;
&lt;p&gt;每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）&lt;/p&gt;
&lt;p&gt;类似于数据库的表名&lt;/p&gt;
&lt;h4 id=&#34;3-partition&#34;&gt;3 &lt;strong&gt;Partition&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;topic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。&lt;/p&gt;
&lt;h4 id=&#34;4-producer&#34;&gt;4 Producer&lt;/h4&gt;
&lt;p&gt;生产者即数据的发布者，该角色将消息发布到Kafka的topic中。broker接收到生产者发送的消息后，broker将该消息&lt;strong&gt;追加&lt;/strong&gt;到当前用于追加数据的segment文件中。生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。&lt;/p&gt;
&lt;h4 id=&#34;5-consumer&#34;&gt;5 Consumer&lt;/h4&gt;
&lt;p&gt;消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。&lt;/p&gt;
&lt;h4 id=&#34;6-consumer-group&#34;&gt;6 Consumer Group&lt;/h4&gt;
&lt;p&gt;每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制-给consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。&lt;/p&gt;
&lt;h4 id=&#34;7-leader&#34;&gt;7 Leader&lt;/h4&gt;
&lt;p&gt;每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。&lt;/p&gt;
&lt;h4 id=&#34;8-follower&#34;&gt;8 Follower&lt;/h4&gt;
&lt;p&gt;Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新创建一个Follower。&lt;/p&gt;
&lt;h4 id=&#34;9-offset&#34;&gt;9 Offset&lt;/h4&gt;
&lt;p&gt;kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka&lt;/p&gt;
&lt;h2 id=&#34;一kafka的架构&#34;&gt;一、Kafka的架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220305215857.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。&lt;/p&gt;
&lt;h3 id=&#34;21-分布式模型&#34;&gt;2.1 分布式模型&lt;/h3&gt;
&lt;p&gt;Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本（Leader），其他节点作为备份副本（Follower，也叫作从副本）。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本出现故障时，备份副本中的一个副本会被选择为新的主副本。因为每个分区的副本中只有主副本接受读写，所以每个服务器端都会作为某些分区的主副本，以及另外一些分区的备份副本，这样Kafka集群的所有服务端整体上对客户端是负载均衡的。&lt;/p&gt;
&lt;p&gt;Kafka的生产者和消费者相对于服务器端而言都是客户端。&lt;/p&gt;
&lt;p&gt;Kafka生产者客户端发布消息到服务端的指定主题，会指定消息所属的分区。生产者发布消息时根据消息是否有键，采用不同的分区策略。消息没有键时，通过轮询方式进行客户端负载均衡；消息有键时，根据分区语义（例如hash）确保相同键的消息总是发送到同一分区。&lt;/p&gt;
&lt;p&gt;Kafka的消费者通过订阅主题来消费消息，并且每个消费者都会设置一个消费组名称。因为生产者发布到主题的每一条消息都只会发送给消费者组的一个消费者。所以，如果要实现传统消息系统的“队列”模型，可以让每个消费者都拥有相同的消费组名称，这样消息就会负责均衡到所有的消费者；如果要实现“发布-订阅”模型，则每个消费者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。&lt;/p&gt;
&lt;p&gt;分区是消费者现场模型的最小并行单位。如下图（图1）所示，生产者发布消息到一台服务器的3个分区时，只有一个消费者消费所有的3个分区。在下图（图2）中，3个分区分布在3台服务器上，同时有3个消费者分别消费不同的分区。假设每个服务器的吞吐量时300MB，在下图（图1）中分摊到每个分区只有100MB，而在下图（图2）中，集群整体的吞吐量有900MB。可以看到，增加服务器节点会提升集群的性能，增加消费者数量会提升处理性能。&lt;/p&gt;
&lt;p&gt;同一个消费组下多个消费者互相协调消费工作，Kafka会将所有的分区平均地分配给所有的消费者实例，这样每个消费者都可以分配到数量均等的分区。Kafka的消费组管理协议会动态地维护消费组的成员列表，当一个新消费者加入消费者组，或者有消费者离开消费组，都会触发再平衡操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220305215916.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Kafka的消费者消费消息时，只保证在一个分区内的消息的完全有序性，并不保证同一个主题汇中多个分区的消息顺序。而且，消费者读取一个分区消息的顺序和生产者写入到这个分区的顺序是一致的。比如，生产者写入“hello”和“Kafka”两条消息到分区P1，则消费者读取到的顺序也一定是“hello”和“Kafka”。如果业务上需要保证所有消息完全一致，只能通过设置一个分区完成，但这种做法的缺点是最多只能有一个消费者进行消费。一般来说，只需要保证每个分区的有序性，再对消息假设键来保证相同键的所有消息落入同一分区，就可以满足绝大多数的应用。&lt;/p&gt;
&lt;h2 id=&#34;二topics和partition&#34;&gt;二、Topics和Partition&lt;/h2&gt;
&lt;p&gt;Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220305215934.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。例如可以通过配置$KAFKA_HOME/config/server.properties，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The minimum age of a log file to be eligible for deletion&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;log.retention.hours&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;168&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The maximum size of a log segment file. When this size is reached a new log segment will be created.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;log.segment.bytes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1073741824&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The interval at which log segments are checked to see if they can be deleted according to the retention policies&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;log.retention.check.interval.ms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;300000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;log.cleaner.enable&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。&lt;/p&gt;
&lt;h2 id=&#34;三producer消息路由&#34;&gt;三、Producer消息路由&lt;/h2&gt;
&lt;p&gt;Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。可以在$KAFKA_HOME/config/server.properties中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。&lt;/p&gt;
&lt;p&gt;在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。&lt;/p&gt;
&lt;h2 id=&#34;四consumer-group&#34;&gt;四、Consumer Group&lt;/h2&gt;
&lt;p&gt;使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220305220015.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。&lt;/p&gt;
&lt;p&gt;实际上，Kafka的设计理念之一就是同时提供离线处理和实时处理。根据这一特性，可以使用Storm这种实时流处理系统对消息进行实时在线处理，同时使用Hadoop这种批处理系统进行离线处理，还可以同时将数据实时备份到另一个数据中心，只需要保证这三个操作所使用的Consumer属于不同的Consumer Group即可。&lt;/p&gt;
&lt;h2 id=&#34;五push-vs-pull&#34;&gt;五、Push vs. Pull&lt;/h2&gt;
&lt;p&gt;作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。&lt;/p&gt;
&lt;p&gt;push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。&lt;/p&gt;
&lt;p&gt;对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。&lt;/p&gt;
&lt;h2 id=&#34;六kafka-delivery-guarantee&#34;&gt;六、Kafka delivery guarantee&lt;/h2&gt;
&lt;p&gt;有这么几种可能的delivery guarantee：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At most once 　　消息可能会丢，但绝不会重复传输&lt;/p&gt;
&lt;p&gt;At least one 　　 消息绝不会丢，但可能会重复传输&lt;/p&gt;
&lt;p&gt;Exactly once 　　 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。&lt;/p&gt;
&lt;p&gt;接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kafka默认保证At least once&lt;/strong&gt;，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>消息队列中的问题| 丢消息| 重复消费| 消息积压</title>
        <link>https://lxb.wiki/ff873094/</link>
        <pubDate>Sat, 24 Jul 2021 23:05:09 +0000</pubDate>
        
        <guid>https://lxb.wiki/ff873094/</guid>
        <description>&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;1-丢消息&#34;&gt;1. 丢消息&lt;/h1&gt;
&lt;h2 id=&#34;检测消息丢失的方法&#34;&gt;检测消息丢失的方法&lt;/h2&gt;
&lt;p&gt;一般而言，一个新的系统刚刚上线，各方面都不太稳定，需要一个磨合期，这个时候，特别需要监控到你的系统中是否有消息丢失的情况。&lt;/p&gt;
&lt;p&gt;如果是 IT 基础设施比较完善的公司，一般都有分布式链路追踪系统，使用类似的追踪系统可以很方便地追踪每一条消息。&lt;/p&gt;
&lt;p&gt;可以利用消息队列的有序性来验证是否有消息丢失。原理非常简单，在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性。&lt;/p&gt;
&lt;p&gt;如果没有消息丢失，Consumer 收到消息的序号必然是连续递增的，或者说收到的消息，其中的序号必然是上一条消息的序号 +1。如果检测到序号不连续，那就是丢消息了。还可以通过缺失的序号来确定丢失的是哪条消息，方便进一步排查原因。&lt;/p&gt;
&lt;p&gt;大多数消息队列的客户端都支持拦截器机制，你可以利用这个拦截器机制，在 Producer 发送消息之前的拦截器中将序号注入到消息中，在 Consumer 收到消息的拦截器中检测序号的连续性，这样实现的好处是消息检测的代码不会侵入到你的业务代码中，待你的系统稳定后，也方便将这部分检测的逻辑关闭或者删除。&lt;/p&gt;
&lt;p&gt;如果是在一个分布式系统中实现这个检测方法，有几个问题需要你注意。&lt;/p&gt;
&lt;p&gt;首先，像 Kafka 和 RocketMQ 这样的消息队列，它是不保证在 Topic 上的严格顺序的，只能保证分区上的消息是有序的，所以我们在发消息的时候必须要指定分区，并且，在每个分区单独检测消息序号的连续性。&lt;/p&gt;
&lt;p&gt;如果你的系统中 Producer 是多实例的，由于并不好协调多个 Producer 之间的发送顺序，所以也需要每个 Producer 分别生成各自的消息序号，并且需要附加上 Producer 的标识，在 Consumer 端按照每个 Producer 分别来检测序号的连续性。&lt;/p&gt;
&lt;p&gt;Consumer 实例的数量最好和分区数量一致，做到 Consumer 和分区一一对应，这样会比较方便地在 Consumer 内检测消息序号的连续性。&lt;/p&gt;
&lt;h2 id=&#34;确保消息可靠传递&#34;&gt;确保消息可靠传递&lt;/h2&gt;
&lt;p&gt;整个消息从生产到消费的过程中，哪些地方可能会导致丢消息，以及应该如何避免消息丢失。一条消息从生产到消费完成这个过程，可以划分三个阶段&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220224160823.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生产阶段: 在这个阶段，从消息在 Producer 创建出来，经过网络传输发送到 Broker 端。&lt;/li&gt;
&lt;li&gt;存储阶段: 在这个阶段，消息在 Broker 端存储，如果是集群，消息会在这个阶段被复制到其他的副本上。&lt;/li&gt;
&lt;li&gt;消费阶段: 在这个阶段，Consumer 从 Broker 上拉取消息，经过网络传输发送到 Consumer 上。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-生产阶段&#34;&gt;1. 生产阶段&lt;/h3&gt;
&lt;p&gt;在生产阶段，消息队列通过最常用的请求确认机制，来保证消息的可靠传递：当你的代码调用发消息方法时，消息队列的客户端会把消息发送到 Broker，Broker 收到消息后，会给客户端返回一个确认响应，表明消息已经收到了。客户端收到响应后，完成了一次正常消息的发送。&lt;/p&gt;
&lt;p&gt;只要 Producer 收到了 Broker 的确认响应，就可以保证消息在生产阶段不会丢失。有些消息队列在长时间没收到发送确认响应后，会自动重试，如果重试再失败，就会以返回值或者异常的方式告知用户。&lt;/p&gt;
&lt;p&gt;你在编写发送消息代码时，需要注意，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失。以 Kafka 为例，我们看一下如何可靠地发送消息：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;同步发送时，只要注意捕获异常即可&lt;/strong&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    RecordMetadata metadata &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; producer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(record)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    System&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;println(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; 消息发送成功。&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;} catch (Throwable e) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    System&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;println(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; 消息发送失败！&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    System&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;println(e);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;异步发送时，则需要在回调方法里进行检查。这个地方是需要特别注意的，很多丢消息的原因就是，我们使用了异步发送，却没有在回调中检查发送结果。&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;producer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(record, (metadata, exception) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (metadata &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; null) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        System&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;println(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; 消息发送成功。&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        System&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;println(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; 消息发送失败！&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        System&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;println(exception);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;2-存储阶段&#34;&gt;2. 存储阶段&lt;/h3&gt;
&lt;p&gt;在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的。&lt;/p&gt;
&lt;p&gt;如果对消息的可靠性要求非常高，可以通过配置 Broker 参数来避免因为宕机丢消息。&lt;/p&gt;
&lt;p&gt;对于单个节点的 Broker，需要配置 Broker 参数，在收到消息后，&lt;strong&gt;将消息写入磁盘后再给 Producer 返回确认响应，这样即使发生宕机，由于消息已经被写入磁盘，就不会丢失消息&lt;/strong&gt;，恢复后还可以继续消费。例如，在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘。&lt;/p&gt;
&lt;p&gt;如果是 Broker 是由多个节点组成的集群，需要将 Broker 集群配置成：&lt;strong&gt;至少将消息发送到 2 个以上的节点，再给客户端回复发送确认响应&lt;/strong&gt;。这样当某个 Broker 宕机时，其他的 Broker 可以替代宕机的 Broker，也不会发生消息丢失。消息队列&lt;strong&gt;通过消息复制来确保消息的可靠性的&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;3-消费阶段&#34;&gt;3. 消费阶段&lt;/h3&gt;
&lt;p&gt;消费阶段采用和生产阶段类似的确认机制来保证消息的可靠传递，客户端从 Broker 拉取消息后，执行用户的消费业务逻辑，成功后，才会给 Broker 发送消费确认响应。如果 Broker 没有收到消费确认响应，下&lt;/p&gt;
&lt;p&gt;次拉消息的时候还会返回同一条消息，确保消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失。&lt;/p&gt;
&lt;p&gt;你在编写消费代码时需要注意的是，不要在收到消息后就立即发送消费确认，而是应该&lt;strong&gt;在执行完所有消费业务逻辑之后，再发送消费确认&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;同样，我们以用 Python 语言消费 RabbitMQ 消息为例，来看一下如何实现一段可靠的消费代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;callback&lt;/span&gt;(ch, method, properties, body):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; [x] 收到消息 &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; body)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 在这儿处理收到的消息&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    database&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(body)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; [x] 消费完成 &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 完成消费业务逻辑后发送消费确认响应&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;basic_ack(delivery_tag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; method&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delivery_tag)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;channel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;basic_consume(queue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;hello&amp;#39;&lt;/span&gt;, on_message_callback&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;callback)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在消费的回调方法 callback 中，正确的顺序先是把消息保存到数据库，然后再发送消费确认响应。这样如果保存消息到数据库失败了，就不会执行消费确认的代码，下次拉到的还是这条消息，直到消费成功。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;两个消费者先后去拉消息是否能拉到同一条消息？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首先，消息队列一般都会有&lt;strong&gt;协调机制&lt;/strong&gt;，不会让这种情况出现，但是由于网络不确定性，这种情况还是在极小概率下会出现的。&lt;/p&gt;
&lt;p&gt;在同一个消费组内，A消费者拉走了index=10的这条消息，还没返回确认，这时候这个分区的消费位置还是10，B消费者来拉消息，可能有2种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;\1. 超时前，Broker认为这个分区还被A占用着，会拒绝B的请求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;\2. 超时后，Broker认为A已经超时没返回，这次消费失败，当前消费位置还是10，B再来拉消息，会给它返回10这条消息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在生产阶段，你需要捕获消息发送的错误，并重发消息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在存储阶段，你可以通过配置刷盘和复制相关的参数，让消息写入到多个副本的磁盘上，来确保消息不会因为某个 Broker 宕机或者磁盘损坏而丢失。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在消费阶段，你需要在处理完全部消费业务逻辑之后，再发送消费确认。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你在理解了这几个阶段的原理后，如果再出现丢消息的情况，应该可以通过在代码中加一些日志的方式，很快定位到是哪个阶段出了问题，然后再进一步深入分析，快速找到问题原因。&lt;/p&gt;
&lt;h1 id=&#34;2-重复消息&#34;&gt;2. 重复消息&lt;/h1&gt;
&lt;p&gt;在消息传递过程中，如果出现传递失败的情况，发送方会执行重试，重试的过程中就有可能会产生重复的消息。对使用消息队列的业务系统来说，如果没有对重复消息进行处理，就有可能会导致系统的数据出现错误。&lt;/p&gt;
&lt;h2 id=&#34;消息重复的情况必然存在&#34;&gt;消息重复的情况必然存在&lt;/h2&gt;
&lt;p&gt;在 &lt;strong&gt;MQTT 协议&lt;/strong&gt;中，给出了三种传递消息时能够提供的服务质量标准，这三种服务质量从低到高依次是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At most once: 至多一次。消息在传递时，最多会被送达一次。换一个说法就是，没什么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。&lt;/li&gt;
&lt;li&gt;At least once: 至少一次。消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现。&lt;/li&gt;
&lt;li&gt;Exactly once：恰好一次。消息在传递时，只会被送达一次，不允许丢失也不允许重复，这个是最高的等级。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个服务质量标准不仅适用于 MQTT，对所有的消息队列都是适用的。我们&lt;strong&gt;现在常用的绝大部分消息队列提供的服务质量都是 At least once，包括 RocketMQ、RabbitMQ 和 Kafka 都是这样。也就是说，消息&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;队****列很难保证消息不重复。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kafka 支持的“Exactly once”和我们刚刚提到的消息传递的服务质量标准“Exactly once”是不一样的，它是 Kafka 提供的另外一个特性，Kafka 中支持的事务也和我们通常意义理解的事务有一定的差异。在 Kafka&lt;/p&gt;
&lt;p&gt;中，&lt;strong&gt;事务和 Excactly once 主要是为了配合流计算使用的特性&lt;/strong&gt;。巧妙地用了两个所有人都非常熟悉的概念“事务”和“Exactly once”来包装它的新的特性，实际上它实现的这个事务和 Exactly once 并不是我们通常&lt;/p&gt;
&lt;p&gt;理解的那两个特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么大部分消息队列都选择只提供 At least once 的服务质量，而不是级别更高的 Exactly once？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决一个问题，往往会引发别的问题。若消息队列实现了exactly once，会引发的问题有：&lt;/p&gt;
&lt;p&gt;①消费端在pull消息时，需要检测此消息是否被消费，这个检测机制无疑会拉低消息消费的速度。可以预想到，随着消息的剧增，消费性能势必会急剧下降，导致消息积压；&lt;/p&gt;
&lt;p&gt;②检查机制还需要业务端去配合实现，若一条消息长时间未返回ack，消息队列需要去回调看下消费结果（这个类似于事物消息的回查机制）。这样就会增加业务端的压力，与很多的未知因素。&lt;/p&gt;
&lt;p&gt;所以，消息队列不实现exactly once，而是at least once + 幂等性，这个幂等性让给我们去处理。&lt;/p&gt;
&lt;p&gt;最重要的原因是消息队列即使做到了Exactly once级别，consumer也还是要做幂等。因为在consumer从消息队列取消息这里，如果consumer消费成功，但是ack失败，consumer还是会取到重复的消息，所以消&lt;/p&gt;
&lt;p&gt;息队列花大力气做成Exactly once并不能解决业务侧消息重复的问题。&lt;/p&gt;
&lt;p&gt;1、At least once + 幂等消费 = Exactly once，所以对于消息队列来讲，要做到Exactly once，其实是需消费端的共同配合（幂等消费）才可完成，消息队列基本只提供At least once的实现；&lt;/p&gt;
&lt;p&gt;2、从给的几种幂等消费的方案看，需要引入数据库、条件更新、分布式事务或锁等额外辅助，消息队列如果需要保障Exactly once，会导致消费端代码侵入，例如需要消费端增加消息队列用来处理幂等的client&lt;/p&gt;
&lt;p&gt;端，而消费端的形态可是太多了，兼容适配工作量巨大。故这个Exactly once留给用户自己处理，并且具有选择权，毕竟不是所有业务场景都需要Exactly once，例如机房温度上报的案例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果队列的实现是At least once，但是为了确保消息不丢失，Broker Service会进行一定的重试，但是不可能一直重试，如果一直重试失败怎么处理了？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有的消息队列会有一个特殊的队列来保存这些总是消费失败的“坏消息”，然后继续消费之后的消息，避免坏消息卡死队列。这种坏消息一般不会是因为网络原因或者消费者死掉导致的，大多都是消息数据本身有&lt;/p&gt;
&lt;p&gt;问题，消费者的业务逻辑处理不了导致的。&lt;/p&gt;
&lt;h2 id=&#34;用幂等性解决重复消息问题&#34;&gt;用幂等性解决重复消息问题&lt;/h2&gt;
&lt;p&gt;一般解决重复消息的办法是，在消费端，让我们消费消息的操作具备幂等性。&lt;/p&gt;
&lt;p&gt;幂等（Idempotence）是一个数学上的概念，它是这样定义的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;如果一个函数 f(x) 满足：f(f(x)) = f(x)，则函数 f(x) 满足幂等性。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个概念被拓展到计算机领域，被用来描述一个操作、方法或者服务。一个幂等操作的特点是，&lt;strong&gt;其任意多次执行所产生的影响均与一次执行的影响相同。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一个幂等的方法，使用同样的参数，对它进行多次调用和一次调用，对系统产生的影响是一样的。所以，对于幂等的方法，不用担心重复执行会对系统造成任何改变。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;比如在不考虑并发的情况下，“将账户 X 的余额设置为 100 元”，执行一次后对系统的影响是，账户 X 的余额变成了 100 元。只要提供的参数 100 元不变，那即使再执行多少次，账户 X 的余额始终都是 100 元，&lt;/p&gt;
&lt;p&gt;不会变化，这个操作就是一个幂等的操作。&lt;/p&gt;
&lt;p&gt;再比如“将账户 X 的余额加 100 元”，这个操作它就不是幂等的，每执行一次，账户余额就会增加 100 元，执行多次和执行一次对系统的影响（也就是账户的余额）是不一样的。&lt;/p&gt;
&lt;p&gt;如果我们系统&lt;strong&gt;消费消息的业务逻辑具备幂等性&lt;/strong&gt;，那就不用担心消息重复的问题了，因为&lt;strong&gt;同一条消息，消费一次和消费多次对系统的影响是完全一样的。也就可以认为，消费多次等于消费一次。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从对系统的影响结果来说：At least once + 幂等消费 = Exactly once。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;那么如何实现幂等操作呢？最好的方式就是，从业务逻辑设计上入手，将消费的业务逻辑设计成具备幂等性的操作。但是，不是所有的业务都能设计成天然幂等的，这里就需要一些方法和技巧来实现幂等。&lt;/p&gt;
&lt;p&gt;下面我给你介绍几种常用的设计幂等操作的方法：&lt;/p&gt;
&lt;h3 id=&#34;1-利用数据库的唯一约束实现幂等&#34;&gt;1. 利用数据库的唯一约束实现幂等&lt;/h3&gt;
&lt;p&gt;刚刚提到的那个不具备幂等特性转账的例子：将账户 X 的余额加 100 元。在这个例子中，我们可以通过改造业务逻辑，让它具备幂等性。&lt;/p&gt;
&lt;p&gt;首先，我们可以限定，对于每个转账单每个账户只可以执行一次变更操作，在分布式系统中，这个限制实现的方法非常多，最简单的是我们在数据库中建一张转账流水表，这个表有三个字段：转账单 ID、账户&lt;/p&gt;
&lt;p&gt;ID 和变更金额，然后&lt;strong&gt;给转账单 ID 和账户 ID 这两个字段联合起来创建一个唯一约束，这样对于相同的转账单 ID 和账户 ID，表里至多只能存在一条记录。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这样，我们消费消息的逻辑可以变为：“在转账流水表中增加一条转账记录，然后再根据转账记录，异步操作更新用户余额即可。”在转账流水表增加一条转账记录这个操作中，由于我们在这个表中预先定义了“账&lt;/p&gt;
&lt;p&gt;户 ID 转账单 ID”的唯一约束，对于同一个转账单同一个账户只能插入一条记录，后续重复的插入操作都会失败，这样就实现了一个幂等的操作。我们只要写一个 SQL，正确地实现它就可以了。&lt;/p&gt;
&lt;p&gt;基于这个思路，不光是可以使用关系型数据库，&lt;strong&gt;只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等&lt;/strong&gt;，&lt;/p&gt;
&lt;p&gt;比如， **Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂****等消费。 （ redis中的hash：**&lt;strong&gt;hsetnx&lt;/strong&gt; &lt;strong&gt;&lt;!-- raw HTML omitted --&gt; &lt;!-- raw HTML omitted --&gt; &lt;!-- raw HTML omitted --&gt;；&lt;/strong&gt; &lt;strong&gt;）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;比如，Elasticsearch中的幂等操作：&lt;/strong&gt; &lt;strong&gt;PUT /movie_index/movie/3，加上文档 ID&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-为更新的数据设置前置条件&#34;&gt;2. 为更新的数据设置前置条件&lt;/h3&gt;
&lt;p&gt;另外一种实现幂等的思路是，给数据变更设置一个前置条件，如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。这样，重复执行这个操作时，由于第一次&lt;/p&gt;
&lt;p&gt;更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。&lt;/p&gt;
&lt;p&gt;比如，刚刚我们说过，“将账户 X 的余额增加 100 元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果账户 X 当前的余额为 500 元，将余额加 100 元”，这个操作就具备了幂等&lt;/p&gt;
&lt;p&gt;性。对应到消息队列中的使用时，可以在发消息时在消息体中带上当前的余额，在消费的时候进行判断数据库中，当前余额是否与消息中的余额相等，只有相等才执行变更操作。&lt;/p&gt;
&lt;p&gt;但是，如果我们要更新的数据不是数值，或者我们要做一个比较复杂的更新操作怎么办？用什么作为前置判断条件呢？更加通用的方法是，&lt;strong&gt;给你的数据增加一个版本号属性，每次更数据前，比较当前数据的版本&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;3-记录并检查操作&#34;&gt;3. 记录并检查操作&lt;/h3&gt;
&lt;p&gt;如果上面提到的两种实现幂等方法都不能适用于你的场景，我们还有一种通用性最强，适用范围最广的实现幂等性方法：&lt;strong&gt;记录并检查操作&lt;/strong&gt;，也称为“Token 机制或者 GUID（全局唯一 ID）机制”，实现的思路特别&lt;/p&gt;
&lt;p&gt;简单：在执行数据更新操作之前，先检查一下是否执行过这个更新操作。&lt;/p&gt;
&lt;p&gt;具体的实现方法是，&lt;strong&gt;在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原理和实现是不是很简单？其实一点儿都不简单，在分布式系统中，这个方法其实是非常难实现的。首先，给每个消息指定一个全局唯一的 ID 就是一件不那么简单的事儿，方法有很多，但都不太好同时满足简&lt;/p&gt;
&lt;p&gt;单、高可用和高性能，或多或少都要有些牺牲。更加麻烦的是，在“检查消费状态，然后更新数据并且设置消费状态”中，三个操作必须作为一组操作保证原子性，才能真正实现幂等，否则就会出现 Bug。&lt;/p&gt;
&lt;p&gt;比如说，对于同一条消息：“全局 ID 为 8，操作为：给 ID 为 666 账户增加 100 元”，有可能出现这样的情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t0 时刻：Consumer A 收到条消息，检查消息执行状态，发现消息未处理过，开始执行“账户增加 100 元”；&lt;/li&gt;
&lt;li&gt;t1 时刻：Consumer B 收到条消息，检查消息执行状态，发现消息未处理过，因为这个时刻，Consumer A 还未来得及更新消息执行状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样就会导致账户被错误地增加了两次 100 元，这是一个在分布式系统中非常容易犯的错误，一定要引以为戒。&lt;/p&gt;
&lt;p&gt;对于这个问题，当然我们可以用事务来实现，也可以用锁来实现，但是在分布式系统中，无论是分布式事务还是分布式锁都是比较难解决问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;几种实现幂等操作的方法&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以利用数据库的约束来防止重复更新数据，&lt;/li&gt;
&lt;li&gt;可以为数据更新设置一次性的前置条件，来防止重复消息，如果这两种方法都不适用于你的场景，&lt;/li&gt;
&lt;li&gt;还可以用“记录并检查操作”的方式来保证幂等，这种方法适用范围最广，但是实现难度和复杂度也比较高，一般不推荐使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些实现幂等的方法，不仅可以用于解决重复消息的问题，也同样适用于，在其他场景中来&lt;strong&gt;解决重复请求或者重复调用的问题&lt;/strong&gt;。比如，我们可以将 HTTP 服务设计成幂等的，解决前端或者 APP 重复提交表单数&lt;/p&gt;
&lt;p&gt;据的问题；也可以将一个微服务设计成幂等的，解决 RPC 框架自动重试导致的重复调用问题。这些方法都是通用的。&lt;/p&gt;
&lt;h1 id=&#34;3-消息积压问题&#34;&gt;3. 消息积压问题&lt;/h1&gt;
&lt;p&gt;在使用消息队列遇到的问题中，消息积压这个问题，应该是最常遇到的问题。&lt;/p&gt;
&lt;p&gt;消息积压的直接原因，一定是系统中的某个部分出现了性能问题，来不及处理上游发送的消息，才会导致消息积压。 所以在使用消息队列时，如何来优化代码的性能，避免出现消息积压。&lt;/p&gt;
&lt;h2 id=&#34;优化性能来避免消息积压&#34;&gt;优化性能来避免消息积压&lt;/h2&gt;
&lt;p&gt;在使用消息队列的系统中，对于性能的优化，主要体现在生产者和消费者这一收一发两部分的业务逻辑中。对于消息队列本身的性能，不需要太关注。&lt;/p&gt;
&lt;p&gt;主要原因是，对于绝大多数使用消息队列的业务来说，消息队列本身的处理能力要远大于业务系统的处理能力。主流消息队列的单个节点，消息收发的性能可以达到每秒钟处理几万至几十万条消息的水平，还可&lt;/p&gt;
&lt;p&gt;以通过水平扩展 Broker 的实例数成倍地提升处理能力。而一般的业务系统需要处理的业务逻辑远比消息队列要复杂，单个节点每秒钟可以处理几百到几千次请求，已经可以算是性能非常好的了。所以，对于消&lt;/p&gt;
&lt;p&gt;息队列的性能优化，我们更关注的是，在消息的收发两端，我们的业务代码怎么和消息队列配合，达到一个最佳的性能。&lt;/p&gt;
&lt;h3 id=&#34;1-发送端性能优化&#34;&gt;1. 发送端性能优化&lt;/h3&gt;
&lt;p&gt;如果说，代码发送消息的性能上不去，需要优先检查一下，是不是发消息之前的业务逻辑耗时太多导致的。&lt;/p&gt;
&lt;p&gt;对于发送消息的业务逻辑，只需要注意&lt;strong&gt;设置合适的并发和批量大小，就可以达到很好的发送性能&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Producer 发送消息的过程，Producer 发消息给 Broker，Broker 收到消息后返回确认响应，这是一次完整的交互。假设这一次交互的平均时延是 1ms，把这 1ms 的时间分解开，它包括了下面这些步骤的耗时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发送端准备数据、序列化消息、构造请求等逻辑的时间，也就是发送端在发送网络请求之前的耗时；&lt;/li&gt;
&lt;li&gt;发送消息和返回响应在网络传输中的耗时；&lt;/li&gt;
&lt;li&gt;Broker 处理消息的时延。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果是单线程发送，每次只发送 1 条消息，那么每秒只能发送 1000ms / 1ms * 1 条 /ms = 1000 条 消息，这种情况下并不能发挥出消息队列的全部实力。&lt;/p&gt;
&lt;p&gt;无论是增加每次发送消息的批量大小，还是增加并发，都能成倍地提升发送性能。&lt;strong&gt;至于到底是选择批量发送还是增加并发，主要取决于发送端程序的业务性质&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如说，你的&lt;strong&gt;消息发送端是一个微服务&lt;/strong&gt;，主要接受 RPC 请求处理在线业务。很自然的，微服务在处理每次请求的时候，就在当前线程直接发送消息就可以了，因为所有 RPC 框架都是多线程支持多并发的，自&lt;/p&gt;
&lt;p&gt;然也就实现了并行发送消息。并且在线业务比较在意的是请求响应时延，选择批量发送必然会影响 RPC 服务的时延。这种情况，比较明智的方式就是&lt;strong&gt;通过并发来提升发送性能&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果你的&lt;strong&gt;系统是一个离线分析系统&lt;/strong&gt;，离线系统在性能上的需求是什么呢？它不关心时延，更注重整个系统的吞吐量。发送端的数据都是来自于数据库，这种情况就更适合批量发送，你可以批量从数据库读取数&lt;/p&gt;
&lt;p&gt;据，然后批量来发送消息，同样&lt;strong&gt;用少量的并发就可以获得非常高的吞吐量&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;2-消费端性能优化&#34;&gt;2. 消费端性能优化&lt;/h3&gt;
&lt;p&gt;使用消息队列的时候，大部分的性能问题都出现在消费端，如果消费的速度跟不上发送端生产消息的速度，就会造成消息积压。如果这种性能倒挂的问题只是暂时的，那问题不大，只要消费端的性能恢复之后，超过发送端的性能，那积压的消息是可以逐渐被消化掉的。&lt;/p&gt;
&lt;p&gt;要是消费速度一直比生产速度慢，时间长了，整个系统就会出现问题，要么，消息队列的存储被填满无法提供服务，要么消息丢失，这对于整个系统来说都是严重故障。&lt;/p&gt;
&lt;p&gt;所以，在设计系统的时候，&lt;strong&gt;一定要保证消费端的消费性能要高于生产端的发送性能，这样的系统才能健康的持续运行。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;消费端的性能优化除了优化消费业务逻辑以外，也可以通过水平扩容，增加消费端的并发数来提升总体的消费性能&lt;/strong&gt;。特别需要注意的一点是，&lt;strong&gt;在扩容 Consumer 的实例数量的同时，必须同步扩容主题中的分区&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（也叫队列）数量，确保 Consumer 的实例数和分区数量是相等的。如果 Consumer 的实例数量超过分区数量，这样的扩容实际上是没有效果的&lt;/strong&gt;。因为对于消费者，在每个分区上实际只能支持单线程消费。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一个解决消费慢的问题常见的错误：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/lxbwolf/blog_source_image@main/20220224161200.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;它收消息处理的业务逻辑可能比较慢，也很难再优化了，为了避免消息积压，在收到消息的 OnMessage 方法中，不处理任何业务逻辑，把这个消息放到一个内存队列里面就返回了。然后它可以启动很多的业务&lt;/p&gt;
&lt;p&gt;线程，这些业务线程里面是真正处理消息的业务逻辑，这些线程从内存队列里取消息处理，这样它就解决了单个 Consumer 不能并行消费的问题。&lt;/p&gt;
&lt;p&gt;这个方法是不是很完美地实现了并发消费？错误！ 因为会丢消息。如果收消息的节点发生宕机，在内存队列中还没来及处理的这些消息就会丢失。&lt;/p&gt;
&lt;p&gt;在onMessage方法结束后，如果没有抛异常，就自动ACK了。而这个时候，消息只是在内存队列中，并没有被真正处理完。&lt;/p&gt;
&lt;p&gt;如果onMessage方法中，收到消息后不确认，等真正处理完消息再确认，就可以了吧，这样就可以用内存队列了&lt;/p&gt;
&lt;p&gt;理论上是可以的，但要注意，像RocketMQ，采用默认配置的时候，onMessage方法结束后，如果没抛异常，默认就会自动确认了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在消费端是否可以通过批量消费的方式来提升消费性能？在什么样场景下，适合使用这种方法？或者说，这种方法有什么局限性？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;批量消费即一次取一批消息，等这一批消息都成功了，再提交最后一条消息的位置作为新的消费位置。如果其中任何一条失败，则认为整批都失败。&lt;/p&gt;
&lt;p&gt;批量消费应该是与消息处理是需要实时与否有关。如果需要实时处理，如订单相关的，就不能批量，但是发送提醒邮件之类的，就可以。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;批量消费有意义的场景要求：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.要么消费端对消息的处理支持批量处理，比如批量入库&lt;/li&gt;
&lt;li&gt;\2. 要么消费端支持多线程/协程并发处理，业务上也允许消息无序。&lt;/li&gt;
&lt;li&gt;\3. 或者网络带宽在考虑因素内，需要减少消息的overhead。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;批量消费的局限性：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\1. 需要一个整体ack的机制，一旦一条靠前的消息消费失败，可能会引起很多消息重试。&lt;/li&gt;
&lt;li&gt;\2. 多线程下批量消费速度受限于最慢的那个线程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但其实以上局限并没有影响主流MQ的实现了批量功能。&lt;/p&gt;
&lt;p&gt;1、要求消费端能够批量处理或者开启多线程进行单条处理
2、批量消费一旦某一条数据消费失败会导致整批数据重复消费
3、对实时性要求不能太高，批量消费需要Broker积累到一定消费数据才会发送到Consumer&lt;/p&gt;
&lt;p&gt;消费端进行批量操作，感觉和上面的先将消息放在内存队列中，然后在并发消费消息，如果机器宕机，这些批量消息都会丢失，如果在数据库层面，批量操作在大事务，会导致锁的竞争，并且也会导致主备的不&lt;/p&gt;
&lt;p&gt;一致。如果是一些不重要的消息如对日志进行备份，就可以使用批量操作之类的提高消费性能，因为一些日志消息丢失也是可以接受的。&lt;/p&gt;
&lt;p&gt;如果使用了批量消费的方式，那么就需要批量确认，如果一次消费十条消息，除了第七条消费失败了，其他的都处理成功了，但是这中情况下broker只能将消费的游标修改成消息7，而之后的消息虽然处理成功&lt;/p&gt;
&lt;p&gt;了，但是也只能使用类似于拉回重传的方式再次消费，浪费性能，而且这种批量消费对于消费者的并发我觉得不是很友好，可能消费者1来了取走了十条消息在处理，这时候消费者2过来了也想取十条消息，但是&lt;/p&gt;
&lt;p&gt;他需要等待消费者1进行ack才可以取走消息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如何判断增加多少consumer消费实例的个数？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以简单计算一下，消费并行度：单实例平均消费tps * 消费并行度 &amp;gt; 生产消息的总tps
消费并行度 = min（consumer实例数，分区数量）&lt;/p&gt;
&lt;h2 id=&#34;消息积压的紧急处理&#34;&gt;消息积压的紧急处理&lt;/h2&gt;
&lt;p&gt;还有一种消息积压的情况是，日常系统正常运转的时候，没有积压或者只有少量积压很快就消费掉了，但是某一个时刻，突然就开始积压消息并且积压持续上涨。这种情况下需要你在短时间内找到消息积压的原&lt;/p&gt;
&lt;p&gt;因，迅速解决问题才不至于影响业务。&lt;/p&gt;
&lt;p&gt;导致突然积压的原因肯定是多种多样的，不同的系统、不同的情况有不同的原因，不能一概而论。但是，排查消息积压原因，是有一些相对固定而且比较有效的方法的。&lt;/p&gt;
&lt;p&gt;能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。&lt;/p&gt;
&lt;p&gt;大部分消息队列都内置了监控的功能，只要通过监控数据，很容易确定是哪种原因。如果是单位时间发送的消息增多，比如说是赶上大促或者抢购，短时间内不太可能优化消费端的代码来提升消费性能，唯一的&lt;/p&gt;
&lt;p&gt;方法是通过扩容消费端的实例数来提升总体的消费能力。&lt;/p&gt;
&lt;p&gt;如果短时间内没有足够的服务器资源进行扩容，没办法的办法是，将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。&lt;/p&gt;
&lt;p&gt;还有一种不太常见的情况，你通过监控发现，无论是发送消息的速度还是消费消息的速度和原来都没什么变化，这时候你需要检查一下你的消费端，是不是消费失败导致的一条消息反复消费这种情况比较多，这&lt;/p&gt;
&lt;p&gt;种情况也会拖慢整个系统的消费速度。&lt;/p&gt;
&lt;p&gt;如果监控到消费变慢了，你需要检查你的消费实例，分析一下是什么原因导致消费变慢。优先检查一下日志是否有大量的消费错误，如果没有错误的话，可以通过打印堆栈信息，看一下你的消费线程是不是卡在&lt;/p&gt;
&lt;p&gt;什么地方不动了，比如触发了死锁或者卡在等待某些资源上了。&lt;/p&gt;
&lt;p&gt;**优化消息收发性能，**预防消息积压的方法有两种，增加批量或者是增加并发，在发送端这两种方法都可以使用，在消费端需要注意的是，增加并发需要同步扩容分区数量，否则是起不到效果的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;对于系统发生消息积压的情况&lt;/strong&gt;，需要先解决积压，再分析原因，毕竟保证系统的可用性是首先要解决的问题。快速解决积压的方法就是通过水平扩容增加 Consumer 的实例数量。&lt;/p&gt;
&lt;p&gt;消息积压处理：
1、发送端优化，增加批量和线程并发两种方式处理
2、消费端优化，优化业务逻辑代码、水平扩容增加并发并同步扩容分区数量
查看消息积压的方法：
1、消息队列内置监控，查看发送端发送消息与消费端消费消息的速度变化
2、查看日志是否有大量的消费错误
3、打印堆栈信息，查看消费线程卡点信息&lt;/p&gt;
&lt;p&gt;面试解决消息积压的方法：
（1）临时扩容，增加消费端，用硬件提升消费速度。
（2）服务降级，关闭一些非核心业务，减少消息生产。
（3）通过日志分析，监控等找到挤压原因，消息队列三部分，上游生产者是否异常生产大量数据，中游消息队列存储层是否出现问题，下游消费速度是否变慢，就能确定哪个环节出了问题
（4）根据排查解决异常部分。
（5）等待积压的消息被消费，恢复到正常状态，撤掉扩容服务器。&lt;/p&gt;
&lt;h1 id=&#34;4-如何保证消息的严格顺序&#34;&gt;4. 如何保证消息的严格顺序？&lt;/h1&gt;
&lt;p&gt;怎么来保证消息的严格顺序？主题层面是无法保证严格顺序的，&lt;strong&gt;只有在队列上才能保证消息的严格顺序。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果说，&lt;strong&gt;你的业务必须要求全局严格顺序，就只能把消息队列数配置成 1，生产者和消费者也只能是一个实例，这样才能保证全局严格顺序。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大部分情况下，并不需要全局严格顺序，只要保证局部有序就可以满足要求了。比如，在传递账户流水记录的时候，&lt;strong&gt;只要保证每个账户的流水有序就可以了，不同账户之间的流水记录是不需要保证顺序的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果需要&lt;strong&gt;保证局部严格顺序&lt;/strong&gt;，可以这样来实现。在发送端，我们使用账户 ID 作为 Key，&lt;strong&gt;采用一致性哈希算法计算出队列编号&lt;/strong&gt;，&lt;strong&gt;指定队列来发送消息。一致性哈希算法可以保证&lt;/strong&gt;，&lt;strong&gt;相同 Key 的消息总是发送到同一&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;个队列上&lt;/strong&gt;，这样可以保证相同 Key 的消息是严格有序的。如果不考虑队列扩容，也可以用队列数量取模的简单方法来计算队列编号。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
