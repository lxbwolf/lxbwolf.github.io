<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Github 博客绑定域名]]></title>
    <url>%2Fe9fbcad9%2F</url>
    <content type="text"><![CDATA[某篇文章说, CNAME 解析只支持 www 不支持@所以@ 只能 解析到一个一个的 IP 1. source 添加 CNAME 文件在源码的source 目录下, 添加一个CNAME文件文件内容为 1lxb.wiki 2. DNS 设置 记录类型 主机记录 解析路线(isp) 记录值 MX优先级 TTL 状态 操作 CNAME www 默认 lxbwolf.github.io – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.108.153 – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.111.153 – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.110.153 – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.109.153 – 10 分钟 正常 修改暂停删除备注 3. hexo 部署1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello Hexo]]></title>
    <url>%2Fa1751c09%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Xargs]]></title>
    <url>%2F38dfadad%2F</url>
    <content type="text"><![CDATA[xargs 是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。 xargs 可以将管道或标准输入（stdin）数据转换成命令行参数，也能够从文件的输出中读取数据。 xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。 xargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。 xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。 之所以能用到这个命令，关键是由于很多命令不支持|管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令 例如: 12find /sbin -perm 700 |ls -l #这个命令是错误的find /sbin -perm 700 |xargs ls -l #这样才是正确的 命令格式somecommand |xargs -item command 重要参数: -i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给 {}，可以用 {} 代替。 其他参数: -a file 从文件中读入作为sdtin -e flag ，注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。 -p 当每次执行一个argument的时候询问一次用户。 -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。 -t 表示先打印命令，然后再执行。 -r no-run-if-empty 当xargs的输入为空的时候则停止xargs，不用再去执行了。 -s num 命令行的最大字符数，指的是 xargs 后面那个命令的最大命令行字符数。 -L num 从标准输入一次读取 num 行送给 command 命令。 -l 同 -L。 -d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。 -x exit的意思，主要是配合-s使用。。 -P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。 实例:1. 多行变成单行1234567# cat test.txta b c d e f gh i j k l m no p qr s tu v w x y z 12# cat test.txt | xargsa b c d e f g h i j k l m n o p q r s t u v w x y z 2. 一次使用n个参数1234567891011# cat test.txt | xargs -n3a b cd e fg h ij k lm n op q rs t uv w xy z 3. d选项指定分隔符123# echo &quot;nameXnameXnameXname&quot; | xargs -dXname name name name 结合-n 选项使用 1234# echo &quot;nameXnameXnameXname&quot; | xargs -dX -n2name namename name 4. I选项的使用4.1 获取参数并替换{}假设一个命令为 sk.sh 和一个保存参数的文件 arg.txt： 1234#!/bin/bash#sk.sh命令内容，打印出所有参数。echo $* arg.txt.文件内容 12345# cat arg.txtaaabbbccc xargs 的一个选项 -I，使用 -I 指定一个替换字符串 {}，这个字符串在 xargs 扩展时会被替换掉，当 -I 与 xargs 结合使用，每一个参数命令都会被执行一次： 12345# cat arg.txt | xargs -I &#123;&#125; ./sk.sh sombefore &#123;&#125; someaftersombefore aaa someaftersombefore bbb someaftersombefore ccc someafter 4.2 复制文件实例复制所有图片文件到 /data/images 目录下： 1ls *.jpg | xargs -n1 -I &#123;&#125; cp &#123;&#125; /data/images/ 4.3 xargs 结合find 使用用 rm 删除太多的文件时候，可能得到一个错误信息：/bin/rm Argument list too long. 用 xargs 去避免这个问题： find . -type f -name &quot;*.log&quot; -print0 | xargs -0 rm -f xargs -0 将 \0 作为定界符。 统计一个源代码目录中所有 php 文件的行数： find . -type f -name &quot;*.php&quot; -print0 | xargs -0 wc -l 查找所有的 jpg 文件，并且压缩它们： find . -type f -name &quot;*.jpg&quot; -print | xargs tar -czvf images.tar.gz 4.4 下载多个文件假如你有一个文件包含了很多你希望下载的 URL，你能够使用 xargs下载所有链接： # cat url-list.txt | xargs wget -c]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[升级Https]]></title>
    <url>%2Fddf7de45%2F</url>
    <content type="text"><![CDATA[环境 CentOS nginx获取证书HTTPS 证书分三类：1. DV 域名验证证书 2. OV 组织机构验证证书 3. EV 增强的组织机构验证证书。每类证书的审核要求不同，在浏览器地址栏也会有区分，对于个人网站而言，使用免费的 DV 证书就足够了。 我使用了大名鼎鼎的 Let’s Encrypt 来生成证书。 1. 安装 certbotcertbot 是 Let’s Encrypt 提供的一套自动化工具。 yum install epel-release yum install certbot2. 生成证书这里采用 webroot 作为 Let’s Encrypt 的认证方式。 certbot certonly -a webroot --webroot-path=/your/project/path -d example.com -d www.example.comwebroot-path就是项目根路径，使用 -d 可以添加多个域名。这时证书就已经生成成功了，默认保存在 /etc/letsencrypt/live/example.com/ 下。证书文件包括： cert.pem: 服务端证书 chain.pem: 浏览器需要的所有证书但不包括服务端证书，比如根证书和中间证书 fullchain.pem: 包括了cert.pem和chain.pem的内容 privkey.pem: 证书私钥 3. 生成迪菲-赫尔曼密钥交换组（ Strong Diffie-Hellman Group）为了进一步提高安全性，也可以生成一个 Strong Diffie-Hellman Group。 openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048配置nginx编辑 Nginx 配置文件，如果你不知道配置文件在哪，可以用 locate /nginx.conf 命令查找。添加以下内容，具体参数以你的实际情况为准。 server { listen 443 ssl; # 启用http2 # 需要安装 Nginx Http2 Module # listen 443 http2 ssl; server_name my_server_name; #证书文件 ssl_certificate /etc/letsencrypt/live/my_server_name/fullchain.pem; #私钥文件 ssl_certificate_key /etc/letsencrypt/live/my_server_name/privkey.pem; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 优先采取服务器算法 ssl_prefer_server_ciphers on; # 定义算法 ssl_ciphers &amp;quot;EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH&amp;quot;; ssl_ecdh_curve secp384r1; ssl_session_cache shared:SSL:10m; ssl_session_tickets off; ssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4 valid=300s; resolver_timeout 5s; add_header Strict-Transport-Security &amp;quot;max-age=63072000; includeSubdomains&amp;quot;; add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; # 使用DH文件 ssl_dhparam /etc/ssl/certs/dhparam.pem; location ~ /.well-known { allow all; } location ~ \.php$ { root my_root; fastcgi_pass my_host:my_port; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } root my_root; index index.html index.php; location / { root my_root; autoindex on; index index.html index.php; client_max_body_size 1024m; } }其中的几项配置: ssl_stapling on; 开启 OCSP Stapling，使服务端主动获取 OCSP 查询结果并随着证书一起发送给客户端，从而让客户端跳过自己去验证的过程，提高 TLS 握手效率。 add_header Strict-Transport-Security &amp;quot;max-age=63072000; includeSubdomains&amp;quot;; 启用 HSTS 策略，强制浏览器使用 HTTPS 连接，max-age设置单位时间内強制使用 HTTPS 连接；includeSubDomains 可选，设置所有子域同时生效。浏览器在获取该响应头后，在 max-age 的时间内，如果遇到 HTTP 连接，就会通过 307 跳转強制使用 HTTPS 进行连接 add_header X-Frame-Options DENY; 添加 X-Frame-Options 响应头，可以禁止网站被嵌入到 iframe 中，减少点击劫持 (clickjacking)攻击。 add_header X-Content-Type-Options nosniff; 添加 X-Content-Type-Options 响应头，防止 MIME 类型嗅探攻击 测试nginx.conf 是否有语法错误 nginx -t 重启nginx nginx -s reload]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker挂载目录失败/权限拒绝]]></title>
    <url>%2F498654c2%2F</url>
    <content type="text"><![CDATA[把宿主机的一个目录挂载到容器中的一个目录，当访问容器中的这个目录时，出现如下问题： ls: cannot open directory .: Permission denied无法访问目录，权限拒绝。该问题通常在centos7下出现。或者一个容器启动成功后，里面的服务无法成功访问，这是因为centos7中的安全模块selinux把权限禁掉了，一般的解决方案有以下两种：（1）临时关闭selinux直接在centos服务器上执行以下命令即可。执行完成以后建议重新docker run。 setenforce 0（2）给容器加权限在docker run时给该容器加权限，加上以下参数即可： --privileged=true 一般都推荐使用这种方式。 按上述方法修改后, 如果执行下面命令失败 docker run --name rookie-nginx-test -d -p 8082:80 -v ~/nginx/www:/usr/share/nginx/html -v ~/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v ~/nginx/logs:/var/log/nginx -v ~/nginx/conf/conf.d:/etc/nginx/conf.d --link php7-fpm:php nginx则是因为~/nginx/www/ 目录下没有index 文件导致. 手动创建index.php 文件解决]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mac iTerm2登陆CentOS提示warning: Setlocale: LC_CTYPE: Cannot Change Locale (UTF-8): No Such File or Directory]]></title>
    <url>%2F784beb8f%2F</url>
    <content type="text"><![CDATA[【报错原因】：没有utf-8这个语系（没添加语言_国名前缀），LC_ALL又没设定值。 服务端解决方法： 在远程系统上， /etc/environment加入以下两行，重新登陆即可。 LANG=en_US.utf-8 LC_ALL=en_US.utf-8Mac终端解决方法： 编辑~/.bashrc或者~/.zshrc文件，添加 export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[升级到php7.1之后wordpress 网站出现Error Establishing a Database Connection的解决方法]]></title>
    <url>%2Fb6b408b2%2F</url>
    <content type="text"><![CDATA[现在很多WordPress的插件都推荐将php版本升级到7.0或者7.1以上，于是就折腾了一下把几个blog升级到了7.1.5，升级的过程不难，无非就是额外安装一个php，然后启动自带的配套php-fpm7，然后nginx里location转发到新的php socket文件，这里就不表了。 升级完了，phpinfo()发现一切都正常，但是访问WordPress，却意外提示Error establishing a database connection，但是db的连接信息明明没有问题，经过反复搜索尝试，发现只要将 /usr/share/nginx/html/wp-config.php 文件里的 define(&#39;DB_HOST&#39;, &#39;localhost&#39;); 修改为 define(&#39;DB_HOST&#39;, &#39;127.0.0.1&#39;); 即可解决，猜测原因可能是php7.1中对域的resolve问题 另外, 为了 Debug, 可以把 /usr/share/nginx/html/wp-config.php 的 debug 改为 true define(&#39;WP_DEBUG&#39;, true); 改好了, 再改成 false.]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>wordpress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Date 命令转换时间戳]]></title>
    <url>%2F7b4019ad%2F</url>
    <content type="text"><![CDATA[给定时间戳, 转换成日期网上所有的命令都是date -d @$stamp &quot;+%Y-%m-%d&quot; 但是一直提示 date: invalid date@stamp’带上&quot;@&quot; 符号, 就参数错误 正确使用方法:date -d “1970-01-01 UTC 1287331200 seconds” +%F或者使用awkawk ‘{print strftime(“%Y%m”, 1287331200)}’调用外部命令耗时比较长, 更高效的方法:printf “%(%Y%m)T\n” “$str” &gt;&gt; file如果bash 版本低于4, printf 不支持打印日期格式, 因此使用 下面这个bash/opt/compiler/gcc-4.8.2/bin/bash`]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大小端]]></title>
    <url>%2F7ee0edaa%2F</url>
    <content type="text"><![CDATA[计算机系统中内存是以字节为单位进行编址的，每个地址单元都唯一的对应着1个字节（8 bit）。这可以应对char类型数据的存储要求，因为char类型长度刚好是1个字节，但是有些类型的长度是超过1个字节的（字符串虽然是多字节的，但它本质是由一个个char类型组成的类似数组的结构而已），比如C/C++中，short类型一般是2个字节，int类型一般4个字节等。因此这里就存在着一个如何安排多个字节数据中各字节存放顺序的问题。正是因为不同的安排顺序导致了大端存储模式和小端存储模式的存在。 1. 解释假如有一个4字节的数据为 0x12 34 56 78（十进制：305419896，0x12为高字节，0x78为低字节），若将其存放于地址 0x4000 8000中，则有： 内存地址 0x4000 8000（低地址） 0x4000 8001 0x4000 8002 0x4000 8003（高地址） 大端模式 0x12（高字节） 0x34 0x56 0x78（低字节） 小端模式 0x78（低字节） 0x56 0x34 0x12（高字节） 大端模式：是指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中 小端模式，是指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中 为什么截然相反的大小端存储模式能够并存至今？在标准化备受推崇的今天，为什么大小端谁都没有被另外一个所同化？我想这除了历史的惯性使然，还与它们各自的优缺点有关。 大端模式优点：符号位在所表示的数据的内存的第一个字节中，便于快速判断数据的正负和大小 小端模式优点：1. 内存的低地址处存放低字节，所以在强制转换数据时不需要调整字节的内容（注解：比如把int的4字节强制转换成short的2字节时，就直接把int数据存储的前两个字节给short就行，因为其前两个字节刚好就是最低的两个字节，符合转换逻辑）； 2. CPU做数值运算时从内存中依顺序依次从低位到高位取数据进行运算，直到最后刷新最高位的符号位，这样的运算方式会更高效 其各自的优点就是对方的缺点，正因为两者彼此不分伯仲，再加上一些硬件厂商的坚持（见1.3节），因此在多字节存储顺序上始终没有一个统一的标准 Intel的80×86系列芯片使用小端存储模式 ARM芯片默认采用小端，但可以切换为大端 MIPS芯片采用大端，但可以在大小端之间切换 在网络上传输的数据普遍采用的都是大端 2. 判断方法一：通过将多字节数据强制类型转换成单字节数据，再通过判断起始存储位置是数据高字节还是低字节进行检测 // @Ret: 大端，返回true; 小端，返回false bool IsBigEndian_1() { int nNum = 0x12345678; char cLowAddressValue = *(char*)&amp;nNum; // 低地址处是高字节，则为大端 if ( cLowAddressValue == 0x12 ) return true; return false; }方法二：利用联合体union的存放顺序是所有成员都从低地址开始存放这一特性进行检测 // @Ret: 大端，返回true; 小端，返回false bool isBigEndian_2() { union uendian { int nNum; char cLowAddressValue; }; uendian u; u.nNum = 0x12345678; if ( u.cLowAddressValue == 0x12 ) return true; return false; }3. 转换大小端转换 // 实现16bit的数据之间的大小端转换 #define BLSWITCH16(A) ( ( ( (uint16)(A) &amp; 0xff00 ) &gt;&gt; 8 ) | \ ( ( (uint16)(A) &amp; 0x00ff ) &lt;&lt; 8 ) ) // 实现32bit的数据之间的大小端转换 #define BLSWITCH32(A) ( ( ( (uint32)(A) &amp; 0xff000000) &gt;&gt; 24) |\ (((uint32)(A) &amp; 0x00ff0000) &gt;&gt; 8) | \ (((unit32)(A) &amp; 0x0000ff00) &lt;&lt; 8) | \ (((uint32)(A) &amp; 0x000000ff) &lt;&lt; 32) )由于网络字节序一律为大端，而目前个人PC大部分都是X86的小端模式，因此网络编程中不可避免得要进行网络字节序和主机字节序之间的相互转换]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式发号器架构设计]]></title>
    <url>%2F3d5a1f1d%2F</url>
    <content type="text"><![CDATA[一 需求设计 分布式环境下，保证每个序列号（sequence）是全系统唯一的； 序列号可排序，满足单调递增的规律； 特定场景下，能生成无规则（或者看不出规则）的序列号； 生成的序列号尽量短； 序列号可进行二次混淆，提供可扩展的interface，业务方自定义实现。 二 方案设计为了满足上述需求，发号器必须能够支持不同的生成策略，最好是还能支持自定义的生成策略，这就对系统本身的可扩展性提出了要求。 目前，发号器设计了两种比较通用的基础策略，各有优缺点，但结合起来，能达到优势互补的目的。 1. segment第一种策略称之为『分段』（segment），下文将对其进行详细阐述： 整个segment发号器有两个重要的角色：Redis和MongoDB，理论上MongoDB是可以被MySQL或其他DB产品所替代的。 segment发号器所产生的号码满足单调递增的规律，短时间内产生的号码不会有过长的问题（可根据实际需要，设置初始值，比如 100）。 Redis数据结构（Hash类型）key: &lt;string&gt;，表示业务主键/名称 value: { cur: &lt;long&gt;，表示当前序列号 max: &lt;long&gt;，表示这个号段最大的可用序列号 }取号的大部分操作都集中在Redis，为了保证序列号递增的原子性，取号的功能可以用Lua脚本实现。 --[[ 由于RedisTemplate设置的HashValueSerializer是GenericToStringSerializer，故此处的HASH结构中的 VALUE都是string类型，需要使用tonumber函数转换成数字类型。 ]] local max = redis.pcall(&quot;HGET&quot;, KEYS[1], &quot;max&quot;) --获取一段序列号的max local cur = redis.pcall(&quot;HGET&quot;, KEYS[1], &quot;cur&quot;) --获取当前发号位置 if tonumber(cur) &gt;= tonumber(max) then --没有超过这段序列号的上限 local step = ARGV[1] if (step == nil) then --没有传入step参数 step = redis.pcall(&quot;HGET&quot;, KEYS[1], &quot;step&quot;) --获取这段序列号的step配置参数值 end redis.pcall(&quot;HSET&quot;, KEYS[1], &quot;max&quot;, tonumber(max) + tonumber(step)) --调整max参数值，扩展上限 end return redis.pcall(&quot;HINCRBY&quot;, KEYS[1], &quot;cur&quot;, 1) --触发HINCRBY操作，对cur自增，并返回自增后的值注意：在redis执行lua script期间，redis处于BUSY状态，这个时候对redis的任何形式的访问都会抛出JedisBusyException异常，所以lua script中的处理逻辑不得太复杂。 值得一提的是，即使切换到一个新的database，或者开启新线程执行lua script，都将会遇到同样的问题，毕竟redis是单进程单线程的。 如果不幸遇到上述问题，需要使用redis-cli客户端连上redis-server，向其发送SCRIPT KILL命令，即可终止脚本执行， 如果想避免上述问题，也可以直接使用Springboot提供的RedisTemplate，能支持绝大部分redis command。 MongoDB 数据结构{ bizTag: &lt;string&gt;, 表示业务主键/名称 max: &lt;long&gt;, 表示这个号段最大的可用序列号 step: &lt;int&gt;, 每次分段的步长 timestamp: &lt;long&gt;, 更新数据的时间戳（毫秒） }MongoDB部分主要是对号段的分配进行管理，一个号段不能多发，也可以根据发号情况，适当放缩号段步长（step）。 到此为止，segment发号器的雏形已经形成了。 一个比较突出的问题是在两个号段衔接的时间点，当一个segment派发完了后，会对MongoDB和Redis中的数据中的max扩容，I/O消耗比正常发号要稍多，会遇到“尖刺” 为了消除“尖刺”，可以使用双Buffer模型 这个模型的核心思想就是“预分配”。可以设置一个阈值（threshold），比如20%，当Buffer-1里面的号段已经消耗了20%，那么立刻根据Buffer-1的max和step，开辟Buffer-2。 当Buffer-1完全消耗了，可以无缝衔接Buffer-2,。如果Buffer-2的消耗也达到阈值了，又可以开辟Buffer-1，如此往复。 接下来，我们来讨论一下异常/故障情况。 ① Redis宕机。因为大部分发号工作都是依靠Redis完成的，所以发生了这种情况是非常糟糕的。如果想有效降低此风险，最行之有效的办法是对Redis进行集群化，通常是1主2从，这样可以挺住非常高的QPS了。 当然也有退而求其次的办法，就是利用上述提到的双Buffer模型。不依赖Redis取号，直接通过程序控制，利用机器内存。所以当需要重启发号服务之前，要确保依赖的组件是运行良好的，不然号段就丢失了。 ② 要不要持久化的问题。这个问题主要是针对Redis，如果没有记录下当前的取号进度，那么随着Redis的宕机，取号现场就变得难以恢复了；如果每次都记录取号进度，那么这种I/O高密度型的作业会对服务性能 造成一定影响，并且随着取号的时间延长，恢复取号现场就变得越来越慢了，甚至到最后是无法忍受的。除了对Redis做高可用之外，引入MongoDB也是出于对Redis持久化功能辅助的考虑。 个人建议：如果Redis已经集群化了，而且还开启了双Buffer的策略，以及MongoDB的加持，可以不用再开启Redis的持久化了。 如果考虑到极端情况下，Redis还是宕机了，我们可以使用MongoDB里面存下来的max，就max+1赋值给cur（避免上个号段取完，正好宕机了）。 ③ MongoDB宕机。这个问题不是很严重，只要将step适当拉长一些（至少取号能支撑20分钟），利用Redis还在正常取号的时间来抢救MongoDB。不过，考虑到实际可能没这么快恢复mongo服务，可以在程序中采取 一些容错措施，比如号段用完了，mongo服务无法到达，直接关闭取号通道，直到MongoDB能正常使用；或者程序给一个默认的step，让MongoDB中的max延长到max+step*n（可能取了N个号段MongoDB才恢复过来）， 这样取号服务也可以继续。依靠程序本身继续服务，那么需要有相关的log，这样才有利于恢复MongoDB中的数据。 ④ 取号服务宕机。这个没什么好说的，只能尽快恢复服务运行了。 ⑤ Redis，MongoDB都宕机了。这种情况已经很极端了，只能利用双Buffer策略，以及程序默认的设置进行工作了，同样要有相关的log，以便恢复Redis和MongoDB。 ⑥ 都宕机了。我有一句mmp不知当讲不当讲…… 2、snowflake第二种策略是Twitter出品，算法思想比较巧妙，实现的难度也不大。 以上示意图描述了一个序列号的二进制组成结构。 第一位不用，恒为0，即表示正整数； 接下来的41位表示时间戳，精确到毫秒。为了节约空间，可以将此时间戳定义为距离某个时间点所经历的毫秒数（Java默认是1970-01-01 00:00:00）； 再后来的10位用来标识工作机器，如果出现了跨IDC的情况，可以将这10位一分为二，一部分用于标识IDC，一部分用于标识服务器； 最后12位是序列号，自增长。 snowflake的核心思想是64bit的合理分配，但不必要严格按照上图所示的分法。 如果在机器较少的情况下，可以适当缩短机器id的长度，留出来给序列号。 当然，snowflake的算法将会面临两个挑战： ① 机器id的指定。这个问题在分布式的环境下会比较突出，通常的解决方案是利用Redis或者Zookeeper进行机器注册，确保注册上去的机器id是唯一的。为了解决 强依赖Redis或者Zookeeper的问题，可以将机器id写入本地文件系统。 ② 机器id的生成规则。这个问题会有一些纠结，因为机器id的生成大致要满足三个条件：a. int类型(10bit)纯数字，b. 相对稳定，c. 与其他机器要有所区别。至于优雅美观，都是其次了。对于机器id的存储，可以使用HASH结构，KEY的规则是“application-name.port.ip”，其中ip是通过算法转换成了一段长整型的纯数字，VALUE则是机器id， 服务id，机房id，其中，可以通过服务id和机房id反推出机器id。 假设服务id(workerId)占8bit，机房id(rackId)占2bit，从1开始，workerId=00000001，rackId=01，machineId=00000000101 如果用Redis存储，其表现形式如下： 如果存储在文件中（建议properties文件），则文件名是sequence-client:8112:3232235742.properties，文件内容如下： 如果发号服务上线，直接按照“application-name.port.ip”的规则取其内容。 ③ 时钟回拨。因为snowflake对系统时间是很依赖的，所以对于时钟的波动是很敏感的，尤其是时钟回拨，很有可能就会出现重复发号的情况。时钟回拨问题解决策略通常是直接拒绝发号，直到时钟正常，必要时进行告警。 三 程序设计整个发号过程可以分成三个层次： 1、策略层(strategy layer)：这个层面决定的是发号方法/算法，涵盖了上述所讲的segment和snowflake两种方式，当然，用户也可以自己扩展实现其他发号策略。 最顶上定义Sequence实际上就是发号的结果。bizType是对发号业务场景的定义，比如订单号，用户ID，邀请好友的分享码。 发号策略的init接口是发号前的初始化工作，而generate接口就是调用发号器的主入口了。 当然，考虑到各种异常情况，加入了拒绝发号的处理器（SequenceRejectedHandler），默认实现只是记录日志，用户可根据需求去实现该处理器，然后用set方法设置发号策略的拒绝处理器。 2、插件层(plugin layer)：此处的插件可以理解是一种拦截器，贯穿SequenceStrategy的发号全周期。引入插件后，无疑是丰富了整个发号的操作过程，用户可以从中干预到发号的整个流程，以便达到其他的目的，比如：记录发号历史，统计发号速率，发号二次混淆等。 可以看出，插件被设计成『注册式』的，发号策略只有注册了相关插件之后，插件才能生效， 当然，一个插件能被多个发号策略所注册，一个发号策略也能同时注册多个插件，所以两者是多对多的关系，PluginManager的出现就是解决插件的注册管理问题。 从SequencePlugin的定义中可以发现，插件是有优先级（Order）的，通过getOrder()可以获得，在这套发号系统里，Order值越小，表示该插件越优先执行。此外，插件有三个重要的操作： before，表示发号之前的处理。若返回了false，那么该插件后面的操作都失效了，否则继续执行发号流程。 after，表示发号之后的处理。 doException，表示插件发生异常的处理方法。 3、持久层(persistence layer)：这个层面指代的是上述所提的MongoDB部分，如果不需要持久化的支持，可以不实现此接口，那么整个发号器就变成纯内存管理的了。 PersistRepository定义了基本的CRUD方法，其中persistId可以理解成上述提到的BizType。 一切的持久化对象都是从PersistModel开始的，上图中的Segment、PersistDocument都是为了实现分段发号器而定义的。 四 总结这篇文章详细阐述了分布式发号器系统的设计，旨在能做出一个可扩展，易维护的发号系统。业界比较知名的发号算法似乎也不多，整个发号系统不一定就按照笔者所做的设计，还是要立足于具体的业务需求。]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>idalloc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysqldump: Got Error: 1044: Access Denied for User]]></title>
    <url>%2Fa333bc04%2F</url>
    <content type="text"><![CDATA[mysqldump -u username -p dbname &gt; dbname.sql mysqldump: Got error: 1044: Access denied for user XXX to database XXX when using LOCK TABLES 解决方法: mysqldump -u dbuser -ppass db --skip-lock-tables &gt; db.sql]]></content>
      <categories>
        <category>DB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DDBS]]></title>
    <url>%2F728c18a3%2F</url>
    <content type="text"><![CDATA[业务规模较小时，使用单机mysql作存储。但伴随业务发展，存储容量和并发能力会有瓶颈。 首先，假设单机的硬盘为1.8T，也可以挂更大容量硬盘，但仍有限。 其次，单机的读写并发能力有限，假设峰值写入qps1000，峰值读取qps3000，网卡对读取时流量也有要求，单次访问的读取量不应过大。 单机的链接数也有限。 那么，当使用单机mysql的业务发展，受到以上瓶颈时，一般的思路会是什么呢？一台机器不行，用两台呢，再不行，扩展更多台。 一台扩展为两台，磁盘容量扩大了，通过分表，将表打散在不同机器上，共同承担写入任务，并发也提高了，感觉这个思路是对的。 那么在这个过程中，我们需要做什么？ 业务发展到单机无法承受，即使在单机上，很多表应该也做过分表了。一般会根据业务选择分表键。单个表的大小mysql也有一定要求，一般存储量不大于1G，单条记录小一些，一般不超过1k，条数一般不超过1000万条，最多不超过5000万条，否则表的使用和维护效率都很低。假设业务已经做了足够多的分表，满足三年的数据增长需要，第一年过后，每个分表的条数达到200万条，整机存储容量使用了一半，此时我们想拆分为两台机器。 此时我们可以将原机器上部分表数据同步到新机器上，并在model层抽象一个路由层，将对数据库的操作发到不同的机器上，上层业务仍可以认为在使用单机。此时可以将原机器上不归属自己管理范围的表删除，腾出空间。 一台变成了两台，向分布式走了一步。此时存储容量和并发都提高了，由路由层管理两台机器。如果两台或今后的多台机器，并发数高于路由层处理能力怎么办？那还要把路由层机器也扩一下，把路由规则都写进去，大家按一个格则办事。 经过上面的一番折腾，数据库机器水平扩展，解决了单机存在的一些问题。在这个扩展的过程中，是否会对业务产生中断影响呢？ 会有一点影响。至少在路由层改路由表时，会中断数据库的写入，读取此时可以不中断。 ddbs中，使用到的多台机器，都叫做分片。分片提高了系统存储容量和并发能力，引入分片，也是系统的复杂度提高了，需要引入路由层机器，路由机器也可能需要扩展，复杂操作，还需要添加更多逻辑功能。但至少可以可业务逻辑区分开，业务可以把ddbs当做单机在使用。 那么ddbs有哪些不足呢？ ddbs还是要基于分表、分片实现的。那么对数据库的任何操作，首要条件是需要指明操作的分表键。没有这个维度的准确值，就不能对数据库操作，当然除非是备用库，那你随便扫表，因为备用库可以转为冗余安全，不走线上流量，可以做统计任务。 单指明分表键还不行，还要注意操作的数据可能会分布在不同分表、不同分片中，这样的操作会引发ddbs产生大量并发操作，业务的一个请求就会占用多个机器多个链接，使ddbs得并发能力大打折扣。比如‘where 分表键 in （）’操作，这种操作要慎重，in中个数不可太多。 分表键最好选用整形，字符串型，可能hash后分配不均，表大小不均衡。 事物操作在ddbs中的实现，非常耗费系统性能。事务类操作需要路由控制层控制整个操作过程，期中可能涉及多个分片，多个不同的表的操作，对系统整体可用性要求高]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跑道问题]]></title>
    <url>%2F81be14b5%2F</url>
    <content type="text"><![CDATA[25个人，每5个人一个跑道，最少经过几次比赛，得到前三名 初步思路: 第一步, 每5人一组, 全跑完后, 每组的后两名一定不在最终要的”前三名” 结果内, 所以每组可以排除2人, 剩下25-2_5=15人. 共经过5次比赛 第二步, 剩下的15人, 每5人一组, 跑完后, 每组淘汰2人, 剩下 15-2_3=9人. 经过3次比赛 第三步, 剩下的9个人分两组, A组5人B组4人, 跑完后, A组淘汰2人, B组淘汰1人, 剩下 9-2-1=6人. 经过2次比赛 第四步, 剩下的6人分两组, C组5人D组1人, A组跑完后, 淘汰2人, B组1人不需要跑, 剩下 6-2=4人. 经过1次比赛 第五步, 剩下的4个人, 跑一次, 得出前三名. 经过1次比赛 共经过 5+3+2+1+1=12次 在第一步中, 5组全跑完后, 每组的第一名再跑一次, 按速度快慢分别标为A1 B1 C1 D1 E1. 则A1 为25人中的第一名. 经过5+1=6次比赛 在第6次比赛中, 落后的两名D1 和E1, 可以被排除, 进而整个D组和E组都可以排除. C1不可能是第二名. 第二名可能的人员有A2 B1, 第三名可能的人员有 B1 A3 B2 A2 C1. 第二名的集合是第三名集合的子集. 第三名所有可能的5个人跑一次, 得出第二名和第三名.经过1次比赛 共经过7次比赛]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[架构设计原则]]></title>
    <url>%2F88049151%2F</url>
    <content type="text"><![CDATA[GRASP 通用职责分配软件模式来自 Craig Larman 的软件设计书《UML 和模式应用》[附录 1]，Larman 在书中提出软件设计的关键任务是职责分配，并提炼总结出 9 种 (5 种核心 +4 种扩展) 软件职责分配模式，这些模式是比 GoF 设计模式更抽象的元模式。 1. 信息专家 (Information Expert) 为对象分配职责的通用原则 – 把职责分配给拥有足够信息可以履行职责的专家 2. 创建者 (Creator) 将创建 A 的职责赋给 B，如果至少下面一种情况为真： B“包含”或者聚合 A B 记录 A 的实例 B 密切地使用 A B 拥有 A 的初始化数据 3. 低耦合 (Low Coupling) 赋予职责使得对象间的耦合度尽可能低，最小化对象间的依赖和变更影响，最大化重用。 4. 高内聚 (High Cohesion) 赋予职责使得每个对象的职责尽可能保持聚焦和单一，易于管理和理解。 5. 控制器 (Controller) 把职责赋予系统、设备或者子系统的表示类 (门面控制器)，或者某个用例的表示类 (用例控制器)，让控制器接收事件并协调整个系统的运作。 6. 多态 (Polymorphism) 将职责分配给多个具有同名方法的多态子类，运行时根据需要动态切换子类，让系统行为变得可插拔。 7. 纯虚构 (Pure Fabrication) 针对真实问题域中不存在，但是设计建模中有用的概念，设计虚构类并赋予职责。 8. 间接 (Indirection) 在两个或者多个对象间有交互的情况下，为避免直接耦合，提高重用性，创建中间类并赋予职责，对象的交互交由中间类协调。 9. 受保护的变化 (Protected Variation) 简单讲就是封装变化。识别系统中可能的不稳定或者变化，在不稳定组件上创建稳定的抽象接口，将可能的变化封装在接口之后，使得系统内部的不稳定或者变化不会对系统的其它部分产生不良影响。 SOLID 面向对象设计原则S.O.L.I.D 是面向对象设计和编程 (OOD&amp;OOP) 中几个重要原则的首字母缩写，受 Robert Martin 推崇。 1. 单一职责原则 (The Single Responsibility Principle) 修改某个类的理由应该只有一个，如果超过一个，说明类承担不止一个职责，要视情况拆分。 2. 开放封闭原则 (The Open Closed Principle) 软件实体应该对扩展开放，对修改封闭。一般不要直接修改类库源码（即使你有源代码），通过继承等方式扩展。 3. 里氏替代原则 (The Liskov Substitution Principle) 当一个子类的实例能够被替换成任何超类的实例时，它们之间才是真正的 is-a 关系。 4. 依赖倒置原则 (The Dependency Inversion Principle) 高层模块不应该依赖于底层模块，二者都应该依赖于抽象。换句话说，依赖于抽象，不要依赖于具体实现。比方说，你不会把电器电源线焊死在室内电源接口处，而是用标准的插头插在标准的插座 (抽象) 上。 5. 接口分离原则 (The Interface Segregation Principle) 不要强迫用户去依赖它们不使用的接口。换句话说，使用多个专门的接口比使用单一的大而全接口要好。 备注 高内聚 + 低耦合，就像道中的一阴一阳，是所有其它 OO 设计原则的原则 (元原则)，其它设计原则都是在这两个基础上泛化衍生出来的。 上述原则虽然是针对 OO 设计和编程提出，但是对于大规模系统架构仍然适用。比如，微服务架构就体现了： 单一职责：一个微服务尽可能要职责单一，提供的接口也尽可能单一 (接口分离原则)，安全 / 路由 / 限流等跨横切面的关注点 (Cross-Cutting Concerns) 由独立网关负责，体现关注分离 (Separation of Concerns)。 信息专家：当不确定哪个团队应该负责某个微服务时，一般原则也是谁拥有数据谁负责，基于有界上下文 Bounded Context（一般是边界比较清晰的领域数据源）构建微服务。 松散耦合：服务之间通过 HTTP/JSON 等轻量机制通信，服务之间不强耦合。 受保护的变化和依赖倒置：服务之间只依赖抽象接口，实现可能随时变化。 间接：网关在外面的客户端和内部的服务之间增加了一层间接，使两者不强耦合，可以相互独立演化。 作为架构师或者设计师，有两个设计能力是需要重点培养的，也是最难和最能体现架构设计水平的： 合理的职责分配能力，也就是每个类 / 组件 / 子系统应该承担什么职责，如何保证职责单一，它们之间如何协作； 系统抽象和核心领域建模能力，需要深入一线业务域。 分布式系统架构设计原则和理论AKF 架构原则这 15 个架构原则来自《架构即未来 (The Art of Scalability)》[附录 2] 一书，作者马丁 L. 阿伯特和迈克尔 T. 费舍尔分别是 eBay 和 PayPal 的前 CTO，他们经历过 eBay 和 PayPal 大规模分布式电商平台的架构演进，在一线实战经验的基础上总结并提炼出 15 条架构原则： 1.N + 1 设计 永远不要少于两个，通常为三个。比方说无状态的 Web/API 一般部署至少&gt;=2 个。 2. 回滚设计 确保系统可以回滚到以前发布过的任何版本。可以通过发布系统保留历史版本，或者代码中引入动态开关切换机制 (Feature Switch)。 3. 禁用设计 能够关闭任何发布的功能。新功能隐藏在动态开关机制 (Feature Switch) 后面，可以按需一键打开，如发现问题随时关闭禁用。 4. 监控设计 在设计阶段就必须考虑监控，而不是在实施完毕之后补充。例如在需求阶段就要考虑关键指标监控项，这就是度量驱动开发 (Metrics Driven Development) 的理念。 5. 设计多活数据中心 不要被一个数据中心的解决方案把自己限制住。当然也要考虑成本和公司规模发展阶段。 6. 使用成熟的技术 只用确实好用的技术。商业组织毕竟不是研究机构，技术要落地实用，成熟的技术一般坑都被踩平了，新技术在完全成熟前一般需要踩坑躺坑。 7. 异步设计 能异步尽量用异步，只有当绝对必要或者无法异步时，才使用同步调用。 8. 无状态系统 尽可能无状态，只有当业务确实需要，才使用状态。无状态系统易于扩展，有状态系统不易扩展且状态复杂时更易出错。 9. 水平扩展而非垂直升级 永远不要依赖更大、更快的系统。一般公司成长到一定阶段普遍经历过买更大、更快系统的阶段，即使淘宝当年也买小型机扛流量，后来扛不住才体会这样做不 scalable，所以才有后来的去 IOE 行动。 10. 设计时至少要有两步前瞻性 在扩展性问题发生前考虑好下一步的行动计划。架构师的价值就体现在这里，架构设计对于流量的增长要有提前量。 11. 非核心则购买 如果不是你最擅长，也提供不了差异化的竞争优势则直接购买。避免 Not Invented Here 症状，避免凡事都要重造轮子，毕竟达成业务目标才是重点。 12. 使用商品化硬件 在大多数情况下，便宜的就是最好的。这点和第 9 点是一致的，通过商品化硬件水平扩展，而不是买更大、更快的系统。 13. 小构建、小发布和快试错 全部研发要小构建，不断迭代，让系统不断成长。这个和微服务理念一致。 14. 隔离故障 实现故障隔离设计，通过断路保护避免故障传播和交叉影响。通过舱壁泳道等机制隔离失败单元 (Failure Unit)，一个单元的失败不至影响其它单元的正常工作。 15. 自动化 设计和构建自动化的过程。如果机器可以做，就不要依赖于人。自动化是 DevOps 的基础。 备注 这 15 条架构原则基本上是 eBay 在发展，经历过流量数量级增长冲击过程中，通过不断踩坑踩出来的，是干货中的干货。消化吸收这 15 条原则，基本可保系统架构不会有原则性问题。 这 15 条原则同样适用于现在的微服务架构。eBay 发展较早，它内部其实很早 (差不多 2010 年前) 就已形成完善的微服务生态，只是没有提出微服务这个概念。 这 15 条原则可根据 TTM(Time To Market)，可用性 / 可扩展性 / 质量，成本 / 效率分布在三个环内，如下图所示。 12 要素应用基于上百万应用的托管和运营经验，创始人 Adam Wiggins 提出了 12 要素应用宣言 。简单讲，满足这 12 个要素的应用是比较容易云化并居住在 Heroku 平台上的。 1. 基准代码 一份基准代码，多份部署。如果用镜像部署方式，则一个镜像可以部署到多个环境 (测试，预发，生产)，而不是给每个环境制作一个不同镜像。 2. 依赖 显式声明依赖。如果用镜像部署，则一般依赖被直接打在镜像中，或者声明在 docker file 中。 3. 配置 在环境中存储配置。在 Heroku 或者类似的 PaaS 平台上，配置一般是推荐注入到环境变量中的。现在采用集中式配置中心也是一种流行方式。 4. 后端服务 把后端服务 (例如缓存，数据库，MQ 等) 当作附加资源，相关配置和连接字符串通过环境变量注入，或者采用配置中心。 5. 构建、发布和运行 严格分离构建和运行。如果使用镜像部署，则构建、发布 / 运行是通过镜像这种中间格式严格分离的。 6. 进程 一个或者多个无状态的进程运行应用。容器运行时相当于进程，适用于无状态 Web/API。 7. 端口绑定 通过端口绑定提供服务。容器也是通过端口绑定对外提供服务。 8. 并发 通过进程模型进行扩展。容器运行时相当于进程，通过起多个容器可以任意扩展并发数量。 9. 易处理 快速启动和优雅终止可最大化健壮性。docker 容器支持秒级启动和关闭。 10. 开发环境和线上环境等价 尽可能保持开发、测试、预发和线上环境相同。容器可以保证容器内运行时环境的一致性，还需要保证不同环境的一致性，例如不同环境内的操作系统，负载均衡，服务发现，后台服务，监控告警等要尽可能一致。 11. 日志 把日志当作数据流。Heroku 不支持本地文件，所以必须以流方式把日志输送到后台日志服务。除了日志以外还要补充考虑 metrics 流的采集和输送。 12. 管理进程 后台管理任务当作一次性的进程。其实相当于在 Heroku 上以独立进程方式运行任务 Job。 备注 12 要素应用也是当前云原生应用 (Cloud Native App) 的参考标准，也称为云应用迁移原则。满足这 12 个要素的应用，可以比较顺利迁移到各种云平台 (Kubernetes, Marathon, Cloud Foundry 等) 上。 对于面临企业遗留应用改造和云化迁移的架构师，可以重点参考这 12 条迁移原则。 Docker 容器技术可以认为是为云迁移量身定制的技术。容器化是后续云迁移的捷径，所以遗留应用改造可以先想办法做到容器化。 CAP 定理2000 年 7 月，加州大学伯克利分校的 Eric Brewer 教授在 ACM PODC 会议上提出 CAP 猜想。2 年后，麻省理工学院的 Seth Gilbert 和 Nancy Lynch 从理论上证明了 CAP。之后，CAP 理论正式成为分布式计算领域的公认定理。 CAP 认为：一个分布式系统最多同时满足一致性 (Consistency)，可用性 (Availability) 和分区容忍性 (Partition Tolerance) 这三项中的两项。 1.一致性 (Consistency) 一致性指“all nodes see the same data at the same time”，即更新操作成功，所有节点在同一时间的数据完全一致。 2.可用性 (Availability) 可用性指“Reads and writes always succeed”，即服务一直可用，而且响应时间正常。 3.分区容忍性 (Partition tolerance) 分区容忍性指“the system continue to operate despite arbitrary message loss or failure of part of the system.”，即分布式系统在遇到某节点或网络分区故障时，仍然能够对外提供满足一致性和可用性的服务。 BASE 理论eBay 架构师 Dan Pritchett 基于对大规模分布式系统的实践总结，在 ACM 上发表文章提出了 BASE 理论，BASE 理论是对于 CAP 理论的延伸，核心思想是即使无法做到强一致性 (Strong Consistency，CAP 中的一致性指强一致性)，但是可以采用适当的方式达到最终一致性 (Eventual Consistency)。 BASE 指基本可用 (Basically Available)、软状态 (Soft State) 和最终一致性 (Eventual Consistency)。 1.基本可用 (Basically Available) 基本可用是指分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。比如服务降级。 2.软状态 (Soft State) 软状态是指允许系统存在中间状态，而该中间状态不会影响系统的整体可用性。分布式存储中一般一份数据至少存三个副本，允许不同节点间副本同步的延迟就是软状态的体现。 3.最终一致性 (Eventual Consistency) 最终一致性是指系统中的所有数据副本经过一段时间后，最终能够达成一致状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。 备注 CAP 和 BASE 理论可以抠得很深，背后甚至有很复杂的数学证明。我理解得相对简单浅显：性能、高可用、不丢数据和数据一致性对分布式系统来说一般是强需求，随着流量的增长，复制和分区在所难免： 复制 (replication)：数据在多个节点上存多份保证不丢和高可用； 分区 (partition)：数据按某个纬度切分分布在不同节点上分摊流量压力保证高性能，同时也是为了降低每个节点的复杂性。例如数据库的分库分表，系统拆分微服务化也是一种分区。这两者都会带来一致性问题，一致性在时间上有一点妥协的余地 - 即是最终一致性；时间上要求强一致的话，只有可用性可以适当折中。系统架构的游戏很大部分是和状态一致性作斗争的游戏。 选择使用分布式产品时，比如 NoSQL 数据库，你需要了解它在 CAP 环中所在的位置，确保它满足你的场景需要。 组织和系统改进原则康威法则Melvin Conway 在 1967 年提出所谓康威法则 ，指出组织架构和系统架构之间有一种隐含的映射关系： Organization which design system […] are constrained to produce designs which are copies of the communication structures of these organization. 设计系统的组织其产生的设计等价于组织间的沟通结构。 康威法则也可以倒过来阐述： Conway’s law reversed：You won’t be able to successfully establish an efficient organization structure that is not supported by your system design(architecture)。 如果系统架构不支持，你无法建立一个高效的组织；同样，如果你的组织架构不支持，你也无法建立一个高效的系统架构。 系统改进三原则IT 运维管理畅销书《凤凰项目》[附录 8] 的作者 Gene Kim 在调研了众多高效能 IT 组织后总结出支撑 DevOps 运作的三个原理 (The Three Ways: The Principles Underpinning DevOps)[附录 9]，我认为也是系统改进提升的一般性原理 [附录 7]，见下图： 原理一：系统思考 (System Thinking) 开发驱动的组织，其能力不是制作软件，而是持续的交付客户价值。价值从业务需求开始，经过研发测试，到部署运维，依次流动，并最终以服务形式交付到客户手中。整个价值链流速并不依赖单个部分 (团队或个人) 的杰出工作，而是受整个价值链最薄弱环节 (瓶颈) 的限制。所以局部优化通常无效，反而招致全局受损。 Gene Kim 特别指出：Any improvements made anywhere besides the bottleneck are an illusion. 在瓶颈之外的任何优化提升都只是幻象。 原理二：强化反馈环 (Amplify Feedback Loops) 过程改进常常通过加强反馈环来达成。原理二强调企业和客户之间、组织团队间、流程上和系统内的反馈环。没有测量就没有提升，反馈要以测量数据为准，通过反馈数据优化改进系统。 原理三：持续试验和学习的文化 (Culture of Continual Experimentation And Learning) 在企业管理文化层面强调勇于试错和持续试验、学习和改进的文化。 备注 康威法则给我们的启示：系统架构和组织架构之间有隐含的映射关系，你不能单方面改变一方的结构，调整时必须两边联动。系统架构如果是耦合的，就很难组织分散式的团队结构，两边映射不起来，团队之间容易摩擦导致生产率下降。所以一般先按业务边界对单块应用进行解耦拆分，同时做相应的团队拆分，使两边可以映射，每个团队可以独立开发、测试和部署各自的微服务，进而提升生产率。这就是近年流行的微服务架构背后的组织原则。详见我之前发表的文章《企业的组织架构是如何影响技术架构的》[附录 6]。 系统思考要求我们加强团队合作，培养流式思维和瓶颈约束思维，找出瓶颈并针对性地优化。在研发型组织中，常见的系统瓶颈如运维机器资源提供 (Provisioning) 缓慢，发布流程繁琐容易出错，开发 / 测试／UAT 环境缺失或不完善，遗留系统耦合历史负担重，基础研发平台薄弱等等。这些瓶颈点特别需要关注优化。 反馈原理要求我们关注基于数据的反馈，技术上的手段包括大数据分析和系统各个层次的测量监控。没有测量就没有反馈，没有反馈就没有提升。 在管理文化层面： 管理层要承认企业内部近 50% 的创新或流程改进项目是有可能失败的，即使失败，员工不会受到责罚，鼓励持续的试验和从中学习； 管理层要有技术偿债意识，勿追求 100% 员工利用率，要预留 20%~30% 的时间给员工做创新和系统改进提升项目]]></content>
      <categories>
        <category>Design</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[秒杀系统优化思路]]></title>
    <url>%2Fbcca3074%2F</url>
    <content type="text"><![CDATA[一、秒杀业务为什么难做1）im系统，例如qq或者微博，每个人都读自己的数据（好友列表、群列表、个人信息）； 2）微博系统，每个人读你关注的人的数据，一个人读多个人的数据； 3）秒杀系统，库存只有一份，所有人会在集中的时间读和写这些数据，多个人读一个数据。 例如：小米手机每周二的秒杀，可能手机只有1万部，但瞬时进入的流量可能是几百几千万。 又例如：12306抢票，票是有限的，库存一份，瞬时流量非常多，都读相同的库存。读写冲突，锁非常严重，这是秒杀业务难的地方。那我们怎么优化秒杀业务的架构呢？ 二、优化方向优化方向有两个（今天就讲这两个点）： （1）将请求尽量拦截在系统上游（不要让锁冲突落到数据库上去）。传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的有效流量甚小。以12306为例，一趟火车其实只有2000张票，200w个人来买，基本没有人能买成功，请求有效率为0。 （2）充分利用缓存，秒杀买票，这是一个典型的读多写少的应用场景，大部分请求是车次查询，票查询，下单和支付才是写请求。一趟火车其实只有2000张票，200w个人来买，最多2000个人下单成功，其他人都是查询库存，写比例只有0.1%，读比例占99.9%，非常适合使用缓存来优化。好，后续讲讲怎么个“将请求尽量拦截在系统上游”法，以及怎么个“缓存”法，讲讲细节。 三、常见秒杀架构（1）浏览器端，最上层，会执行到一些JS代码 （2）站点层，这一层会访问后端数据，拼html页面返回给浏览器 （3）服务层，向上游屏蔽底层数据细节，提供数据访问 （4）数据层，最终的库存是存在这里的，mysql是一个典型（当然还有会缓存） 四、各层次优化细节第一层，客户端怎么优化（浏览器层，APP层）微信的摇一摇抢红包，每次摇一摇，就会往后端发送请求么？下单抢票的场景，点击了“查询”按钮之后，系统那个卡呀，进度条涨的慢呀，作为用户，我会不自觉的再去点击“查询”，对么？继续点，继续点，点点点。。。有用么？平白无故的增加了系统负载，一个用户点5次，80%的请求是这么多出来的，怎么整？ （a）产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求； （b）JS层面，限制用户在x秒之内只能提交一次请求； APP层面，可以做类似的事情，虽然你疯狂的在摇微信，其实x秒才向后端发起一次请求。这就是所谓的“将请求尽量拦截在系统上游”，越上游越好，浏览器层，APP层就给拦住，这样就能挡住80%+的请求，这种办法只能拦住普通用户（但99%的用户是普通用户）对于群内的高端程序员是拦不住的。firebug一抓包，http长啥样都知道，js是万万拦不住程序员写for循环，调用http接口的，这部分请求怎么处理？ 第二层，站点层面的请求拦截怎么拦截？怎么防止程序员写for循环调用，有去重依据么？ip？cookie-id？…想复杂了，这类业务都需要登录，用uid即可。在站点层面，对uid进行请求计数和去重，甚至不需要统一存储计数，直接站点层内存存储（这样计数会不准，但最简单）。一个uid，5秒只准透过1个请求，这样又能拦住99%的for循环请求。 5s只透过一个请求，其余的请求怎么办？缓存，页面缓存，同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面。同一个item的查询，例如车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面。如此限流，既能保证用户有良好的用户体验（没有返回404）又能保证系统的健壮性（利用页面缓存，把请求拦截在站点层了）。 页面缓存不一定要保证所有站点返回一致的页面，直接放在每个站点的内存也是可以的。优点是简单，坏处是http请求落到不同的站点，返回的车票数据可能不一样，这是站点层的请求拦截与缓存优化。 好，这个方式拦住了写for循环发http请求的程序员，有些高端程序员（黑客）控制了10w个肉鸡，手里有10w个uid，同时发请求（先不考虑实名制的问题，小米抢手机不需要实名制），这下怎么办，站点层按照uid限流拦不住了。 第三层 服务层来拦截（反正就是不要让请求落到数据库上去）服务层怎么拦截？大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？没错，请求队列！ 对于写请求，做请求队列，每次只透有限的写请求去数据层（下订单，支付这样的写业务） 1w部手机，只透1w个下单请求去db 3k张火车票，只透3k个下单请求去db 如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”。 对于读请求，怎么优化？cache抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的。如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了。 当然，还有业务规则上的一些优化。回想12306所做的，分时分段售票，原来统一10点卖票，现在8点，8点半，9点，…每隔半个小时放出一批：将流量摊匀。 其次，数据粒度的优化：你去购票，对于余票查询这个业务，票剩了58张，还是26张，你真的关注么，其实我们只关心有票和无票？流量大的时候，做一个粗粒度的“有票”“无票”缓存即可。 第三，一些业务逻辑的异步：例如下单业务与 支付业务的分离。这些优化都是结合 业务 来的，我之前分享过一个观点“一切脱离业务的架构设计都是耍流氓”架构的优化也要针对业务。 第四层 最后是数据库层浏览器拦截了80%，站点层拦截了99.9%并做了页面缓存，服务层又做了写请求队列与数据缓存，每次透到数据库层的请求都是可控的。db基本就没什么压力了，闲庭信步，单机也能扛得住，还是那句话，库存是有限的，小米的产能有限，透这么多请求来数据库没有意义。 全部透到数据库，100w个下单，0个成功，请求有效率0%。透3k个到数据，全部成功，请求有效率100%。 五、总结上文应该描述的非常清楚了，没什么总结了，对于秒杀系统，再次重复下我个人经验的两个架构优化思路： （1）尽量将请求拦截在系统上游（越上游越好）； （2）读多写少的常用多使用缓存（缓存抗读压力）； 浏览器和APP：做限速 站点层：按照uid做限速，做页面缓存 服务层：按照业务做写请求队列控制流量，做数据缓存 数据层：闲庭信步 并且：结合业务做优化]]></content>
      <categories>
        <category>Design</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[制作种子]]></title>
    <url>%2F7592d71b%2F</url>
    <content type="text"><![CDATA[1.下载mktorrentgit clone https://github.com/lxbwolf/mktorrent.git2.下载完成后进入到文件夹里面例如：cd mktorrent（如果是根目录的话）3. make4. make install5. 默认安装目录位于/usr/local/bin，使用cd命令，从默认的/root路径切换到要制作成种子的文件上一级。 例如cd /Downloads6. 制作种子命令为： mktorrent -v -p -l 22 -a tracker_address -o name.torrent file_name参数说明： tracker_address为你要发布的网站的tracker。 name.torrent为对生成torrent种子文件的命名，规则为：xxx.torrent。 file_name为你要做种的文件或文件夹。避免含有空格。7. 等待一会儿会提示做种完成，在当前目录下即可找到。]]></content>
      <categories>
        <category>Rasp</category>
      </categories>
      <tags>
        <tag>torrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派搭建迅雷远程下载服务器]]></title>
    <url>%2F1846a864%2F</url>
    <content type="text"><![CDATA[1. 下载路由器固件从 官网 或者 百度网盘 解压到指定目录如 /root/xunlei 进入目录 执行./portal 稍等片刻，会在最后输出一个激活码 2. 在迅雷远程下载页面绑定树莓派登录迅雷远程下载主页,登录之后，左侧会有一个添加按钮，点击添加按钮 不需要选择绑定设备类型, 直接将树莓派上获得的激活码填入框中，点击绑定后左侧就会出现树莓派对应的设备列表了，但是，如果我们此时就在右侧点击新建之后会发现,弹出的新建页面中会提示找不到挂载磁盘 3. 自定义迅雷的下载目录进入/mnt目录，创建目录TDDOWNLOAD(名字随意) 执行mount --bind /data/TDDOWNLOAD /mnt/TDDOWNLOAD 其中/data/TDDOWNLOAD就是自定义的下载目录，你可以指定为其他任何目录。 然后再刚刚迅雷固件的解压目录下创建目录etc,同时在etc下创建文件thunder_mounts.cfg,编辑此文件, 写入内容 avaliable_mount_path_pattern { /mnt/TDDOWNLOAD }重启路由器固件 ./root/xunlei/portal 再进入远程下载界面新建下载就没有了没挂载磁盘的提示了 4. 迅雷路由器固件开机启动在/etc/init.d/下新建xunlei脚本，写入: #!/bin/sh # # Xunlei initscript # ### BEGIN INIT INFO # Provides: xunlei # Required-Start: $network $local_fs $remote_fs # Required-Stop:: $network $local_fs $remote_fs # Should-Start: $all # Should-Stop: $all # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start xunlei at boot time # Description: A downloader ### END INIT INFO do_start() { ./root/xunlei/portal } do_stop() { ./root/xunlei/portal -s } case &quot;$1&quot; in start) do_start ;; stop) do_stop ;; esac然后将该脚本加入默认自启动中 update-rc.d xunlei defaults]]></content>
      <categories>
        <category>Rasp</category>
      </categories>
      <tags>
        <tag>thunder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派基础环境]]></title>
    <url>%2F5976aace%2F</url>
    <content type="text"><![CDATA[修改软件源sudo -s echo -e &quot;deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi \n deb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi&quot; &amp;gt; /etc/apt/sources.list echo -e &quot;deb http://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui&quot; &amp;gt; /etc/apt/sources.list.d/raspi.list exit sudo apt update &amp;amp;&amp;amp; sudo apt -y upgrade中文输入法sudo apt-get install -y ttf-wqy-zenhei sudo apt-get install -y scim-pinyin看门狗(防止树莓派死机的监控)当利用树莓派来做一些需要长期待机的应用时，如下载机、云储存、家庭影院等应用，我们往往会遇到的一个问题就是树莓派会因为过热而死机，需要我们重新启动树莓派，然后再次开启树莓派上的应用。这会给我们的日常操作带来许多麻烦。 Watchdog（看门狗）就能让树莓派永不死机。 //树莓派自带看门狗模块，我们需要添加进去就好。 sudo modprobe bcm2708_wdog echo -e &quot;\nbcm2708_wdog&quot; &amp;gt; sudo tee -a /etc/modules // 安装看门狗软件 sudo apt-get install -y chkconfig watchdog // 配置 sudo vim /etc/watchdog.conf // 去掉&quot;watchdog-device=/dev/watchdog&quot;这一行的#注释 // 其它配置参考如下: # 用于设定CPU温度重启条件 temperature-device = /sys/class/thermal/thermal_zone0/temp # 最大温度为100度，超过立即重启 max-temperature = 100000 # 1分钟最多进程为24个，超过即重启 max-load-15=12 # 5分钟最多进程为18个，超过即重启 max-load-15=12 # 15分钟最多进程为12个，超过即重启 max-load-15=12 // 完成配置后，启动看门狗 sudo /etc/init.d/watchdog start // 设置为开机自启 chkconfig watchdog onScreen(让树莓派永不失联)利用SSH（Serare Shell，安全外壳协议）来远程控制树莓派应该是我们最常用的 操作树莓派的方式，但在用SSH连接时，我们常常会遇到连接突然断开的问题。连 接一旦断开，原米我们进行的操作也就中断了，若再使用，就得从头再来了。相信你肯定因为电脑待机而中断树莓派的任务而苦恼过。 Screen来让树莓派永不失联的方法。此方法下，就算连接断开了，当我们重新连接后依旧进行原来的操作，而不需要从头再来。 // 直接安装Screen sudo apt-get install -y screen // 开启一个后台view（后台的终端，不会因为断开连接而终止） screen -S 终端名 // 然后就可以继续你的操作了]]></content>
      <categories>
        <category>Rasp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树莓派3B+ 安装系统]]></title>
    <url>%2Fcfbe6b0a%2F</url>
    <content type="text"><![CDATA[安装步骤: 官网下载系统 -- 刷入TF卡 -- 设置开启显示器和SSH -- 通电 -- 进入系统 1. 进入官方网站下载系统镜像官方系统 raspbian地址 https://www.raspberrypi.org/downloads/ 2. Windows系统下的安装2.1 下载SD格式化工具SDFormatter 地址 https://www.sdcard.org/downloads/formatter\_4/eula\_windows/ 安装后直接用默认选项 格式化SD卡 2.2 下载写镜像工具Win32 DiskImager 地址 http://sourceforge.net/projects/win32diskimager/ 3. MAC系统下的安装3.1 查看当前已挂载的卷[liuxb@liuxb-mac]$ df -h Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk1 112Gi 81Gi 30Gi 73% 1014786 4293952493 0% / devfs 188Ki 188Ki 0Bi 100% 654 0 100% /dev map -hosts 0Bi 0Bi 0Bi 100% 0 0 100% /net map auto_home 0Bi 0Bi 0Bi 100% 0 0 100% /home /dev/disk2s3 92Gi 51Gi 41Gi 56% 336662 42525054 1% /Volumes/系统 /dev/disk2s4 20Gi 15Gi 4.4Gi 78% 92859 4579733 2% /Volumes/数据 /dev/disk3s1 29Gi 2.3Mi 29Gi 1% 107876 8373436 2% /Volumes/未命名对比Size和Name可以找到SD卡的分区在系统里对应的设备文件（这里是/dev/disk3s1），如果你有多个分区，可能还会有disk3s2之类的 3.2 使用diskutil unmount将分区卸载[liuxb@liuxb-mac]$ diskutil unmount /dev/disk3s1 Volume 未命名 on disk3s1 unmounted3.3 先对下载的zip压缩包进行解压，然后使用dd命令将系统镜像写入，需要特别特别注意disk后的数字，不能搞错说明：/dev/disk3s1是分区，/dev/disk3是块设备，/dev/rdisk3是原始字符设备 [liuxb@liuxb-mac]$ unzip 2017-09-07-raspbian-stretch.zip [liuxb@liuxb-mac]$ sudo dd bs=16m if=2017-09-07-raspbian-stretch.img of=/dev/rdisk3 _ 输入用户密码经过几分钟的等待，出现下面的提示，说明TF卡刷好了： 1172+1 records in 1172+1 records out 4916019200 bytes transferred in 127.253638 secs (9691442 bytes/sec)4. 开启SSH在TF卡分区里创建一个名为”ssh”的不带后缀的空文件 5. 开启强制HDMI输出在TF卡分区，打开config.txt文件(开机后位置： /boot/config.txt)，修改如下： hdmi_safe=1 config_hdmi_boost=4 hdmi_ignore_edid=0xa5000080 hdmi_group=2 hdmi_mode=82参数介绍: 项 解释 hdmi_safe=1 安全启动HDMI config_hdmi_boost=4 开启热插拔 hdmi_group=1 CEA电视显示器 hdmi_group=2 DMT电脑显示器 hdmi_ignore_edid=0xa5000080 忽略自动探测的分辨率 输出分辨率： hdmi_mode=4 640x480 60Hz hdmi_mode=9 800x600 60Hz hdmi_mode=16 1024x768 60Hz hdmi_mode=82 1080p 60Hz 6.设置无线WI-FI连接：（假设没有网线，而且没能连接显示器）在TF卡的boot分区，创建wpa_supplicant.conf文件，加入如下内容： country=CN ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=&quot;lxb-wifi&quot; psk=&quot;123456789&quot; priority=1 }在树莓派通电后会自动添加到/etc/wpa_supplicant/wpa_supplicant.conf文件里面，进行自动连接。 // 详细介绍： #ssid:网络的ssid #psk:密码 #priority:连接优先级，数字越大优先级越高（不可以是负数） #scan_ssid:连接隐藏WiFi时需要指定该值为1 // 如果WiFi 没有密码 network={ ssid=&quot;无线网络名称（ssid）&quot; key_mgmt=NONE } // 如果WiFi 使用WEP加密 network={ ssid=&quot;无线网络名称（ssid）&quot; key_mgmt=NONE wep_key0=&quot;wifi密码&quot; } // 如果你的 WiFi 使用WPA/WPA2加密 network={ ssid=&quot;无线网络名称（ssid）&quot; key_mgmt=WPA-PSK psk=&quot;wifi密码&quot; }以上设置完成后, TF卡可以插入树莓派了, 通电. 默认登录账号:pi 密码: raspberry]]></content>
      <categories>
        <category>Rasp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Go 发送邮件]]></title>
    <url>%2Fc296dcc8%2F</url>
    <content type="text"><![CDATA[需要引入 smtp包 mail.go package main import ( &quot;bytes&quot; &quot;encoding/base64&quot; &quot;fmt&quot; &quot;io/ioutil&quot; &quot;net/smtp&quot; &quot;strings&quot; ) const const_smtp_server = &quot;server-ip:port&quot; //const const_email_content_type = &quot;Content-Type: text/plain; charset=UTF-8&quot; const const_email_content_type = &quot;Content-Type: text/html; charset=UTF-8&quot; const const_boundary = &quot;THIS_IS_THE_BOUNDARY_FOR_EMAIL_BY_LXB&quot; func SendEmail(sender string, receivers []string, subject string, content string, attach_files []string) error { var buf bytes.Buffer buf.WriteString(&quot;To: &quot;) buf.WriteString(strings.Join(receivers, &quot;,&quot;)) buf.WriteString(&quot;\r\nFrom: &quot;) //nickname := strings.Split(sender,&quot;@&quot;)[0] //buf.WriteString(nickname) buf.WriteString(&quot;&lt;&quot;) buf.WriteString(sender) buf.WriteString(&quot;&gt;&quot;) buf.WriteString(&quot;\r\nSubject: &quot;) buf.WriteString(subject) buf.WriteString(&quot;\r\nContent-Type: multipart/mixed; boundary=&quot;) buf.WriteString(const_boundary) buf.WriteString(&quot;\r\n--&quot;) buf.WriteString(const_boundary) buf.WriteString(&quot;\r\n&quot;) buf.WriteString(const_email_content_type) buf.WriteString(&quot;\r\n\r\n&quot;) buf.WriteString(content) buf.WriteString(&quot;\r\n\r\n--&quot;) buf.WriteString(const_boundary) buf.WriteString(&quot;\r\n&quot;) for _, filepath := range attach_files { // 第一个附件 filedepts := strings.Split(filepath, &quot;/&quot;) filename := filedepts[len(filedepts)-1] buf.WriteString(&quot;Content-Type: application/octet-stream\r\n&quot;) buf.WriteString(&quot;Content-Description: 附件\r\n&quot;) buf.WriteString(&quot;Content-Transfer-Encoding: base64\r\n&quot;) buf.WriteString(&quot;Content-Disposition: attachment; filename=\&quot;&quot; + filename + &quot;\&quot;\r\n\r\n&quot;) //读取并编码文件内容 attaData, err := ioutil.ReadFile(filepath) if err != nil { print(err) return err } b := make([]byte, base64.StdEncoding.EncodedLen(len(attaData))) base64.StdEncoding.Encode(b, attaData) buf.Write(b) buf.WriteString(fmt.Sprintf(&quot;\r\n--%s\r\n&quot;, const_boundary)) } fmt.Println(buf.String()) err := smtp.SendMail(const_smtp_server, nil, sender, receivers, buf.Bytes()) fmt.Println(&quot;send mail err:&quot;, err) return err }main.go package main import ( //&quot;flag&quot; //&quot;fmt&quot; &quot;os&quot; ) func main() { //var task string //flag.StringVar(&amp;task, &quot;t&quot;, &quot;&quot;, &quot;task id&quot;) //flag.Parse() //if task == &quot;&quot; { // fmt.Println(&quot;task is required.&quot;) // flag.Usage() // os.Exit(2) //} testStr := os.Args[1] cont := &quot;&lt;html&gt;&lt;body&gt;&lt;p align=\&quot;center\&quot;&gt;表: 1&lt;/p&gt;&lt;table align=\&quot;center\&quot; border=\&quot;1\&quot; cellpadding=\&quot;10\&quot;&gt;&lt;tr&gt;&lt;td&gt;任务ID&lt;/td&gt;&lt;td&gt;列1&lt;/td&gt;&lt;td&gt;列2&lt;/td&gt;&lt;td&gt;列3&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;&quot; sender := &quot;&quot; rcvs := []string{} sbj := &quot;test email&quot; // cont := &quot;This is content&quot; file := []string{} SendEmail(sender, rcvs, sbj, cont, file) }]]></content>
      <categories>
        <category>Golang</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell数组笔记]]></title>
    <url>%2F21c4d609%2F</url>
    <content type="text"><![CDATA[Bash shell 只支持一维数组. 初始化时不需要定义数组大小(与 PHP 类似). 数组元素的下标由0开始 shell 数组用括号来表示, 元素用”空格”符号分隔开, 语法: array_name=(value1 value2 ...valuen) 实例#!/bin/bash my_array=(A B &quot;C&quot; D)也可以用下标来定义数组 array_name[0]=value0 array_name[1]=value1 array_name[2]=value2读取数组${array_name[index]} 实例#!/bin/bash my_array=(A B &quot;C&quot; D) echo &quot;第一个元素为: ${my_array[0]}&quot; echo &quot;第二个元素为: ${my_array[1]}&quot; echo &quot;第三个元素为: ${my_array[2]}&quot; echo &quot;第四个元素为: ${my_array[3]}&quot;获取数组中的所有元素使用@ 或 * 可以后去数组中的所有元素 #!/bin/bash my_array[0]=A my_array[1]=B my_array[2]=C my_array[3]=D echo &quot;数组的元素为: ${my_array[*]}&quot; echo &quot;数组的元素为: ${my_array[@]}&quot;获取数组的长度获取数组长度的方法与获取字符串长度的方法相同 #!/bin/bash my_array[0]=A my_array[1]=B my_array[2]=C my_array[3]=D echo &quot;数组元素个数为: ${#my_array[*]}&quot; echo &quot;数组元素个数为: ${#my_array[@]}&quot;]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Split命令]]></title>
    <url>%2Faa4c47b6%2F</url>
    <content type="text"><![CDATA[选项-b 值为每一个输出档案的大小, 单位为byte -C 每一个输出档中, 单行的最大byte 数 -d 使用数字作为后缀 -l 值为每一个输出档的行数大小实例生成一个大小为100KB 的测试文件 dd if=/dev/zero bs=100k count=1 of=date.file 1+0 records in 1+0 records out 102400 bytes (102 kB) copied, 0.00043 seconds, 238 MB/s使用split 命令将上面创建的date.file文件分割成大小为10KB 的小文件 $ split -b 10k date.file $ ls date.file xaa xab xac xad xae xaf xag xah xai xaj文件被分割成带有字母的后缀文件, 如果想用数字后缀可使用-d参数, 同时可以使用-a length指定后缀的长度 [root@localhost split]# split -b 10k date.file -d -a 3 [root@localhost split]# ls date.file x000 x001 x002 x003 x004 x005 x006 x007 x008 x009为分割后的文件指定文件名的前缀 [root@localhost split]# split -b 10k date.file -d -a 3 split_file [root@localhost split]# ls date.file split_file000 split_file001 split_file002 split_file003 split_file004 split_file005 split_file006 split_file007 split_file008 split_file009使用-l选项根据文件的行数来分割文件,如把文件分割成每个包含10行的小文件 split -l 10 date.file]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mysql启动时 &quot;No Space Left on Device&quot;]]></title>
    <url>%2F77f38978%2F</url>
    <content type="text"><![CDATA[先用free 命令查看剩余空间 [root@tokyo mysqld]# free total used free shared buff/cache available Mem: 1016108 632132 205776 66344 178200 180496 Swap: 0 0 0发现swap 为零了 执行 dd if=/dev/zero of=/swapfile bs=1M count=1024 mkswap /swapfile wapon /swapfile swapon /swapfile再用free查看 [root@tokyo ~]# free total used free shared buff/cache available Mem: 1016108 732148 63984 51400 219976 71132 Swap: 1048572 209240 839332启动mysql, 解决]]></content>
      <categories>
        <category>DB</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 后台执行命令]]></title>
    <url>%2Fbe78f922%2F</url>
    <content type="text"><![CDATA[当我们在终端或控制台工作时，可能不希望由于运行一个作业而占住了屏幕，因为可能还有更重要的事情要做，比如阅读电子邮件。对于密集访问磁盘的进程，我们更希望它能够在每天的非负荷高峰时间段运行(例如凌晨)。为了使这些进程能够在后台运行，也就是说不在终端屏幕上运行，有几种选择方法可供使用 &amp; nohup ctrl + z ctrl + c jobs bg fg &amp;当在前台运行某个作业时，终端被该作业占据；可以在命令后面加上&amp; 实现后台运行。例如：sh test.sh &amp; 适合在后台运行的命令有f i n d、费时的排序及一些s h e l l脚本。在后台运行作业时要当心：需要用户交互的命令不要放在后台执行，因为这样你的机器就会在那里傻等。不过，作业在后台运行一样会将结果输出到屏幕上，干扰你的工作。如果放在后台运行的作业会产生大量的输出，最好使用下面的方法把它的输出重定向到某个文件中： command &gt; out.file 2&gt;&amp;1 &amp; 当你成功地提交进程以后，就会显示出一个进程号，可以用它来监控该进程，或杀死它。(ps -ef | grep 进程号 或者 kill -9 进程号） nohup使用&amp;命令后，作业被提交到后台运行，当前控制台没有被占用，但是一但把当前控制台关掉(退出帐户时)，作业就会停止运行。nohup命令可以在你退出帐户之后继续运行相应的进程。nohup就是不挂起的意思( no hang up)。该命令的一般形式为： nohup command &amp;如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件： nohup command &gt; myout.file 2&gt;&amp;1 &amp;使用了nohup之后，很多人就这样不管了，其实这样有可能在当前账户非正常退出或者结束的时候，命令还是自己结束了。所以在使用nohup命令后台运行命令之后，需要使用exit正常退出当前账户，这样才能保证命令一直在后台运行 ctrl + z可以将一个正在前台执行的命令放到后台，并且处于暂停状态 ctrl + c终止前台命令 jobs查看当前有多少在后台运行的命令。 jobs -l选项可显示所有任务的PID，jobs的状态可以是running, stopped, Terminated。但是如果任务被终止了（kill），shell 从当前的shell环境已知的列表中删除任务的进程标识 bg将一个在后台暂停的命令，变成继续执行 （在后台执行） 如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid) 将任务转移到后台运行： 先ctrl + z；再bg，这样进程就被移到后台运行，终端还能继续接受命令。 fg将后台中的命令调至前台继续运行 如果后台中有多个命令，可以用 fg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Go Cron 定时任务的用法]]></title>
    <url>%2F4dc3c807%2F</url>
    <content type="text"><![CDATA[原文: https://www.cnblogs.com/zuxingyu/p/6023919.html cron 是什么cron 表达式cron 特定字符说明cron 举例说明下载安装源码解析文件目录cron.go关键方法spec.go结构体及关键方法parser.go项目中的应用]]></content>
      <categories>
        <category>Golang</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Golang Select 的用法]]></title>
    <url>%2Fe353ee8e%2F</url>
    <content type="text"><![CDATA[基本使用select 是 Go 中的一个控制结构, 类似于switch 语句, 用于处理异步 IO 操作. select 语句会监听 case语句中channel 的读写操作, 当case 中 channel 读写操作为非阻塞状态(即能读写)时, 将会触发相应的动作. select 中的 case 语句必须是一个 channel 操作 select 中的 default 子句总是可运行的 如果有多个 case 都可以运行, select 会随机公平地选出一个执行, 其他不会执行 如果没有可运行的 case 语句, 且有 default 语句, 则会执行 default 的动作 如果没有可运行的 case 语句, 且没有 default 语句, select 将阻塞, 知道某个 case 通信可以运行 例 package main import &quot;fmt&quot; func main() { var c1, c2, c3 chan int var i1, i2 int select { case i1 = &lt;-c1: fmt.Printf(&quot;received &quot;, i1, &quot; from c1\n&quot;) case c2 &lt;- i2: fmt.Printf(&quot;sent &quot;, i2, &quot; to c2\n&quot;) case i3, ok := (&lt;-c3): // same as: i3, ok := &lt;-c3 if ok { fmt.Printf(&quot;received &quot;, i3, &quot; from c3\n&quot;) } else { fmt.Printf(&quot;c3 is closed\n&quot;) } default: fmt.Printf(&quot;no communication\n&quot;) } } //输出：no communication典型用法1. 超时判断//比如在下面的场景中，使用全局resChan来接受response，如果时间超过3S,resChan中还没有数据返回，则第二条case将执行 var resChan = make(chan int) // do request func test() { select { case data := &lt;-resChan: doData(data) case &lt;-time.After(time.Second * 3): fmt.Println(&quot;request time out&quot;) } } func doData(data int) { //... }2. 退出//主线程（协程）中如下： var shouldQuit=make(chan struct{}) fun main(){ { //loop } //...out of the loop select { case &lt;-c.shouldQuit: cleanUp() return default: } //... } //再另外一个协程中，如果运行遇到非法操作或不可处理的错误，就向shouldQuit发送数据通知程序停止运行 close(shouldQuit)3. 判断 channel 是否阻塞//在某些情况下是存在不希望channel缓存满了的需求的，可以用如下方法判断 ch := make (chan int, 5) //... data：=0 select { case ch &lt;- data: default: //做相应操作，比如丢弃data。视需求而定 }]]></content>
      <categories>
        <category>Golang</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis 笔记]]></title>
    <url>%2F9f188831%2F</url>
    <content type="text"><![CDATA[数据类型Redis 支持5中数据类型 字符串(string)Redis 中字符串是一个字节序列. Redis 中的字符串是二进制安全的, 这意味着它们的长度不由任何特殊的终止字符决定。因此，可以在一个字符串中存储高达512兆字节的任何内容 例 redis 127.0.0.1:6379&gt; SET name &quot;value&quot; OK redis 127.0.0.1:6379&gt; GET name &quot;value&quot; Redis命令不区分大小写.字符串的最大长度为512M散列/哈希(Hash)Redis散列/哈希(Hashes)是键值对的集合。Redis散列/哈希是字符串字段和字符串值之间的映射。因此，它们用于表示对象。 例 redis 127.0.0.1:6379&gt; HMSET ukey username &quot;yiibai&quot; password &quot;passswd123&quot; points 200散列/哈希数据类型用于存储包含用户的基本信息的用户对象。这里HMSET，HGETALL是Redis的命令，而ukey是键的名称。 每个散列/哈希可以存储多达2^32 - 1个健-值对(超过40亿个)。 列表(List)Redis列表只是字符串列表，按插入顺序排序。可以向Redis列表的头部或尾部添加元素。 例 redis 127.0.0.1:6379&gt; lpush alist redis (integer) 1 redis 127.0.0.1:6379&gt; lpush alist mongodb (integer) 2 redis 127.0.0.1:6379&gt; lpush alist sqlite (integer) 3 redis 127.0.0.1:6379&gt; lrange alist 0 10 1) &quot;sqlite&quot; 2) &quot;mongodb&quot; 3) &quot;redis&quot;集合(Set)Redis集合是字符串的无序集合。在Redis中，可以添加，删除和测试成员存在的时间O(1)复杂性 例 redis 127.0.0.1:6379&gt; sadd yiibailist redis (integer) 1 redis 127.0.0.1:6379&gt; sadd yiibailist mongodb (integer) 1 redis 127.0.0.1:6379&gt; sadd yiibailist sqlite (integer) 1 redis 127.0.0.1:6379&gt; sadd yiibailist sqlite (integer) 0 redis 127.0.0.1:6379&gt; smembers yiibailist 1) &quot;sqlite&quot; 2) &quot;mongodb&quot; 3) &quot;redis&quot; 注意 - 在上面的示例中，sqlite被添加了两次，但是由于集合的唯一属性，所以它只算添加一次可排序集合(ZSET)Redis可排序集合类似于Redis集合，是不重复的字符集合。 不同之处在于，排序集合的每个成员都与分数相关联，这个分数用于按最小分数到最大分数来排序的排序集合。虽然成员是唯一的，但分数值可以重复 例 redis 127.0.0.1:6379&gt; zadd yiibaiset 0 redis (integer) 1 redis 127.0.0.1:6379&gt; zadd yiibaiset 0 mongodb (integer) 1 redis 127.0.0.1:6379&gt; zadd yiibaiset 1 sqlite (integer) 1 redis 127.0.0.1:6379&gt; zadd yiibaiset 1 sqlite (integer) 0 redis 127.0.0.1:6379&gt; ZRANGEBYSCORE yiibaiset 0 1000 1) &quot;mongodb&quot; 2) &quot;redis&quot; 3) &quot;sqlite&quot; 因为 ‘sqlite‘ 的排序值是 1 ，其它两个元素的排序值是 0 ，所以 ‘sqlite‘ 排在最后一个位置上]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Go 环境变量]]></title>
    <url>%2F2ddd6919%2F</url>
    <content type="text"><![CDATA[GOROOT，在Linux系统中一般安装在/usr/go或者/usr/local/go，这样Linux系统中的PATH变量一般都包含了这两个目录，所以就可以直接运行go命令，而Windows系统中一般默认安装在C:\go中 自定义 GO安装路径, 可修改环境变量配置文件 export GOROOT=$HOME/go GOPATHgo的工作目录，这个目录指定了需要从哪个地方寻找GO的包、可执行程序等，这个目录可以是多个目录表示，go编译或者运行时会从这个环境变量中去对应查找，工作目录或者如官方文档中说的workspace 在这个目录进行编译、链接最后生成所需要的库、可执行文件，我们对比C程序的目录，也许更能方便理解，一般在C的工程项目中包含三个文件，一个include目录、src目录、Makefile文件。 include目录存放了所有的头文件可供其他地方包含 src目录则存放所有的.c后缀的源文件 Makefile则是该项目的编译，在编译整个工程时需要执行make命令，这里就发现GO就不需要去写什么Makefile了，执行go build xxx.go命令就可以编译 GOPATH 下的目录下, 一般有三个 目录 bin pkg src bin目录包含了可执行程序，注意是可执行的，不需要解释执行。 pkg目录包含了使用的包或者说库。 src里面包含了go的代码源文件，其中仍按包的不同进行组织 包名一般和目录名相同, 编译时, 可以在某个包下, 执行go build , 也可以在包上层直接编译包名go build pkg_name go install &lt;pkg_name/exe_name/all&gt; 先编译后把编译生成的可执行文件复制到bin 下]]></content>
      <categories>
        <category>Golang</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 安装Bashmarks]]></title>
    <url>%2F472d58f%2F</url>
    <content type="text"><![CDATA[下载源码git clone https://github.com/lxbwolf/bashmarks.git 把bashmarks.sh复制到~/bin/ 添加环境变量在环境变量文件里, 添加 . ~/bin/bashmarks.sh 相关命令s &lt;bookmark_name&gt; - Saves the current directory as &quot;bookmark_name&quot; g &lt;bookmark_name&gt; - Goes (cd) to the directory associated with &quot;bookmark_name&quot; p &lt;bookmark_name&gt; - Prints the directory associated with &quot;bookmark_name&quot; d &lt;bookmark_name&gt; - Deletes the bookmark l - Lists all available bookmarks]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 安装Thefuck]]></title>
    <url>%2F42c1114f%2F</url>
    <content type="text"><![CDATA[下载源码git clone https://github.com/lxbwolf/thefuck.git 配置环境变量 把thefuck/**/libexec/bin 添加进环境变量 eval $(thefuck --alias fuck)]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 安装Samba]]></title>
    <url>%2F8f5b70d0%2F</url>
    <content type="text"><![CDATA[开发机安装 sambayum install samba samba-client samba-swat 添加账号sampasswd -a 用户名 用户名只能为已经存在的账号 配置共享文件夹编辑etc/samba/smb.conf, 追加内容: [samba_share_dir] comment = samba_share path = /home/lxb/samba_share create mask = 0664 directory mask = 0775 writable = yes valid users = lxb browseable = yes配置环境变量环境变量文件添加: export LD_LIBRARY_PATH=/usr/local/samba/lib:$LD_LIBRARY_PATH samba 重启sudo /etc/init.d/smb restart MAC客户端连接Finder -&gt; 前往 -&gt; 连接服务器 -&gt; 输入smb地址 smb://user_name@IP/samba_share_dir]]></content>
      <categories>
        <category>samba</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 配置中的Rewrite]]></title>
    <url>%2F389c639%2F</url>
    <content type="text"><![CDATA[语法 rewrite regex replacement flag flag有如下: - last - break 中止 rewrite, 不再继续匹配 - redirect 返回临时重定向的 HTTP 状态302 - permanet 返回永久重定向的 HTTP 状态301 last 和 break 的不同:break 是终止当前location 的 rewrite 检测, 且不再进行 location 匹配;last是终止当前location的rewrite检测,但会继续重试location匹配并处理区块中的rewrite规则 下面是可以用来判断的表达式: -f 和!-f 判断是否存在文件 -d 和!-d 判断是否存在目录 -e 和!-e 判断是否存在文件或目录 -x 和!-x 判断文件是否可执行 下面是可以用作判断的全局变量 $args 等于请求行中的参数 $content_length 请求头中的Content-length 字段 $content_type 请求头中的Content-Type 字段 $document_root 当前请求在root 指令中指定的值 $host 请求主机头字段, 否则为服务器名称 $http_user_agent 客户端agent 信息 $http_cookie 客户端cookie 信息 $limit_rate 这个变量可以限制连接速率 $request_body_file 客户端请求主题信息的临时文件名 $request_method #客户端请求的动作，通常为GET或POST。 $remote_addr #客户端的IP地址。 $remote_port #客户端的端口。 $remote_user #已经经过Auth Basic Module验证的用户名。 $request_filename #当前请求的文件路径，由root或alias指令与URI请求生成。 $query_string #与$args相同。 $scheme #HTTP方法（如http，https）。 $server_protocol #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 $server_addr #服务器地址，在完成一次系统调用后可以确定这个值。 $server_name #服务器名称。 $server_port #请求到达服务器的端口号。 $request_uri #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。 $uri 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。 $document_uri #与$uri相同 例: http://localhost:88/test1/test2/test.php $host: localhost $server_post: 88 $request_uri: http://localhost:88/test1/test2/test.php $document_uri: /test1/test2/test.php $document_root: /usr/share/nginx/html (在nginx.conf里配置的) $request_filename: /usr/share/nginx/html/test1/test2/test.php (在nginx.conf里配置的)详例:多目录转成参数abc.domain.com/sort/2 =&gt; abc.domain.com/index.php?act=sort&amp;name=abc&amp;id=2 if ($host ~* (.*)\.domain\.com) { set $sub_name $1; rewrite ^/sort\/(\d+)\/?$ /index.php?act=sort&amp;cid=$sub_name&amp;id=$1 last; }目录对换/123456/xxxx =&gt; /xxxx?id=123456 rewrite ^/(\d+)\/(.+)/ /$2?id=$1 last; // rewrite ^/\/(\d+)\/(\w+)\/? /$2?id=$1 last;如果使用IE浏览器, 则重定向到/nginx-ie 目录下 if ($http_user_agent ~ MSIE) { rewrite ^(.*)$ /nginx-ie/$1 break; }*目录自动加 / * 1. if (-d $request_filename){ 2. rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent; 3. }禁止htaccess 1. location ~/\.ht { 2. deny all; 3. }禁止多个目录 1. location ~ ^/(cron|templates)/ { 2. deny all; 3. break; 4. }禁止以/data开头的文件 可以禁止/data/下多级目录下.log.txt等请求; 1. location ~ ^/data { 2. deny all; 3. }禁止单个目录 不能禁止.log.txt能请求 1. location /searchword/cron/ { 2. deny all; 3. }禁止单个文件 1. location ~ /data/sql/data.sql { 2. deny all; 3. }给favicon.ico和robots.txt设置过期时间;这里为favicon.ico为99 天,robots.txt为7天并不记录404错误日志 1. location ~(favicon.ico) { 2. log_not_found off; 3. expires 99d; 4. break; 5. } 6. 7. location ~(robots.txt) { 8. log_not_found off; 9. expires 7d; 10. break; 11. }设定某个文件的过期时间;这里为600秒，并不记录访问日志 1. location ^~ /html/scripts/loadhead_1.js { 2. access_log off; 3. root /opt/lampp/htdocs/web; 4. expires 600; 5. break; 6. }文件反盗链并设置过期时间这里的return 412 为自定义的http状态码，默认为403，方便找出正确的盗链的请求“rewrite ^/ http://leech.c1gstudio.com/leech.gif;”显示一张防盗链图片“access_log off;”不记录访问日志，减轻压力“expires 3d”所有文件3天的浏览器缓存 1. location ~* ^.+\.(jpg|jpeg|gif|png|swf|rar|zip|css|js)$ { 2. valid_referers none blocked *.c1gstudio.com *.c1gstudio.net localhost 208.97.167.194; 3. if ($invalid_referer) { 4. rewrite ^/ http://leech.c1gstudio.com/leech.gif; 5. return 412; 6. break; 7. } 8. access_log off; 9. root /opt/lampp/htdocs/web; 10. expires 3d; 11. break; 12. }只充许固定ip访问网站，并加上密码 1. root /opt/htdocs/www; 2. allow 208.97.167.194; 3. allow 222.33.1.2; 4. allow 231.152.49.4; 5. deny all; 6. auth_basic &quot;C1G_ADMIN&quot;; 7. auth_basic_user_file htpasswd;将多级目录下的文件转成一个文件，增强seo效果/job-123-456-789.html 指向 /job/123/456/789.html 1. rewrite ^/job-([0-9]+)-([0-9]+)-([0-9]+)\.html$ /job/$1/$2/jobshow_$3.html last;将根目录下某个文件夹指向2级目录如/shanghaijob/ 指向 /area/shanghai/ 如果你将last改成permanent，那么浏览器地址栏显是 /location/shanghai/ 1. rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;上面例子有个问题是访问/shanghai 时将不会匹配 1. rewrite ^/([0-9a-z]+)job$ /area/$1/ last; 2. rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;这样/shanghai 也可以访问了，但页面中的相对链接无法使用， 如./list_1.html真实地址是/area /shanghia/list_1.html会变成/list_1.html,导至无法访问。那我加上自动跳转也是不行咯 (-d $request_filename)它有个条件是必需为真实目录，而我的rewrite不是的，所以没有效果 1. if (-d $request_filename){ 2. rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent; 3. }知道原因后就好办了，让我手动跳转吧 1. rewrite ^/([0-9a-z]+)job$ /$1job/ permanent; 2. rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;文件和目录不存在的时候重定向： 1. if (!-e $request_filename) { 2. proxy_pass http://127.0.0.1; 3. }域名跳转 1. server 2. { 3. listen 80; 4. server_name jump.c1gstudio.com; 5. index index.html index.htm index.php; 6. root /opt/lampp/htdocs/www; 7. rewrite ^/ http://www.c1gstudio.com/; 8. access_log off; 9. }多域名转向 1. server_name www.c1gstudio.com www.c1gstudio.net; 2. index index.html index.htm index.php; 3. root /opt/lampp/htdocs; 4. if ($host ~ &quot;c1gstudio\.net&quot;) { 5. rewrite ^(.*) http://www.c1gstudio.com$1 permanent; 6. }三级域名跳转 1. if ($http_host ~* &quot;^(.*)\.i\.c1gstudio\.com$&quot;) { 2. rewrite ^(.*) http://top.yingjiesheng.com$1; 3. break; 4. }域名镜向 1. server 2. { 3. listen 80; 4. server_name mirror.c1gstudio.com; 5. index index.html index.htm index.php; 6. root /opt/lampp/htdocs/www; 7. rewrite ^/(.*) http://www.c1gstudio.com/$1 last; 8. access_log off; 9. }]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置中的location]]></title>
    <url>%2Fa209000e%2F</url>
    <content type="text"><![CDATA[语法location [=|~|~*|^~] /uri/ {...} 上下文: server 此命令随URL 不同而接受不同的结构. 可以配置使用常规字符串和正则表达式. 若使用正则表达式, 则必须使用~*前缀(选择不区分大小写的匹配) 或~前缀(区分大小写的匹配) = 表示uri 以某个常规字符串开头, 理解为匹配url 路径即可. nginx 不对url 做编码, 因此请求为/static/%20%/aa 可以被规则^~ /static/ /aa (有空格) 匹配到. ~ 表示区分大小写的正则匹配 ~* 表示不区分大小写的正则匹配 !~ 和 !~* 分别为区分大小写不匹配 和 不区分大小写不匹配 的正则 / 通用匹配, 任何请求都会匹配到 多个location 配置的情况下, 匹配顺序为: 先匹配=, 其次匹配^~, 再匹配按文件中顺序的正则匹配, 最后匹配/. 当有匹配成功的时候, 停止匹配, 按当前匹配规则处理请求. 例1: location = / { # 规则A } location = /login { # 规则B } location ^~ /static { # 规则C } location ~ \.(gif|jpg|png|js|css)$ { # 规则D } location ~* \.png$ { # 规则E } location !~ \.xhtml$ { # 规则F } location !~* \.xhtml$ { # 规则G } location / { # 规则H }产生效果如下: 1. 访问/ 根目录, 如http://localhost/ 将匹配规则A 2. 访问http://localhost/login 将匹配规则B; http://localhost/register 将匹配规则H 3. 访问http://localhost/static/a.html 将匹配规则C 4. 访问http://localhost/a.png 讲匹配规则D 和规则E, 但规则D 顺序优先, 规则E 不起作用 5. 访问http://localhost/static/c.png 优先匹配到规则C 6. 访问http://localhost/a.PNG 将匹配规则E 7. 访问http://localhost/a.xhtml 不会匹配到规则F 和规则G, http://localhost/a.XHTML 不会匹配到规则G 8. 访问http://localhost/category/id/1111 匹配到规则H, 因为以上规则都不匹配, 这个时候应该是nginx 转发给后端应用服务器, 如FastCGI(php), tomcat(jsp), nginx 作为反向代理服务器存在. 所以实际使用中, 通常有至少三个匹配规则定义, 如下: # 第一个必选规则 直接匹配网站根, 通过域名访问网站首页比较频繁, 使用这个会加速处理; 这里直接转发给后端应用服务器了, 也可以是一个静态首页 location = / { proxy_pass http://tomcat:8080/index } # 第二个必选规则 处理静态文件请求, 这是nginx 作为http 服务器的强项. 有如下两种配置模式, 目录匹配或后缀匹配, 任选其一或搭配使用 location ^~ /static/ { root /webroot/static/; } location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ { root /webroot/res/; } # 第三个必选规则 通用规则, 用来转发动态请求到后端应用服务器. 非静态文件请求就默认是动态请求. location / { proxy_pass http://tomcat:8080/ }]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 寻找Index 原理]]></title>
    <url>%2Fab14d5fa%2F</url>
    <content type="text"><![CDATA[1. nginx 是怎么找index.php 文件的当nginx发现需要/web/echo/index.php 文件时, 就会向内核发起 IO 系统调用(因为要跟硬件打交道, 这里的硬件是指硬盘, 通常需要靠内核来操作, 而内核提供的这些功能是通过系统调用来实现的), 告诉内核, 我需要这个文件, 内核从/ 开始找到web 目录, 再在web 目录下找到echo 目录, 最后在echo 目录下找到index.php 文件, 于是把这个index.php 从硬盘上读取到内核自身的内存空间, 然后再把这个文件复制到nginx进程所在的内存空间, 于是 nginx就得到了自己想要的文件了 2. 寻找文件在文件系统层面是怎么操作的如, nginx 需要得到/web/echo/index.php 这个文件 每个分区(像ext3 等文件系统, block块是文件存储的最小单元, 默认是4096字节) 都是包含元数据区和数据区, 每个文件在元数据区都有元数据条目(一般是128字节大小), 每个条目都有一个编号, 称之为 inode(index node), 这个inode 里包含 文件类型, 权限, 连接次数, 属主和数组的 ID&amp;时间戳, 这个文件占据了哪些磁盘块也就是块的编号(block, 每个文件可以占用多个 block, 且 block 不一定是连续的, 每个 block 都有编号), 如下图: 目录其实也是普通文件, 也需要占用磁盘块, 目录不是一个容器. 默认创建的目录大小为4096字节, 即只需要占用一个磁盘块, 但这是不确定的. 所以要找到目录也是需要到元数据区里找到对应的条目, 只要找到对应的inode就可以找到目录所占用的磁盘块. 目录里存着一张表(映射表), 里面放着 目录或文件的名称和对应的inode号, 如下: - - 文件名称(只是字符串) inode 号 test.txt 100 假如 / 在数据区占据1, 2号 block, `/` 其实也是一个目录, 里面有两个目录, web 和 111 web 占据5号 block, 是目录, 里面有2个目录 echo 和 data echo 占据11号 block, 是目录, 里面有一个文件 index.php index.php 占据15, 16号 block, 是文件其在文件系统中分布如下图: 那么内核究竟是怎么找到index.php 这个文件的呢? 内核拿到 nginx 的 IO 系统调用要获取/web/echo/index.php 这个文件请求之后, 1. 内核读取元数据区 / 的inode, 从 inode 里读取 / 所对应的数据块的编号, 然后在数据区找到其对应的块(1, 2号块), 读取1号块上的映射表找到 web 这个名称在元数据区对应的 inode 号 2. 内核读取 web 对应的 inode(3号), 从中得到 web 在数据区对应的块是5号块, 于是到数据区找到5号块, 从中读取映射表, 知道 echo 对应的 inode 是5号, 于是到元数据区找到5号 inode 3. 内核读取5号 inode, 得到 echo 在数据区对应的事11号块, 于是到数据区读取11号块得到映射表, 得到index.php 对应的 inode 事9号 4. 内核到元数据区读取9号 inode, 得到 index.php 对应的事15号和16号数据块, 于是就到数据区域找到15 16号块, 读取其中的内容, 得到 index.php 的完整内容]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议笔记]]></title>
    <url>%2F4e7c79%2F</url>
    <content type="text"><![CDATA[HTTP协议的特点: 1. 支持客户端/服务器模式 2. 简单快速: 客户端向服务器请求服务时, 只需传送请求方法和路径. 请求方法常用的有GET, HEAD, POST. 每种方法规定了客户端与服务器联系的类型. 由于HTTP协议简单, 使得HTTP服务器的程序规模小, 因而通信速度很快. 3. 灵活: HTTP 允许传输任意类型的数据对象. 正在传输的类型由Content-Type加以标记. 4. 无连接: 限制每次连接只处理一个请求. 服务器处理完客户端的请求, 并收到客户端的应答后, 即断开连接. 采用着用方式可以节省传输时间. 5. 无状态: 无状态是指协议对于事务处理没有记忆能力. 缺少状态意味着如果后续处理需要前面的信息, 则它必须重传, 这样可能导致每次传送的数据量增大. 另一方面, 在服务器不需要先前信息时, 它的应答就较快. URLHTTP是一种基于请求与响应模式的, 无状态的, 应用层的协议, 常基于TCP的连接方式, HTTP1.1版本中给出一种持续连接的机制, 绝大多数的web开发, 都是构建在HTTP协议之上的web应用. HTTP URL(URL是一种特殊类型的URI, 包含了用于查找某个资源的足够的信息)的格式如下: http://host[:port][abs_path] http 表示要通过HTTP协议来定位网络资源 host 表示合法的Internet主机域名或者IP地址 port 指定端口号, 缺省端口为80 abs_path 指定请求资源的URI 若URI 中没有给出abs_path, 那当它作为请求URI时, 必须以”/“ 的形式给出, 通常这个工作浏览器会自动完成 请求HTTP请求由三部分组成: 请求行, 消息报头, 请求正文 请求行以一个方法符号开头, 以空格分开, 后面跟请求的URI和协议的版本, 格式如下: Method Request-URI HTTP-Version CRLF 请求方法有以下几种 - GET 请求获取Request-URI 所标识的资源 - POST 在Request-URI 表标识的资源后附加新的数据 - HEAD 请求获取由Request-URI 所标识的资源的响应消息报头 - PUT 请求服务器存储一个资源, 并用Request-URI 作为其标识 - DELETE 请求服务器删除Request-URI 所标识的资源 - TRACE 请求服务器会送收到的请求信息, 主要用于测试或诊断 - CONNECT 保留将来使用 - OPTIONS 请求查询服务器的性能, 或查询与资源相关的选项和需求 响应HTTP响应由三部分组成: 状态行, 消息报头, 响应正文 状态行格式: HTTP-Version Status-Code Reason-Phrase CRLF 状态码由三位数字组成, 第一个数字定义了响应的类别: - 1xx: 指示信息 – 表示请求已接受, 继续处理 - 2xx: 成功 – 表示请求已被成功接收, 理解, 接受 - 3xx: 重定向 – 要完成请求必须进行更进一步的操作 - 4xx: 客户端错误 – 请求有语法错误或请求无法实现 - 5xx: 服务器端错误 – 服务器未能实现合法的请求 报头HTTP 消息由客户端到服务器的请求和服务器到客户端的响应组成. 请求消息和相应消息都是由开始行(对于请求消息, 开始行就是请求行, 对于响应消息, 开始行就是状态行), 消息报头(可选), 空行(只有CRLF的行), 消息正文(可选) 组成 HTTP 消息报头包括 普通报头, 请求报头, 响应报头, 实体报头 每一个报头域都是由名字 + &quot;:&quot; + 空格 + 值 组成, 消息报头域的名字是大小写无关的. 普通报头在普通报头中, 有少数报头域用于所有的请求和响应消息, 但并用于被传输的实体, 只用于传输的消息 例: Cache-Control 用于指定缓存指令, 缓存指令是单向的(响应中出现的缓存指令在请求中未必会出现), 且是独立的(一个消息的缓存指令不会影响另一个消息处理的缓存机制), HTTP1.0使用类似的报头域为Pragma. 请求时的缓存指令包括: no-cache(用于指示请求或相应消息不能缓存), no-store, max-age, max-stale, min-fresh, only-if-cached; 响应时的缓存指令包括: public, private, no-cache, no-store, no-transform, must-revalidate, proxy-revalidate, max-age, s-maxage eg: 为了指示IE浏览器(客户端)不要缓存页面, 服务器端的JSP程序可以编写如下: response.setHeader(&quot;Cache-Control&quot;, &quot;no-cache&quot;); 或 response.setHeader(&quot;Pragma&quot;, &quot;no-cache&quot;); 两者作用相同, 在发送的响应消息中设置普通报头域: Cache-Control: no-cache. 请求报头请求报头允许客户端向服务器端传递请求的附加信息以及客户端自身的信息. 常用的请求报头域: Accept Accept 请求报头域用于指定客户端接收哪些类型的信息. 如: Accept:image/gif, 表明客户端希望接收GIF图像格式的资源; Accept:text/html, 表明客户端希望接受html 文本 Accept-Charset 指定客户端接受的字符集. 如: Accept-Charset:iso-8859-1,GB2312. 如果请求消息中没有设置这个域, 缺省是任何字符集都可以接受. Accept-Encoding 指定可接受的内容编码. 如Accept-Encoding:gzip.deflate 缺省是任何内容编码都可以接受 Accept-Language 指定一种自然语言. 如 Accept-Language:zh-cn 缺省是任何语言都可以接受 Authorization 用于证明客户端有权查看某个资源. 当浏览器访问一个页面时, 如果收到服务器的响应代码为401, 可以发送一个包含Authorization请求报头域的请求, 要求服务器对其进行验证 Host(发送请求时, 该报头域是必需的) 指定被请求资源的Internet主机和端口号, 通常从HTTP URL中提取出来 User-Agent 允许客户端将它的操作系统, 浏览器和其他属性告诉服务器. 不过这个报头域不是必需的, 如果我们自己写一个浏览器, 不使用User-Agent 请求报头域, 那么服务器端就无法得知我们的信息了. 请求报头示例 GET /form.html HTTP/1.1 (CRLF) Accept:image/gif,image/x-xbigmap,image/jpeg,application/x-shockwave-flash,application/vnd.ms-excel,application/vnd.ms-powerpoint,application/msword,*/* (CRLF) Accept-Language:zh-cn (CRLF) Acdept-Encoding:gzip,deflate (CRLF) If-Modified-Since:Wed,05 Jan 2007 11:21:25 GMT (CRLF) If-None-Match:Mozilla/4.0(compatible;MSIE6.0,Windows NT 5.0) (CRLF) Host:www.guet.edu.cn (CRLF) Connection:Keep-Alive (CRLF) (CRLF)响应报头响应报头允许服务器传递不能放在状态行中的附加响应信息, 以及关于服务器的信息和对Request-URI所标识的资源进行下一步访问的信息 Location 重定向接受者到一个新的位置. Location响应报头域常用在更换域名的时候 Server 包含了服务器用来处理请求的软件信息. 与User-Agent 请求报头域是相对应的 例: Server:Apache-Coyote/1.1 WWW-Authenticate WWW-Authenticate 响应报头域必须包含在401响应消息中, 客户端收到401响应消息的时候, 并发送Authorization报头域请求服务器对其进行验证时, 服务端响应报头就包含该报头域 例: WWW-Authenticate:Basic realm=&quot;Basic Auth Test! 实体报头请求和响应消息都可以传送一个实体. 一个实体由实体报头域和实体正文组成, 但并不是实体报头域和实体正文要在一起发送, 可以只发送实体报头域. 实体报头定义了关于实体正文(如: 有无实体正文) 和请求所标识的资源的元信息. Content-Encoding 被用作媒体类型的修饰符, 它的值指示了已经被应用到实体正文的附加内容的编码, 因而要获得Content-Type 报头域中所引用的媒体类型, 必须采用相应的解码机制. Content-Language 描述了资源所用的自然语言. 没有设置该域则认为实体内容将提供给所有的语言阅读 Content-Length 指明实体正文的长度, 以字节方式存储的十进制数字来表示 Content-Type 指明发送给接受者的实体正文的媒体类型 例: Content-Type:text/html;charset=ISO-8859-1 Content-Type:text/html;charset=GB2312 Last-Modified 指示资源的最后修改日期和时间 Expires 给出响应国企的日期和时间. 为了让代理服务器或浏览器在一段时间以后更新缓存中(再次访问曾访问过的页面时, 直接从缓存中加载, 缩短响应时间和降低服务器负载) 的页面, 我们可以使用Expires 实体报头域指定页面过期的时间. 例: Expires:Thu, 15 Sep 2006 16:23:12 GMT HTTP1.1 的客户端和缓存必须将其他非法的日期格式(包括0) 看作已经过期 为了让浏览器不要缓存页面, 我们也可以利用Expires 实体报头域设置为0, jsp中程序如下: response.setDateHeader(&quot;Expires&quot;, &quot;0&quot;);]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Qt 同步方式发送Post 请求]]></title>
    <url>%2Fc57f4e3b%2F</url>
    <content type="text"><![CDATA[不成功的方式: 1. QNetworkReply的isFinished()函数, 通过while循环判断reply是否已经结束, 结束后再调用readAll()读取响应信息, 结果与判断isRunning() 方式结果一样, 都会进入死循环, 没有响应. 2. QNetworkReply继承自QIODevice, 尝试调用QIODevice的waitForReadyRead()方法, 结果不阻塞, 直接返回 成功的方式: 使用QEventLoop来阻塞运行, 知道信号发出 QNetworkReply *reply = _manager-&gt;post(QNetworkRequest(QUrl(SERVER_URL)), data); QByteArray responseData; QEventLoop eventLoop; connect(_manager, SIGNAL(finished(QNetworkReply*)), &amp;eventLoop, SLOT(quit())); eventLoop.exec(); //block until finish responseData = reply-&gt;readAll();]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Qt 程序打包]]></title>
    <url>%2F7e8574d1%2F</url>
    <content type="text"><![CDATA[设置程序图标 把ico文件放到源文件目录下, 命名为”test.ico” 创建一个myico.rc 文件, 输入如下内容 IDI_ICON1 ICON DISCARDABLE &quot;test.ico&quot; 在pro文件写入 RC_FILE = myico.rc 执行qmake, 编译 编译, 打包 选择release编译运行 将生成的exe文件放到某个路径下, 如 Desktop/Test 在cmd里, 进入到exe存放路径, 使用wendeployqt工具拷贝exe运行需要的dll 使用Inno Setup Compiler生成安装文件 Inno Setup 工具使用注意事项 添加主执行文件外的其他应用程序文件夹下的文件时, 需要编辑一次, 重新指定目标子文件夹 编译脚本为*.iss 文件, 编译后默认在源exe的Base 目录下生成Output文件夹, 指定的setup.exe文件生成在Output 文件夹下 Inno Setup 工具基础版不支持中文. 如需显示中文, 需要找汉化版]]></content>
      <categories>
        <category>Qt</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gerrit + Apache 安装]]></title>
    <url>%2F2eb2e06d%2F</url>
    <content type="text"><![CDATA[使用gerrit自带的数据库h2, 验证方式为HTTP, SMTP 服务器未配置 git 安装可直接从yum 源安装 gerrit 安装先添加gerrit 用户. gerrit 从2.10开始, 换成了新版界面. 几乎国内所有的镜像都会下载失败, 需要翻墙下载. 下载完成后, 初始化命令为: java -jar gerrrit-war init -d /home/gerrit/repository 初始化启动时, “Authentication method” 设为”http” ,其他默认 “Listen on port [8080]“ 可用默认, 如端口被占用, 初始化后也可在配置文件修改 apache 安装直接从yum源安装 apache名字为httpd, 服务名也是httpd. 服务启动后, 默认以apache用户运行. 如需访问其他用户的文件, 如/home/gerrit/repository/htpasswd, 需要确保apache 用户有足够的权限 配置 apache修改 apache 的conf 文件, 一般路径为/etc/httpd/conf/httpd.conf windows 下的配置文件路径为 INSTALL_DIR/conf/httpd.conf 去掉下面几行的注释 LoadModule proxy_module modules/mod_proxy.so LoadModule proxy_connect_module modules/mod_proxy_connect.so LoadModule proxy_http_module modules/mod_proxy_http.so LoadModule proxy_ftp_module modules/mod_proxy_ftp.so LoadModule negotiation_module modules/mod_negotiation.so在最后追加下面配置 &lt;VirtualHost *:8080&gt; ServerName v3server ProxyRequests Off ProxyVia Off ProxyPreserveHost On &lt;Proxy *:8080&gt; Order deny,allow Allow from all &lt;/Proxy&gt; &lt;Location /login/&gt; AuthType Basic AuthName &quot;Gerrit Code Review&quot; Require valid-user AuthUserFile D:/git/htpasswd &lt;/Location&gt; ProxyPass / http://10.14.132.179:9080/ ProxyPassReverse / http://10.14.132.179:9080/ &lt;/VirtualHost&gt;如端口被占用, 修改conf文件的”Listen 8080” 字段, 换成其他的端口 查看某个端口是否被占用 : netstat -lnp | grep 8080 ProxyPass 和 proxyPassReverse 的端口需与gerrit的conf文件里端口一致 配置 gerrit修改GERRIT_DIR/etc/gerrit.config 文件 [gerrit] basePath = git canonicalWebUrl = http://10.14.132.179:9080/ [database] type = H2 database = db/ReviewDB [auth] type = HTTP logoutUrl = http://aa:aa@10.14.132.179:8080/ [sendemail] smtpServer = smtp.163.com smtpUser = useremail@163.com smtpPass = userpass from = useremail@163.com [container] user = admin javaHome = C:\\Program Files\\Java\\jdk1.6.0_27\\jre [sshd] listenAddress = *:29418 [httpd] listenUrl = http://10.14.132.179:9080/ [cache] directory = cache需要修改的内容: - canonicalWebUrl - auth/type 需要注意: canonicalWebUrl 和 listenAddress 不是8080. apache 的端口和 gerrit 的端口是不同的, 用户访问地址为 apache 的地址 启动 gerritGERRIT_DIR/bin/gerrit.sh start 启动 apacheservice httpd start 添加账号和密码htpasswd -cm /home/gerrit/repository/htpasswd USER_NAME htpasswd 为apache 的命令工具 参数c 意为新建文件, 即 /home/gerrit/repository/htpasswd 文件不存在时, 新建名为htpasswd的文件 参数m 为使用md5 加密 当htpasswd文件存在时, 可以使用htpasswd -m /PATH_TO_HTPASSWD USER_NAME 添加账号 保存账号密码信息的文件(htpasswd), 名字为自定义的, 但需要与apache 的conf 配置文件里 AuthUserFile 一致]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Qt 工程的几种文件]]></title>
    <url>%2F924904f6%2F</url>
    <content type="text"><![CDATA[*.proqmake的工程(project)文件 例子: TEMPLATE = app CONFIG += QT QT += core gui TARGET = somename SOURCES += main.cpp \ widget.cpp HEADERS += widget.h FORMS += widget.ui 前三行是qmake的默认值, 都可以省略 TARGET 行指定工程名, 也可以省略 *.priinclude 文件 接上面的例子, 我们可以将源文件的设置独立处理, 放到somename.pri文件内: SOURCES += main.cpp \ widget.cpp HEADERS += widget.h FORMS += widget.ui这时, pro 文件就可以简化为: TEMPLATE = app CONFIG += QT QT += core gui TARGET = somename include(somename.pri)*.prf特性(feature) 文件 和pri文件类似, prf文件也是要被包含进pro文件. 只是它更隐蔽. 在上面的例子中, 其实已经用到了prf, 就是 CONFIG += QT 当在CONFIG 中指定一个值时, qmake就会尝试去加载相应的feature文件: - Qt安装目录下的mkspecs/features/qt.prf - features 文件的文件名必须小写 例子: win32:CONFIG += console // 为win32程序添加控制台把该文件命名为a.prf, 放到前面提到的目录中, 然后在pro文件内添加 CONFIG += a 也可以使用load命令来加载prf文件 load(a)]]></content>
      <categories>
        <category>Qt</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Qt UI 编码规范]]></title>
    <url>%2F9c64b54d%2F</url>
    <content type="text"><![CDATA[变量声明 每行只声明一个变量 避免使用短的/无意义的命名 当一个变量被用到时再声明 // Wrong int a, b; char* c, * d; // Correct int height; int width; char* nameOfOne; char* nameOfOther; 变量命名 变量名/函数名采用驼峰命名法(lowerCaseCamel), 首字母缩写词出现的命名中, 缩写也用驼峰命名 // Wrong short Cntr; char ITEM_DELIM = &apos;&apos;; void myXMLStreamReader(); // Correct short counter; char itemDelimiter = &apos;&apos;; void myXmlStreamReader(); 空行/空格 用一个且仅用一个空行在适当的地方划分代码块 在关键词和小括号之间总是只用一个空格符 // Wrong if(foo) { } // Correct if (foo) { } 指针/引用 在类型名和*或&amp;之间没有空格, 在*或&amp;与变量名之间有一个空格 char* someValue; const QString&amp; myString; const char* const WOR = &quot;hello&quot;; 符号与空格 二元操作符左右两边都有一个空格 一元操作符与变量之间不留空格 逗号左右没有空格, 右边一个空格 分号左边没有空格; 分号作为语句的结束符, 右边一般不再有内容 #号右边没有空格 左引号的左边和右引号的各一个空格, 左引号的右边和右引号的左边没有空格 如果右引号右边是右括号, 它们之间没有空格 cast 避免C语言的cast, 尽量用C++的cast(static_cast, const_cast, reinterpret_cast). reinterpret_cast 和 C风格的cast用起来都是危险的，但至少 reinterpret_cast 不会把const修饰符去掉 涉及到QObjects或重构自己的代码时，不要使用dynamic_cast,而是用qobject_cast，例如在引进一个类型的方法时 用构造函数去cast简单类型,例如：用int(myFloat)代替(int)myFloat // Wrong char* blockOfMemory = (char* ) malloc(data.size()); // Correct char *blockOfMemory = reinterpret_cast&lt;char *&gt;(malloc(data.size())); 语句 不要在一行写多条语句 括号 每个大括号单独一行 不论条件语句的执行部分有几行, 必须使用大括号 小括号用来给语句分组 // Wrong if (address.isEmpty()) { return false; } for (int i = 0; i &lt; 10; +’’i) { qDebug(&quot;%i&quot;, i); } // Correct if (address.isEmpty()) { return false; } else { return true; } for (int i = 0; i &lt; 10;i) { qDebug(&quot;%i&quot;, i); } // Wrong if (a &amp;&amp; b || c) // Correct if ((a &amp;&amp; b) || c) // Wrong a + b &amp; c // Correct (a + b) &amp; cswitch语句 case缩进 除enum外, 每组case最后都要加default; switch (myEnum) { case Value1: doSomething(); break; case Value2: case Value3: doSomethingElse(); // fall through break; default: defaultHandling(); break; } goto 禁止使用goto 换行 每行代码不多于120字符 逗号在行尾. 操作符在新行的开头位置 换行时尽量避免行与行之间看起来参差不齐 // Wrong if (longExpression + otherLongExpression + otherOtherLongExpression) { } // Correct if (longExpression + otherLongExpression + otherOtherLongExpression) { } C++特性 不要使用异常处理 不要使用运行时类型识别 理智地使用模板 Qt源码中的规范 所有代码都是ascii，使用者如果不确定的话，只可能是7字节 每一个QObject的子类都必须有Q_OBJECT宏，即使这个类没用到信号或槽。否则qobject_cast将不能使用 在connect语句中，使信号和槽的参数规范化（参看 QMetaObject::normalizedSignature），可以加快信号/槽的查找速度。可以使用qtrepotools/util/normalize规范已有代码 包含头文件顺序 源文件对应的头文件 &lt;分隔&gt; C系统文件 &lt;分隔&gt; C++系统文件 &lt;分隔&gt; Qt库文件 &lt;分隔&gt; 其他目录 每组文件按字母升序排列 编译器/平台 使用三目运算符 ？时要特别小心，如果每次的返回值的类型可能不一样的话，一些编译器会在运行时生成冲突的代码（此时编译器甚至不会报错） QString s; return condition ? s : &quot;nothing&quot;; // crash at runtime - QString vs. const char * 要特别小心对齐问题。无论何时，当一个指针被cast后的对齐数是增加的时候，它都可能会崩溃。例如一个const char 被cast成了cons int，当cast之后的数字不得不在2或4个字节之间对齐时，指针就会在机器上崩溃 任何需要需要执行构造函数或相关代码进行初始化的实例，都不能用作库代码中的全局实例。因为当构造函数或代码将要运行的时候，该实例还没有被定义（在第一次调用该实例时，在加载库时，在执行main()之前） // global scope static const QString x; // Wrong - default constructor needs to be run to initialize x static const QString y = &quot;Hello&quot;; // Wrong - constructor that takes a const char * has to be run QString z; // super wrong static const int i = foo(); // wrong - call time of foo() undefined, might not be called at all 可以使用下面方法: // global scope static const char x[] = &quot;someText&quot;; // Works - no constructor must be run, x set at compile time static int y = 7; // Works - y will be set at compile time static MyStruct s = {1, 2, 3}; // Works - will be initialized statically, no code being run static QString *ptr = 0; // Pointers to objects are ok - no code needed to be run to initialize ptr 用Q_GLOBAL_STATIC定义全局实例 Q_GLOBAL_STATIC(QString, s) void foo() { s()-&gt;append(&quot;moo&quot;); } char型变量是有符号的还是无符号的取决于它运行环境的架构。如果你明确地想使用一个signed或unsinged char，就使用signed char或unsigned char。以下代码运行在把char默认为无符号的平台上时，其条件判断恒为真 char c; // c can&#39;t be negative if it is unsigned /********/ /*******/ if (c &gt; 0) { … } // WRONG - condition is always true on platforms where the default is unsigned 避免64位的枚举值 嵌入式应用系统二进制接口将所有的枚举类型的值硬编码成32位int值 微软的编译器不支持64位的枚举值 编程偏好 用枚举值定义常量而非用const int或defines 枚举值会在编译时被编译器用实际值替换掉，因而运行时得出结果的速度更快 defines不是命名空间安全的（并且看起来很丑） 当重新实现一个虚方法时，在Qt5中，用 Q_DECL_OVERRIDE宏在函数声明之后，分号之前注解它 不要把const-iterator和none-const iterator搞混 for (Container::const_iterator it = c.begin(); it != c.end(); ++it) // Wrong for (Container::const_iterator it = c.cbegin(); it != c.cend(); ++it) // Right 命名空间 除跟UI直接交互的类外, 其他类必须处在命名空间内 float值 用qFuzzyCompare去和delta比较其值 用qIsNull去判断float值是不是二进制0，而不是和0.0比较 [static] bool qFuzzyCompare(double p1, double p2) // Instead of comparing with 0.0 qFuzzyCompare(0.0,1.0e-200); // This will return false // Compare adding 1 to both values will fix the problem qFuzzyCompare(1 + 0.0, 1 + 1.0e-200); // This will return true 类的成员命名 成员变量一般为名词 函数成员一般为动词/动词+名词，但是当动词为get时，get常常省略。当返回值为Bool型变量时，函数名一般以前缀’is’开头 public: void setColor(const QColor&amp; c); QColor color() const; void setDirty(bool b); bool isDirty() const; private Q_SLOTS: void onParentChanged(); 构造函数 为了使构造函数被错误使用的可能性降到最小，每一个构造函数（除了拷贝构函数）都应该检查自己是否需要加上explicit 符号 注意代码陷阱 不要为了图方便少些一些代码。因为代码是一次书写，后期不止一次地要去理解。例如 QSlider *slider = new QSlider(12, 18, 3, 13, Qt::Vertical, 0, &quot;volume&quot;); 改成下面的方式会更容易理解 QSlider *slider = new QSlider(Qt::Vertical); slider-&gt;setRange(12, 18); slider-&gt;setPageStep(3); slider-&gt;setValue(13); slider-&gt;setObjectName(&quot;volume&quot;); 参考资料 https://wiki.qt.io/Qt\_Contribution\_Guidelines https://wiki.qt.io/Qt\_Coding\_Style https://wiki.qt.io/Coding_Conventions https://community.kde.org/Policies/Library\_Code\_Policy https://wiki.qt.io/UI\_Text\_Conventions https://wiki.qt.io/API\_Design\_Principles http://doc.qt.io/qt-5/qml-codingconventions.html https://google.github.io/styleguide/cppguide.html]]></content>
      <categories>
        <category>Qt</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CentOS7升级gcc 和Gdb]]></title>
    <url>%2Fec9feff6%2F</url>
    <content type="text"><![CDATA[升级后版本: gcc-5.4.0 gdb-7.11.1 安装开发必备环境yum groupinstall &quot;Development Tools&quot; yum install glibc-static libstdc++-static编译安装gcc-5.4.0gcc下载地址 tar -xvf gcc-5.4.0.tar.bz2 cd gcc-5.4.0 ./contrib/download_prerequisits mkdir build cd build ../configure --enable-checking=release --enable-languages=c,c++ --disable-multilib make（建议不要使用make -j来编译，虽然可以缩短编译时间，但极大可能会编译失败） make install其中执行./contrib/download_prerequisits将自动下载以下几个文件，这个几个文件在gcc编译时需要： - mpfr-2.4.2.tar.bz2 - gmp-4.3.2.tar.bz2 - mpc-0.8.1.tar.gz - isl-0.15.tar.bz2 make install 时, 自动安装到/usr/local/gcc-5.40 解决运行程序时, gcc 报错’GLIBCXX_3.4.21’ not found这是因为升级gcc时，生成的动态库没有替换老版本gcc的动态库导致的，将gcc最新版本的动态库替换系统中老版本的动态库即可解决，运行以下命令检查动态库： strings /lib64/libstdc++.so.6 | grep GLIBC 以下是输出结果： GLIBCXX_3.4 GLIBCXX_3.4.1 GLIBCXX_3.4.2 GLIBCXX_3.4.3 GLIBCXX_3.4.4 GLIBCXX_3.4.5 GLIBCXX_3.4.6 GLIBCXX_3.4.7 GLIBCXX_3.4.8 GLIBCXX_3.4.9 GLIBCXX_3.4.10 GLIBCXX_3.4.11 GLIBCXX_3.4.12 GLIBCXX_3.4.13 GLIBCXX_3.4.14 GLIBCXX_3.4.15 GLIBCXX_3.4.16 GLIBCXX_3.4.17 GLIBCXX_3.4.18 GLIBCXX_3.4.19 GLIBC_2.3 GLIBC_2.2.5 GLIBC_2.14 GLIBC_2.4 GLIBC_2.3.2 GLIBCXX_DEBUG_MESSAGE_LENGTH从输出结果可以看到并没有“GLIBCXX_3.4.21“,所以可以断定我们的程序运行时动态加载的是老的动态库，解决这个问题需要将当前链接文件的链接指向改成最新的动态库地址： cp /usr/local/lib64/libstdc++.so.6.0.21 /lib64 cd /lib64 rm -rf libstdc++.so.6 ln -s libstdc++.so.6.0.21 libstdc++.so.6然后你可以执行以下命令来查看’GLIBCXX_3.4.21’已经可以找到了: strings /lib64/libstdc++.so.6 | grep GLIBC 解决了这个问题终于可以执行程序了，然后又测试了-g选项来编译程序，编译好程序调试程序时并不能够设置断点以及print变量的值，gdb调试中出现：Missing separate debuginfos, use: debuginfo-install glibc-2.17-106.e17_2.6.x86_4 libgcc-4.8.5-4.e17.x86_64的问题，通过上网查阅资料，是因为gcc版本和gdb版本并不匹配，或者说gdb版本过低 编译安装gdb-7.11.1gdb下载地址 tar -xvf gdb-7.11.1.tar.gz cd gdb-7.11.1 ./configure make make install当执行make install时gdb安装出现了错误：WARNING: &#39;makeinfo&#39; is missing on your sysem，则需安装相关依赖程序: yum install texinfo libncurses5-dev 如果调试程序时出现下面信息时： warning: File &quot;/usr/local/lib64/libstdc++.so.6.0.21-gdb.py&quot; auto-loading has been declined by your `auto-load safe-path&apos; set to &quot;$debugdir:$datadir/auto-load&quot;. To enable execution of this file add add-auto-load-safe-path /usr/local/lib64/libstdc++.so.6.0.21-gdb.py line to your configuration file &quot;/root/.gdbinit&quot;. To completely disable this security protection add set auto-load safe-path / line to your configuration file &quot;/root/.gdbinit&quot;.解决方法: 将以下信息放入~/.gdbinit add-auto-load-safe-path /usr/local/lib64/libstdc++.so.6.0.21-gdb.py set auto-load safe-path /若想通过gdb来调试STL容器，则还需要做一些配置，可以通过GDB Python pretty printers来解决这个问题： svn checkout svn://gcc.gnu.org/svn/gcc/trunk/libstdc++-v3/python stlPrettyPrinter mv stlPrettyPrinter /usr/local然后将下面的配置信息放入~/.gdbinit python import sys sys.path.insert(0, &apos;/usr/local/stlPrettyPrinter&apos;) from libstdcxx.v6.printers import register_libstdcxx_printers register_libstdcxx_printers (None) end]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ffmpeg 视音频同步]]></title>
    <url>%2Faeb01c06%2F</url>
    <content type="text"><![CDATA[原文地址: https://blog.csdn.net/nonmarking/article/details/50522413 对于直播流来说, 只考虑发送端的同步问题, 原理如下: 1. 解析视音频, 讲视频流和音频流的时间戳用同样的时间基准表示 2. 比较转换后的两个时间戳, 找出较小值, 对应发送偏慢的流 3. 读取, 转码, 发送相应的流, 同时, 若该流的转码时间很快, 超前于wall clock, 则还需要进行相应的延时 4. 重复以上过程 下文包括两部分, 一是音频转码部分, 二是视音频同步 音频转码基本流程首先是一些音频输入输出的基本设置 //Set own audio device&apos;s name if (avformat_open_input(&amp;ifmt_ctx_a, device_name_a, ifmt, &amp;device_param) != 0){ printf(&quot;Couldn&apos;t open input audio stream.（无法打开输入流）\n&quot;); return -1; } …… //input audio initialize if (avformat_find_stream_info(ifmt_ctx_a, NULL) &lt; 0) { printf(&quot;Couldn&apos;t find audio stream information.（无法获取流信息）\n&quot;); return -1; } audioindex = -1; for (i = 0; i &lt; ifmt_ctx_a-&gt;nb_streams; i++) if (ifmt_ctx_a-&gt;streams[i]-&gt;codec-&gt;codec_type == AVMEDIA_TYPE_AUDIO) { audioindex = i; break; } if (audioindex == -1) { printf(&quot;Couldn&apos;t find a audio stream.（没有找到视频流）\n&quot;); return -1; } if (avcodec_open2(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec, avcodec_find_decoder(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;codec_id), NULL) &lt; 0) { printf(&quot;Could not open audio codec.（无法打开解码器）\n&quot;); return -1; } …… //output audio encoder initialize pCodec_a = avcodec_find_encoder(AV_CODEC_ID_AAC); if (!pCodec_a){ printf(&quot;Can not find output audio encoder! (没有找到合适的编码器！)\n&quot;); return -1; } pCodecCtx_a = avcodec_alloc_context3(pCodec_a); pCodecCtx_a-&gt;channels = 2; pCodecCtx_a-&gt;channel_layout = av_get_default_channel_layout(2); pCodecCtx_a-&gt;sample_rate = ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_rate; pCodecCtx_a-&gt;sample_fmt = pCodec_a-&gt;sample_fmts[0]; pCodecCtx_a-&gt;bit_rate = 32000; pCodecCtx_a-&gt;time_base.num = 1; pCodecCtx_a-&gt;time_base.den = pCodecCtx_a-&gt;sample_rate; /** Allow the use of the experimental AAC encoder */ pCodecCtx_a-&gt;strict_std_compliance = FF_COMPLIANCE_EXPERIMENTAL; /* Some formats want stream headers to be separate. */ if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER) pCodecCtx_a-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER; if (avcodec_open2(pCodecCtx_a, pCodec_a, NULL) &lt; 0){ printf(&quot;Failed to open ouput audio encoder! (编码器打开失败！)\n&quot;); return -1; } //Add a new stream to output,should be called by the user before avformat_write_header() for muxing audio_st = avformat_new_stream(ofmt_ctx, pCodec_a); if (audio_st == NULL){ return -1; } audio_st-&gt;time_base.num = 1; audio_st-&gt;time_base.den = pCodecCtx_a-&gt;sample_rate; audio_st-&gt;codec = pCodecCtx_a;接下来, 考虑到输入音频的sample format 可能需要进行转换, 需要用到swresample库的功能 先做好相应的初始化 // Initialize the resampler to be able to convert audio sample formats aud_convert_ctx = swr_alloc_set_opts(NULL, av_get_default_channel_layout(pCodecCtx_a-&gt;channels), pCodecCtx_a-&gt;sample_fmt, pCodecCtx_a-&gt;sample_rate, av_get_default_channel_layout(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;channels), ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_fmt, ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_rate, 0, NULL); swr_init(aud_convert_ctx);此外, 参照transcode_aac.c的做法, 使用FIFO buffer存储从输入端解码得到的音频采样数据, 这些数据在后续将转换sample format并进行编码, 由此即完成了一个音频转码功. 此外, 还需要另外的一个buffer来存储转换合适之后的音频数据 //Initialize the FIFO buffer to store audio samples to be encoded. AVAudioFifo *fifo = NULL; fifo = av_audio_fifo_alloc(pCodecCtx_a-&gt;sample_fmt, pCodecCtx_a-&gt;channels, 1); //Initialize the buffer to store converted samples to be encoded. uint8_t **converted_input_samples = NULL; /** * Allocate as many pointers as there are audio channels. * Each pointer will later point to the audio samples of the corresponding * channels (although it may be NULL for interleaved formats). */ if (!(converted_input_samples = (uint8_t**)calloc(pCodecCtx_a-&gt;channels, sizeof(**converted_input_samples)))) { printf(&quot;Could not allocate converted input sample pointers\n&quot;); return AVERROR(ENOMEM); }至此, 一些基本的初始化工作完成. 音频计算pts的方法和视频类似. 即先通过sample rate算出每两个音频sample之间的时间间隔, 再通过计数当前已编码的音频sample总数(nb_samples变量的作用) 来算出当前编码音频帧的时间戳. 如果和视频的流程做类比, 大概为: framerate 相当于sample rate, framecnt相当于nb_samples. //audio trancoding here const int output_frame_size = pCodecCtx_a-&gt;frame_size; /** * Make sure that there is one frame worth of samples in the FIFO * buffer so that the encoder can do its work. * Since the decoder&apos;s and the encoder&apos;s frame size may differ, we * need to FIFO buffer to store as many frames worth of input samples * that they make up at least one frame worth of output samples. */ while (av_audio_fifo_size(fifo) &lt; output_frame_size) { /** * Decode one frame worth of audio samples, convert it to the * output sample format and put it into the FIFO buffer. */ AVFrame *input_frame = av_frame_alloc(); if (!input_frame) { ret = AVERROR(ENOMEM); return ret; } /** Decode one frame worth of audio samples. */ /** Packet used for temporary storage. */ AVPacket input_packet; av_init_packet(&amp;input_packet); input_packet.data = NULL; input_packet.size = 0; /** Read one audio frame from the input file into a temporary packet. */ if ((ret = av_read_frame(ifmt_ctx_a, &amp;input_packet)) &lt; 0) { /** If we are at the end of the file, flush the decoder below. */ if (ret == AVERROR_EOF) { encode_audio = 0; } else { printf(&quot;Could not read audio frame\n&quot;); return ret; } } /** * Decode the audio frame stored in the temporary packet. * The input audio stream decoder is used to do this. * If we are at the end of the file, pass an empty packet to the decoder * to flush it. */ if ((ret = avcodec_decode_audio4(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec, input_frame, &amp;dec_got_frame_a, &amp;input_packet)) &lt; 0) { printf(&quot;Could not decode audio frame\n&quot;); return ret; } av_packet_unref(&amp;input_packet); /** If there is decoded data, convert and store it */ if (dec_got_frame_a) { /** * Allocate memory for the samples of all channels in one consecutive * block for convenience. */ if ((ret = av_samples_alloc(converted_input_samples, NULL, pCodecCtx_a-&gt;channels, input_frame-&gt;nb_samples, pCodecCtx_a-&gt;sample_fmt, 0)) &lt; 0) { printf(&quot;Could not allocate converted input samples\n&quot;); av_freep(&amp;(*converted_input_samples)[0]); free(*converted_input_samples); return ret; } /** * Convert the input samples to the desired output sample format. * This requires a temporary storage provided by converted_input_samples. */ /** Convert the samples using the resampler. */ if ((ret = swr_convert(aud_convert_ctx, converted_input_samples, input_frame-&gt;nb_samples, (const uint8_t**)input_frame-&gt;extended_data, input_frame-&gt;nb_samples)) &lt; 0) { printf(&quot;Could not convert input samples\n&quot;); return ret; } /** Add the converted input samples to the FIFO buffer for later processing. */ /** * Make the FIFO as large as it needs to be to hold both, * the old and the new samples. */ if ((ret = av_audio_fifo_realloc(fifo, av_audio_fifo_size(fifo) + input_frame-&gt;nb_samples)) &lt; 0) { printf(&quot;Could not reallocate FIFO\n&quot;); return ret; } /** Store the new samples in the FIFO buffer. */ if (av_audio_fifo_write(fifo, (void **)converted_input_samples, input_frame-&gt;nb_samples) &lt; input_frame-&gt;nb_samples) { printf(&quot;Could not write data to FIFO\n&quot;); return AVERROR_EXIT; } } } /** * If we have enough samples for the encoder, we encode them. * At the end of the file, we pass the remaining samples to * the encoder. */ if (av_audio_fifo_size(fifo) &gt;= output_frame_size) /** * Take one frame worth of audio samples from the FIFO buffer, * encode it and write it to the output file. */ { /** Temporary storage of the output samples of the frame written to the file. */ AVFrame *output_frame=av_frame_alloc(); if (!output_frame) { ret = AVERROR(ENOMEM); return ret; } /** * Use the maximum number of possible samples per frame. * If there is less than the maximum possible frame size in the FIFO * buffer use this number. Otherwise, use the maximum possible frame size */ const int frame_size = FFMIN(av_audio_fifo_size(fifo), pCodecCtx_a-&gt;frame_size); /** Initialize temporary storage for one output frame. */ /** * Set the frame&apos;s parameters, especially its size and format. * av_frame_get_buffer needs this to allocate memory for the * audio samples of the frame. * Default channel layouts based on the number of channels * are assumed for simplicity. */ output_frame-&gt;nb_samples = frame_size; output_frame-&gt;channel_layout = pCodecCtx_a-&gt;channel_layout; output_frame-&gt;format = pCodecCtx_a-&gt;sample_fmt; output_frame-&gt;sample_rate = pCodecCtx_a-&gt;sample_rate; /** * Allocate the samples of the created frame. This call will make * sure that the audio frame can hold as many samples as specified. */ if ((ret = av_frame_get_buffer(output_frame, 0)) &lt; 0) { printf(&quot;Could not allocate output frame samples\n&quot;); av_frame_free(&amp;output_frame); return ret; } /** * Read as many samples from the FIFO buffer as required to fill the frame. * The samples are stored in the frame temporarily. */ if (av_audio_fifo_read(fifo, (void **)output_frame-&gt;data, frame_size) &lt; frame_size) { printf(&quot;Could not read data from FIFO\n&quot;); return AVERROR_EXIT; } /** Encode one frame worth of audio samples. */ /** Packet used for temporary storage. */ AVPacket output_packet; av_init_packet(&amp;output_packet); output_packet.data = NULL; output_packet.size = 0; /** Set a timestamp based on the sample rate for the container. */ if (output_frame) { nb_samples += output_frame-&gt;nb_samples; } /** * Encode the audio frame and store it in the temporary packet. * The output audio stream encoder is used to do this. */ if ((ret = avcodec_encode_audio2(pCodecCtx_a, &amp;output_packet, output_frame, &amp;enc_got_frame_a)) &lt; 0) { printf(&quot;Could not encode frame\n&quot;); av_packet_unref(&amp;output_packet); return ret; } /** Write one audio frame from the temporary packet to the output file. */ if (enc_got_frame_a) { output_packet.stream_index = 1; AVRational time_base = ofmt_ctx-&gt;streams[1]-&gt;time_base; AVRational r_framerate1 = { ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_rate, 1 };// { 44100, 1}; int64_t calc_duration = (double)(AV_TIME_BASE)*(1 / av_q2d(r_framerate1)); //内部时间戳 output_packet.pts = av_rescale_q(nb_samples*calc_duration, time_base_q, time_base); output_packet.dts = output_packet.pts; output_packet.duration = output_frame-&gt;nb_samples; //printf(&quot;audio pts : %d\n&quot;, output_packet.pts); aud_next_pts = nb_samples*calc_duration; int64_t pts_time = av_rescale_q(output_packet.pts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if ((pts_time &gt; now_time) &amp;&amp; ((aud_next_pts + pts_time - now_time)&lt;vid_next_pts)) av_usleep(pts_time - now_time); if ((ret = av_interleaved_write_frame(ofmt_ctx, &amp;output_packet)) &lt; 0) { printf(&quot;Could not write frame\n&quot;); av_packet_unref(&amp;output_packet); return ret; } av_packet_unref(&amp;output_packet); } av_frame_free(&amp;output_frame); } 视音频同步首先定义几个变量 int aud_next_pts = 0;//视频流目前的pts,可以理解为目前的进度 int vid_next_pts = 0;//音频流目前的pts int encode_video = 1, encode_audio = 1;//是否要编码视频、音频则相应的视音频同步方法如下: 1. 确定视频, 音频二者中至少有一个是需要进行转码的 2. 比较两个流的进度, 使用av_compare_ts函数, 注意：此时的vid_next_pts和aud_next_pts的time base都是ffmpeg内部基准，即AVRational time_base_q = { 1, AV_TIME_BASE }; 3. 对进度落后的流进行转码, 并相应地对进度进行更新. 对于视频，有 vid_next_pts=framecnt_calc_duration;，对于音频，有 aud_next_pts = nb_samples_calc_duration;这里framecnt和nb_samples都相当于计数器，而calc_duration是对应流每两个frame或sample之间的时间间隔，也是以ffmpeg内部时间基准为单位的 4. 若转码进度很快完成, 则不能急于写入输出流, 而是需要先进行延时, 但是也要保证延时后的时间不会超过另一个流的进度 综上, 流程如下: //start decode and encode int64_t start_time = av_gettime(); while (encode_video || encode_audio) { if (encode_video &amp;&amp; (!encode_audio || av_compare_ts(vid_next_pts, time_base_q, aud_next_pts, time_base_q) &lt;= 0)) { 进行视频转码； 转码完成后； vid_next_pts=framecnt*calc_duration; //general timebase //Delay int64_t pts_time = av_rescale_q(enc_pkt.pts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if ((pts_time &gt; now_time) &amp;&amp; ((vid_next_pts + pts_time - now_time)&lt;aud_next_pts)) av_usleep(pts_time - now_time); 写入流； } else { 进行音频转码； 转码完成后； aud_next_pts = nb_samples*calc_duration; int64_t pts_time = av_rescale_q(output_packet.pts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if ((pts_time &gt; now_time) &amp;&amp; ((aud_next_pts + pts_time - now_time)&lt;vid_next_pts)) av_usleep(pts_time - now_time); 写入流； }至此, 视音频同步完成. 最后再完成一些flush_encoder的工作即可.]]></content>
      <categories>
        <category>A&amp;amp;V</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ffmpeg 推流报错]]></title>
    <url>%2Feaddfbfe%2F</url>
    <content type="text"><![CDATA[在使用dshow设备推流时，经常会报出real time buffer too full dropping frames的错误信息，其原因在这篇文章里有写到，可以通过添加rtbufsize参数来解决，码率越高对应的rtbufsize就需要越高，但过高的rtbufsize会带来视频的延时，若要保持同步，可能就需要对音频人为增加一定的延时。而根据我的测试，即使不添加rtbufszie参数，虽然会报出错误信息，但并不影响直播流的观看或录制，而且可以保持同步。这就是一个trade off的问题了。]]></content>
      <categories>
        <category>A&amp;amp;V</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Protocol Buffer]]></title>
    <url>%2F66065582%2F</url>
    <content type="text"><![CDATA[Developer Guide.proto 文件 message Person { required string name = 1; required int32 id = 2; optional string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { required string number = 1; optional PhoneType type = 2 [default = HOME]; } repeated PhoneNumber phone = 4; }Once you’ve defined your messages, you run the protocol buffer compiler for your application’s language on your .proto file to generate data access classes. These provide simple accessors for each field (like name() and set_name()) as well as methods to serialize/parse the whole structure to/from raw bytes You can add new fields to your message formats without breaking backwards-compatibility; old binaries simply ignore the new field when parsing. So if you have a communications protocol that uses protocol buffers as its data format, you can extend your protocol without having to worry about breaking existing code. Language GuideDefining A Message Typesyntax = &quot;proto3&quot;; // First non-empty; first non-comment line message SearchRequest { string query = 1; // unique numbered tag int32 page_number = 2; int32 result_per_page = 3; }Specifying Field TypesAssigning Tags1-15 one byte 16-2047 two bytes you should reserve the tags 1 through 15 for very frequently occurring message elements. Remember to leave some room for frequently occurring elements that might be added in the future. range: 1 to 536,870,911 You also cannot use the numbers 19000 through 19999 (FieldDescriptor::kFirstReservedNumber through FieldDescriptor::kLastReservedNumber) Specifying Field Rules singular zero or one of this field repeated any number of times Adding More Message TypesReserved Fieldsmessage Foo { reserved 2, 15, 9 to 11; reserved &quot;foo&quot;, &quot;bar&quot;; }Note that you can’t mix field names and tag numbers in the same reserved statement. What’s Generated From Your .proto?Default Valuessigular: - string - byte - bool - numeric type - enum - message field repeated: - repeated filed Enumerationsmessage SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } Corpus corpus = 4; }You can define aliases by assigning the same value to different enum constants enum EnumAllowingAlias { option allow_alias = true; UNKNOWN = 0; STARTED = 1; RUNNING = 1; } enum EnumNotAllowingAlias { UNKNOWN = 0; STARTED = 1; // RUNNING = 1; // Uncommenting this line will cause a compile error inside Google and a warning message outside. }Reserved Valuesenum Foo { reserved 2, 15, 9 to 11, 40 to max; reserved &quot;FOO&quot;, &quot;BAR&quot;; }Note that you can’t mix field names and numeric values in the same reserved statement. Using Other Message TypesDefine a message in the same .proto file. message SearchResponse { repeated Result results = 1; } message Result { string url = 1; string title = 2; repeated string snippets = 3; }Importing DefinitionsBy default you can only use definitions from directly imported .proto files. import &quot;myproject/other_protos.proto&quot;; // new.proto // All definitions are moved here ====================================================== // old.proto // This is the proto that all clients are importing. import public &quot;new.proto&quot;; import &quot;other.proto&quot;; ====================================================== // client.proto import &quot;old.proto&quot;; // You use definitions from old.proto and new.proto, but not other.protoThe protocol compiler searches for imported files in a set of directories specified on the protocol compiler command line using the -I/–proto_path flag. If no flag was given, it looks in the directory in which the compiler was invoked. In general you should set the –proto_path flag to the root of your project and use fully qualified names for all imports. Using proto2 Message TypesIt’s possible to import proto2 message types and use them in your proto3 messages, and vice versa. However, proto2 enums cannot be used directly in proto3 syntax (it’s okay if an imported proto2 message uses them). Nested Typesmessage SearchResponse { message Result { string url = 1; string title = 2; repeated string snippets = 3; } repeated Result results = 1; }If you want to reuse this message type outside its parent message type, you refer to it as Parent.Type: message SomeOtherMessage { SearchResponse.Result result = 1; }You can nest messages as deeply as you like message Outer { // Level 0 message MiddleAA { // Level 1 message Inner { // Level 2 int64 ival = 1; bool booly = 2; } } message MiddleBB { // Level 1 message Inner { // Level 2 int32 ival = 1; bool booly = 2; } } }Updating A Message Type Don’t change the numeric tags for any existing fields If you add new fields, any messages serialized by code using your “old” message format can still be parsed by your new generated code Fields can be removed, as long as the tag number is not used again in your updated message type You may want to rename the field instead, perhaps adding the prefix “OBSOLETE_”, or make the tag reserved, so that future users of your .proto can’t accidentally reuse the number. Compatibility int32, uint32, int64, uint64, and bool are all compatible sint32 and sint64 are compatible with each other but are not compatible with the other integer types string and bytes are compatible as long as the bytes are valid UTF-8 Embedded messages are compatible with bytes if the bytes contain an encoded version of the message fixed32 is compatible with sfixed32, and fixed64 with sfixed64 enum is compatible with int32, uint32, int64, and uint64 in terms of wire format (note that values will be truncated if they don’t fit) Moving any fields into an existing oneof is not safe Anyimport &quot;google/protobuf/any.proto&quot;; message ErrorStatus { string message = 1; repeated google.protobuf.Any details = 2; }OneofYou can add fields of any type, but cannot use repeated fields Features: - Setting a oneof field will automatically clear all other members of the oneof - If the parser encounters multiple members of the same oneof on the wire, only the last member seen is used in the parsed message - If you’re using C++, make sure your code doesn’t cause memory crashes - Again in C++, if you Swap() two messages with oneofs, each message will end up with the other’s oneof case Mapsmap&lt;key_type, value_type&gt; map_field = N The key_type can be any integral or string type. The value_type can be any type except another map. Map fields cannot be repeated Wire format ordering and map iteration ordering of map values is undefined When generating text format for a .proto, maps are sorted by key When parsing from the wire or when merging, if there are duplicate map keys the last key seen is used. When parsing a map from text format, parsing may fail if there are duplicate keys backwords compatibility: message MapFieldEntry { key_type key = 1; value_type value = 2; } repeated MapFieldEntry map_field = N;PackagesJSON Mapping]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Qt 的 Pro 文件]]></title>
    <url>%2F8304997e%2F</url>
    <content type="text"><![CDATA[注释使用# 进行行注释 模板TEMPLATE = app 告诉qmake为这个应用程序生成哪种makefile. - app 默认值. 生成app的makefile - lib 生成一个库的makefile - vcapp 生成一个应用程序的VisualStudio项目文件 - vclib 生成一个库的VisualStudio 项目文件 - subdirs 生成makefile文件编译subdirs指定的子文件夹 应用程序目录指定生成的应用程序放置的目录 DESTDIR += ../bin 配置信息COFNIG 用来告诉qmake 关于应用程序的配置信息 CONFIG += qt warn_on release ui目录指定uic命令将.ui文件转化成的ui_*.h文件的存放目录 UI_DIR += forms rcc目录指定rcc命令将.qrc文件转换成的qrc_*.h文件的存放目录 RCC_DIR += ../tmp moc目录指定moc命令将含Q_OBJECT的头文件转换成标准.h文件的存放目录 MOC_DIR += ../tmp 目标文件目录指定目标文件(obj)的存放目录 OBJECTS_DIR += ../tmp 依赖相关路径程序编译时依赖的相关路径 DEPENDPATH += . forms include qrc sources 头文件包含路径INCLUDEPATH += . qmake时产生的信息message($$(PATH)) 源文件编码方式CODECFORSRC = GBK 工程中包含的头文件HEADERS += include/aa.h 工程中包含的.ui文件FORMS += forms/aa.ui 工程中包含的源文件SOURCES += sources/main.cpp sources/aa.cpp 工程中包含的资源文件RESOURCES += qrc/aa.qrc LIBS += -LfolderPath Release: LIBS += -LfolderReleasePath Debug: LIBS += -LfolderDebugPath DEFINES += XX_XX_XXX // 定义编译选项, 在.h文件中就可以用 #ifdefine XX_XX_XXX RC_FIELS = xxx.icns平台相关性处理根据qmake所运行的平台来使用相应的作用域来进行处理. 为Windows平台添加的依赖平台的文件示例: win32{ SOURCES += hello_win.cpp }生成Makefileqmake -oMakefile hello.pro 对于VisualStudio用户, qmake也可以生成.dsp文件 qmake -tvcapp -o hello.dsp hello.pro pro文件实例TEMPLATE = app #模块配置 LANGUAGE = C++ #C++语言 CONFIG += qt warn_on debug release #引入的lib文件,用于引入动态链接库 LIBS += qaxcontainer.lib #头文件包含路径 INCLUDEPATH += ../../qtcompnent/qtchklisten/inc ../../qtcompnent/qtclearfile/inc ../../validator/inc/validerrcode ../../qtcompnent/qtdir/inc ../inc ../../utillib/inc/xmlapi ../../utillib/inc/util ../../xercesc ../../qtcompnent/qteditor/inc ../../qtcompnent/qtfunreview/inc ../../qtcompnent/qttable/inc ../../qtcompnent/qtversion/inc ../../qtcompnent/qtini/inc ../../icdtool/icdservices/inc ../../icdtool/dataset/inc ../../icdtool/doi/inc ../../icdtool/reportcontrol/inc ../../icdtool/GSEconctrol/inc ../../icdtool/inputs/inc ../../icdtool/SMVconctrol/inc ../../icdtool/logcontrol/inc ../../scdpreview/inc/scdpreviewtoollib ../../scdpreview/form ../../icdtool/sclcontrol/inc ../../icdtool/log/inc ../../icdtool/settingcontrol/inc ../../qtcompnent/qteditor/inc ../../qtcompnent/qttreeview/inc ../../qtcompnent/qttabwidget/inc ../../communication/inc ../../qtcompnent/qtabout/inc ../iedmanage/inc ../ldmanage/inc ../foriecrun/inc ../../qtcompnent/validset/inc #工程中包含的头文件 HEADERS += ../inc/exportstable.h / ../inc/maintabwidget.h / ../inc/outputtab.h / ../inc/strutil.h / ../inc/treeeditview.h / ../inc/MainForm.h / ../inc/recenfileini.h / ../inc/ExportCIDFunction.h #工程中包含的源文件 SOURCES += ../src/main.cpp / ../src/exportstable.cpp / ../src/maintabwidget.cpp / ../src/outputtab.cpp / ../src/treeeditview.cpp / ../src/MainForm.cpp / ../src/recenfileini.cpp / ../src/ExportCIDFunction.cpp #工程中包含的.ui设计文件 FORMS = ../form/scdmainform.ui / ../form/exportiedform.ui / ../form/Exportsedform.ui / ../form/Importsedform.ui / ../form/formiminputs.ui #图像文件 IMAGES = images/substation.png / images/communication.png / images/autocom.png / images/reportcfg.png / images/comcfg.png / images/filetrans.png / images/review.png / images/setting.png #工程中包含的资源文件 RESOURCES = Scintilla.qrc #CONFIG -= release CONFIG -= debug RC_FILE = scdtool.rc BINLIB = ../../bin ../../xercesc/lib UI_HEADERS_DIR = ../inc # .ui文件转会为**.h 存放的目录 UI_SOURCES_DIR = ../src # .ui文件转会为**.cpp 存放的目录 QMAKE_LIBDIR = $${BINLIB} release { TARGET = scdtool #指定生成的应用程序名 OBJECTS_DIR = ../../obj/scdtool/release #指定目标文件(obj)的存放目录 } debug { TARGET = scdtool_d #指定生成的应用程序名 OBJECTS_DIR = ../../obj/scdtool/debug #指定目标文件(obj)的存放目录 } MOC_DIR = $${OBJECTS_DIR} DESTDIR = ../../bin #指定生成的应用程序放置的目录补充: cnblogs]]></content>
      <categories>
        <category>Qt</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于FFmpeg的摄像头直播(推流)]]></title>
    <url>%2Fae1aac27%2F</url>
    <content type="text"><![CDATA[原文地址: http://blog.csdn.net/wh8_2011/article/details/73506154 本文实现: 读取PC摄像头视频数据并以RTMP协议发送为直播流. 示例包含 1. FFmpeg的libavdevice的使用 2. 视频编码, 解码, 推流的基本流程 要使用libavdevice的相关函数, 首先需要注册相关组件 avdevice_register_all() 列出电脑中可用的DShow设备 AVFormatContext *pFmtCtx = avformat_alloc_context(); AVDeviceInfoList *device_info = NULL; AVDictionary* options = NULL; av_dict_set(&amp;options, &quot;list_devices&quot;, &quot;true&quot;, 0); AVInputFormat *iformat = av_find_input_format(&quot;dshow&quot;); printf(&quot;Device Info=============\n&quot;); avformat_open_input(&amp;pFmtCtx, &quot;video=dummy&quot;, iformat, &amp;options); printf(&quot;========================\n&quot;); 也可以直接使用FFmpeg的工具 ffmpeg -list_devices true -f dshow -i dummy PS: avdevice有一个avdevice_list_devices函数可以枚举系统的采集设备, 包括设备名和设备描述, 可以让用户选择要使用的设备, 但是不支持DShow设备. 像打开普通文件一样将上面的具体设备名作为输入打开, 并进行相应的初始化设置 av_register_all(); //Register Device avdevice_register_all(); avformat_network_init(); //Show Dshow Device show_dshow_device(); printf(&quot;\nChoose capture device: &quot;); if (gets(capture_name) == 0) { printf(&quot;Error in gets()\n&quot;); return -1; } sprintf(device_name, &quot;video=%s&quot;, capture_name); ifmt=av_find_input_format(&quot;dshow&quot;); //Set own video device&apos;s name if (avformat_open_input(&amp;ifmt_ctx, device_name, ifmt, NULL) != 0){ printf(&quot;Couldn&apos;t open input stream.（无法打开输入流）\n&quot;); return -1; } //input initialize if (avformat_find_stream_info(ifmt_ctx, NULL)&lt;0) { printf(&quot;Couldn&apos;t find stream information.（无法获取流信息）\n&quot;); return -1; } videoindex = -1; for (i = 0; i&lt;ifmt_ctx-&gt;nb_streams; i++) if (ifmt_ctx-&gt;streams[i]-&gt;codec-&gt;codec_type == AVMEDIA_TYPE_VIDEO) { videoindex = i; break; } if (videoindex == -1) { printf(&quot;Couldn&apos;t find a video stream.（没有找到视频流）\n&quot;); return -1; } if (avcodec_open2(ifmt_ctx-&gt;streams[videoindex]-&gt;codec, avcodec_find_decoder(ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;codec_id), NULL)&lt;0) { printf(&quot;Could not open codec.（无法打开解码器）\n&quot;); return -1; }输入设备初始化后, 需要对输出做相应的初始化. FFmpeg将网络协议和文件同等看待, 同时因为使用RTMP协议进行传输, 因此制定输出为flv格式, 编码器使用H.264 //output initialize avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, &quot;flv&quot;, out_path); //output encoder initialize pCodec = avcodec_find_encoder(AV_CODEC_ID_H264); if (!pCodec){ printf(&quot;Can not find encoder! (没有找到合适的编码器！)\n&quot;); return -1; } pCodecCtx=avcodec_alloc_context3(pCodec); pCodecCtx-&gt;pix_fmt = PIX_FMT_YUV420P; pCodecCtx-&gt;width = ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;width; pCodecCtx-&gt;height = ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;height; pCodecCtx-&gt;time_base.num = 1; pCodecCtx-&gt;time_base.den = 25; pCodecCtx-&gt;bit_rate = 400000; pCodecCtx-&gt;gop_size = 250; /* Some formats,for example,flv, want stream headers to be separate. */ if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER) pCodecCtx-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER; //H264 codec param //pCodecCtx-&gt;me_range = 16; //pCodecCtx-&gt;max_qdiff = 4; //pCodecCtx-&gt;qcompress = 0.6; pCodecCtx-&gt;qmin = 10; pCodecCtx-&gt;qmax = 51; //Optional Param pCodecCtx-&gt;max_b_frames = 3; // Set H264 preset and tune AVDictionary *param = 0; av_dict_set(&amp;param, &quot;preset&quot;, &quot;fast&quot;, 0); av_dict_set(&amp;param, &quot;tune&quot;, &quot;zerolatency&quot;, 0); if (avcodec_open2(pCodecCtx, pCodec,&amp;param) &lt; 0){ printf(&quot;Failed to open encoder! (编码器打开失败！)\n&quot;); return -1; } //Add a new stream to output,should be called by the user before avformat_write_header() for muxing video_st = avformat_new_stream(ofmt_ctx, pCodec); if (video_st == NULL){ return -1; } video_st-&gt;time_base.num = 1; video_st-&gt;time_base.den = 25; video_st-&gt;codec = pCodecCtx; //Open output URL,set before avformat_write_header() for muxing if (avio_open(&amp;ofmt_ctx-&gt;pb,out_path, AVIO_FLAG_READ_WRITE) &lt; 0){ printf(&quot;Failed to open output file! (输出文件打开失败！)\n&quot;); return -1; } //Show some Information av_dump_format(ofmt_ctx, 0, out_path, 1); //Write File Header avformat_write_header(ofmt_ctx,NULL); 完成输入和输出的初始化后, 就可以正式开始解码和编码并推流的流程了. 需要注意的是, 摄像头数据往往是RGB格式的, 需要将其转换为YUV420P格式, 才能推流, 因此要先做如下的准备工作 //prepare before decode and encode dec_pkt = (AVPacket *)av_malloc(sizeof(AVPacket)); //enc_pkt = (AVPacket *)av_malloc(sizeof(AVPacket)); //camera data has a pix fmt of RGB,convert it to YUV420 img_convert_ctx = sws_getContext(ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;width, ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;height, ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;pix_fmt, pCodecCtx-&gt;width, pCodecCtx-&gt;height, PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); pFrameYUV = avcodec_alloc_frame(); uint8_t *out_buffer = (uint8_t *)av_malloc(avpicture_get_size(PIX_FMT_YUV420P, pCodecCtx-&gt;width, pCodecCtx-&gt;height)); avpicture_fill((AVPicture *)pFrameYUV, out_buffer, PIX_FMT_YUV420P, pCodecCtx-&gt;width, pCodecCtx-&gt;height); 现在, 就可以正式开始解码, 编码 和推流了 //start decode and encode int64_t start_time=av_gettime(); while (av_read_frame(ifmt_ctx, dec_pkt) &gt;= 0){ if (exit_thread) break; av_log(NULL, AV_LOG_DEBUG, &quot;Going to reencode the frame\n&quot;); pframe = av_frame_alloc(); if (!pframe) { ret = AVERROR(ENOMEM); return -1; } //av_packet_rescale_ts(dec_pkt, ifmt_ctx-&gt;streams[dec_pkt-&gt;stream_index]-&gt;time_base, // ifmt_ctx-&gt;streams[dec_pkt-&gt;stream_index]-&gt;codec-&gt;time_base); ret = avcodec_decode_video2(ifmt_ctx-&gt;streams[dec_pkt-&gt;stream_index]-&gt;codec, pframe, &amp;dec_got_frame, dec_pkt); if (ret &lt; 0) { av_frame_free(&amp;pframe); av_log(NULL, AV_LOG_ERROR, &quot;Decoding failed\n&quot;); break; } if (dec_got_frame){ sws_scale(img_convert_ctx, (const uint8_t* const*)pframe-&gt;data, pframe-&gt;linesize, 0, pCodecCtx-&gt;height, pFrameYUV-&gt;data, pFrameYUV-&gt;linesize); enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&amp;enc_pkt); ret = avcodec_encode_video2(pCodecCtx, &amp;enc_pkt, pFrameYUV, &amp;enc_got_frame); av_frame_free(&amp;pframe); if (enc_got_frame == 1){ //printf(&quot;Succeed to encode frame: %5d\tsize:%5d\n&quot;, framecnt, enc_pkt.size); framecnt++; enc_pkt.stream_index = video_st-&gt;index; //Write PTS AVRational time_base = ofmt_ctx-&gt;streams[videoindex]-&gt;time_base;//{ 1, 1000 }; AVRational r_framerate1 = ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate;// { 50, 2 }; AVRational time_base_q = { 1, AV_TIME_BASE }; //Duration between 2 frames (us) int64_t calc_duration = (double)(AV_TIME_BASE)*(1 / av_q2d(r_framerate1)); //内部时间戳 //Parameters //enc_pkt.pts = (double)(framecnt*calc_duration)*(double)(av_q2d(time_base_q)) / (double)(av_q2d(time_base)); enc_pkt.pts = av_rescale_q(framecnt*calc_duration, time_base_q, time_base); enc_pkt.dts = enc_pkt.pts; enc_pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base); //(double)(calc_duration)*(double)(av_q2d(time_base_q)) / (double)(av_q2d(time_base)); enc_pkt.pos = -1; //Delay int64_t pts_time = av_rescale_q(enc_pkt.dts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if (pts_time &gt; now_time) av_usleep(pts_time - now_time); ret = av_interleaved_write_frame(ofmt_ctx, &amp;enc_pkt); av_free_packet(&amp;enc_pkt); } } else { av_frame_free(&amp;pframe); } av_free_packet(dec_pkt); } 解码比较简单, 编码部分需要自己计算PTS, DTS, 比较复杂 这里通过帧率计算PTS和DTS, 首先通过帧率计算两帧之间的时间间隔, 但是要换算]]></content>
      <categories>
        <category>A&amp;amp;V</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于FFmpeg的推送文件到RTMP服务器]]></title>
    <url>%2F5722b57a%2F</url>
    <content type="text"><![CDATA[原文地址: http://blog.csdn.net/leixiaohua1020/article/details/39803457 将本地的MOV/AVI/MKV/MP4/FLV等格式的媒体文件， 通过流媒体协议(RTMP, HTTP, UDP, TCP, RTP等)以直播流的形式推送出去. 在这个推流器的基础上, 可以进行以下几种方式的修改, 实现各式各样的推流器. 例如: * 将输入文件改为网络流URL, 可以显示转流器 * 将输入文件改为回调函数(内存读取)的形式, 可以推送内存中的视频数据 * 将输入文件改为系统设备(通过libavdevice), 同时加上编码的功能, 可以实现实时推流器(现场直播) 需要注意的地方封装格式RTMP采用的封装格式FLV, 因此在指定输出流媒体的时候需要制定其封装格式为”flv”. 同理, 其他流媒体协议也需要指定其封装格式. 例如采用UDP推送流媒体的时候, 可以指定其封装格式为”mpegts”. 延时发送流媒体的数据的时候需要延时. 否则, FFmpeg处理数据速度很快, 瞬间就能把所有的数据发送出去, 流媒体服务器是承受不了的. 因此需要按照视频实际的帧率发送数据. 本文的推流器在视频帧与帧之间采用av_usleep()函数休眠的方式来延迟发送. 这样就可以按照视频的帧率发送数据了, 代码如下 //… int64_t start_time=av_gettime(); while (1) { //… //Important:Delay if(pkt.stream_index==videoindex){ AVRational time_base=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; AVRational time_base_q={1,AV_TIME_BASE}; int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if (pts_time &gt; now_time) av_usleep(pts_time - now_time); } //… } //… PTS/DTS问题没有封装格式的裸流(例如H.264裸流)是不包含PTS, DTS这些参数的. 在发送这种数据的时候, 需要自己计算并写入AVPacket的PTS, DTS, duration等参数. //FIX：No PTS (Example: Raw H.264) //Simple Write PTS if(pkt.pts==AV_NOPTS_VALUE){ //Write PTS AVRational time_base1=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; //Duration between 2 frames (us) int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate); //Parameters pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE); pkt.dts=pkt.pts; pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE); } sequence 代码 /** * 最简单的基于FFmpeg的推流器（推送RTMP） * Simplest FFmpeg Streamer (Send RTMP) * * 雷霄骅 Lei Xiaohua * leixiaohua1020@126.com * 中国传媒大学/数字电视技术 * Communication University of China / Digital TV Technology * http://blog.csdn.net/leixiaohua1020 * * 本例子实现了推送本地视频至流媒体服务器（以RTMP为例）。 * 是使用FFmpeg进行流媒体推送最简单的教程。 * * This example stream local media files to streaming media * server (Use RTMP as example). * It&apos;s the simplest FFmpeg streamer. * */ #include &lt;stdio.h&gt; #define __STDC_CONSTANT_MACROS #ifdef _WIN32 //Windows extern &quot;C&quot; { #include &quot;libavformat/avformat.h&quot; #include &quot;libavutil/mathematics.h&quot; #include &quot;libavutil/time.h&quot; }; #else //Linux... #ifdef __cplusplus extern &quot;C&quot; { #endif #include &lt;libavformat/avformat.h&gt; #include &lt;libavutil/mathematics.h&gt; #include &lt;libavutil/time.h&gt; #ifdef __cplusplus }; #endif #endif int main(int argc, char* argv[]) { AVOutputFormat *ofmt = NULL; //输入对应一个AVFormatContext，输出对应一个AVFormatContext //（Input AVFormatContext and Output AVFormatContext） AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL; AVPacket pkt; const char *in_filename, *out_filename; int ret, i; int videoindex=-1; int frame_index=0; int64_t start_time=0; //in_filename = &quot;cuc_ieschool.mov&quot;; //in_filename = &quot;cuc_ieschool.mkv&quot;; //in_filename = &quot;cuc_ieschool.ts&quot;; //in_filename = &quot;cuc_ieschool.mp4&quot;; //in_filename = &quot;cuc_ieschool.h264&quot;; in_filename = &quot;cuc_ieschool.flv&quot;;//输入URL（Input file URL） //in_filename = &quot;shanghai03_p.h264&quot;; out_filename = &quot;rtmp://localhost/publishlive/livestream&quot;;//输出 URL（Output URL）[RTMP] //out_filename = &quot;rtp://233.233.233.233:6666&quot;;//输出 URL（Output URL）[UDP] av_register_all(); //Network avformat_network_init(); //输入（Input） if ((ret = avformat_open_input(&amp;ifmt_ctx, in_filename, 0, 0)) &lt; 0) { printf( &quot;Could not open input file.&quot;); goto end; } if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) &lt; 0) { printf( &quot;Failed to retrieve input stream information&quot;); goto end; } for(i=0; i&lt;ifmt_ctx-&gt;nb_streams; i++) if(ifmt_ctx-&gt;streams[i]-&gt;codec-&gt;codec_type==AVMEDIA_TYPE_VIDEO){ videoindex=i; break; } av_dump_format(ifmt_ctx, 0, in_filename, 0); //输出（Output） avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, &quot;flv&quot;, out_filename); //RTMP //avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, &quot;mpegts&quot;, out_filename);//UDP if (!ofmt_ctx) { printf( &quot;Could not create output context\n&quot;); ret = AVERROR_UNKNOWN; goto end; } ofmt = ofmt_ctx-&gt;oformat; for (i = 0; i &lt; ifmt_ctx-&gt;nb_streams; i++) { //根据输入流创建输出流（Create output AVStream according to input AVStream） AVStream *in_stream = ifmt_ctx-&gt;streams[i]; AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream-&gt;codec-&gt;codec); if (!out_stream) { printf( &quot;Failed allocating output stream\n&quot;); ret = AVERROR_UNKNOWN; goto end; } //复制AVCodecContext的设置（Copy the settings of AVCodecContext） ret = avcodec_copy_context(out_stream-&gt;codec, in_stream-&gt;codec); if (ret &lt; 0) { printf( &quot;Failed to copy context from input to output stream codec context\n&quot;); goto end; } out_stream-&gt;codec-&gt;codec_tag = 0; if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER) out_stream-&gt;codec-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER; } //Dump Format------------------ av_dump_format(ofmt_ctx, 0, out_filename, 1); //打开输出URL（Open output URL） if (!(ofmt-&gt;flags &amp; AVFMT_NOFILE)) { ret = avio_open(&amp;ofmt_ctx-&gt;pb, out_filename, AVIO_FLAG_WRITE); if (ret &lt; 0) { printf( &quot;Could not open output URL &apos;%s&apos;&quot;, out_filename); goto end; } } //写文件头（Write file header） ret = avformat_write_header(ofmt_ctx, NULL); if (ret &lt; 0) { printf( &quot;Error occurred when opening output URL\n&quot;); goto end; } start_time=av_gettime(); while (1) { AVStream *in_stream, *out_stream; //获取一个AVPacket（Get an AVPacket） ret = av_read_frame(ifmt_ctx, &amp;pkt); if (ret &lt; 0) break; //FIX：No PTS (Example: Raw H.264) //Simple Write PTS if(pkt.pts==AV_NOPTS_VALUE){ //Write PTS AVRational time_base1=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; //Duration between 2 frames (us) int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate); //Parameters pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE); pkt.dts=pkt.pts; pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE); } //Important:Delay if(pkt.stream_index==videoindex){ AVRational time_base=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; AVRational time_base_q={1,AV_TIME_BASE}; int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if (pts_time &gt; now_time) av_usleep(pts_time - now_time); } in_stream = ifmt_ctx-&gt;streams[pkt.stream_index]; out_stream = ofmt_ctx-&gt;streams[pkt.stream_index]; /* copy packet */ //转换PTS/DTS（Convert PTS/DTS） pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream-&gt;time_base, out_stream-&gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream-&gt;time_base, out_stream-&gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); pkt.duration = av_rescale_q(pkt.duration, in_stream-&gt;time_base, out_stream-&gt;time_base); pkt.pos = -1; //Print to Screen if(pkt.stream_index==videoindex){ printf(&quot;Send %8d video frames to output URL\n&quot;,frame_index); frame_index++; } //ret = av_write_frame(ofmt_ctx, &amp;pkt); ret = av_interleaved_write_frame(ofmt_ctx, &amp;pkt); if (ret &lt; 0) { printf( &quot;Error muxing packet\n&quot;); break; } av_free_packet(&amp;pkt); } //写文件尾（Write file trailer） av_write_trailer(ofmt_ctx); end: avformat_close_input(&amp;ifmt_ctx); /* close output */ if (ofmt_ctx &amp;&amp; !(ofmt-&gt;flags &amp; AVFMT_NOFILE)) avio_close(ofmt_ctx-&gt;pb); avformat_free_context(ofmt_ctx); if (ret &lt; 0 &amp;&amp; ret != AVERROR_EOF) { printf( &quot;Error occurred.\n&quot;); return -1; } return 0; }]]></content>
      <categories>
        <category>A&amp;amp;V</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ffmpeg 推流工具]]></title>
    <url>%2Fb3ed22dc%2F</url>
    <content type="text"><![CDATA[SRR测试网址 http://www.ossrs.net/srs.release/trunk/research/players/srs_player.html 获取 git clone https://github.com/ossrs/srs.git configure make cd srs/trunk ./configure &amp;&amp; make 开启服务器 ./objs/srs -c conf/srs.conf 列出设备 ./ffmpeg.exe -list_devices true -f dshow -i dummy ffmpeg采集摄像头推流 ffmpeg.exe -f dshow -i video=&quot;EasyCamera&quot; -q 4 -s 640*480 -aspect 4:3 -r 10 -vcodec flv -ar 22050 -ab 64k -ac 1 -acodec libmp3lame -threads 4 -f flv rtmp://192.168.1.102/RTMP/RtmpVideo ffmpeg采集摄像头和麦克风推流 ffmpeg -f dshow -i video=&quot;USB2.0 PC CAMERA&quot; -f dshow -i audio=&quot;麦克风 (2- USB2.0 MIC)&quot; -b:a 600k -ab 128k -f flv rtmp://192.168.1.102/RTMP/RtmpVideo]]></content>
      <categories>
        <category>A&amp;amp;V</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[命令组和代码块]]></title>
    <url>%2F8cb7d3c0%2F</url>
    <content type="text"><![CDATA[命令组 和 代码块() 命令组. 如 (a=hello,echo $a) 在()中的命令列表, 将作为一个子Shell来运行 在()中的变量, 由于是在子Shell总运行的, 因此对脚本剩下的部分是不可见的 如 a=123 (a=321;) echo &quot;a=$a&quot; # a=123 # 在()中的a变量, 更像是一个局部变量{} 代码块, 又称内部组. 这个结构创建了一个匿名的函数, 与函数不同的是, 在{}中声明的变量, 对于脚本剩余的代码是可见的, 如 { local a; a=123; } # bash中的local申请的变量只能用在函数中 a=123; {a=321;} echo &quot;a=$a&quot; # a=321()也可用作初始化数组 array=(element1,element2,element3) {xxx,yyy,zzz} 大括号扩展, 例 cat {file1,file2,file3} &gt; combined_file # 把file1 file2 file3连接在一起, 重定向到combined_file cp file1.{txt,bak} # 把file1.txt 复制到file1.bak一个命令会对大括号中以逗号分隔的文件列表起作用, file globbing会对大括号中的文件名作扩展 # 大括号中不允许有空白, 除非这个空白是有意义的 echo {file1,file2}\ :{\ A,&quot; B&quot;,&apos; C&apos;} # file1 : A file1 : B file1 : C file2 : A file2 : B file2 : C]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java Log]]></title>
    <url>%2F6196ae35%2F</url>
    <content type="text"><![CDATA[Log.d(TAG,new Exception().getStackTrace()[0].getMethodName()); //函数名 Log.d(TAG, Thread.currentThread().getStackTrace()[2].getMethodName()); //函数名 Log.d(TAG, “”+Thread.currentThread().getStackTrace()[2].getLineNumber()); //行号 Log.d(TAG, Thread.currentThread().getStackTrace()[2].getFileName()); //文件名 //文件名+行号 Log.d(TAG, “[“+Thread.currentThread().getStackTrace()[2].getFileName()+”,”+Thread.currentThread().getStackTrace()[2].getLineNumber()+”]“);]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell 中的冒号]]></title>
    <url>%2F9aa135a6%2F</url>
    <content type="text"><![CDATA[冒号(:) 是一个空命令. 作用与true相同. “:”是一个bash内建命令, 返回值为0, 即与true相同. 例: : echo $? # 0死循环 while : do list_1 list_2 doneif/then 中的占位符 if list then : # 什么都不做, 引出分支 else take-some-action fi在一个2元命令中, 提供一个占位符, 表明后面的表达式, 不是一个命令, 如 :$((n=$n+1)如果没有:, bash会尝试把”$((n=$n+1))” 解释成一个命令 使用”参数替换” 来评估字符串变量 :${HOSTNAME?}${USER?}${MAIL?} # 如果一个或多个环境变量没有设置, 则打印错误信息在和&gt;(重定向符号)结合使用时, 把一个文件截断到0长度, 不修改它的权限. 如果文件不存在, 则创建它 : &gt; data.xxx # 文件&quot;data.xxx&quot; 被清空 # 与 cat /dev/null &gt; data.xxx 作用相同, 但是不会产生一个新的进程, 因为:是一个内建命令.只适用于普通文件, 不适用于管道, 符号链接, 和其他特殊文件. 也可以用作注释, :与#不同的是, :不会关闭剩余行的错误检查.]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell编程中select用法]]></title>
    <url>%2F53b3b0c1%2F</url>
    <content type="text"><![CDATA[select提供了一个构建交互式菜单程序的方式, 语法结构: select name [ in word ] ; do list ; done 例: #!/bin/bash select i in a b c d do echo $i done执行结果 $ ./select.sh 1) a 2) b 3) c 4) d #? 选择索引 $ ./select.sh 1) a 2) b 3) c 4) d #? 1 a #? 2 b #? 3 c #? 4 d #? 6 #? 1) a 2) b 3) c 4) d #? 1) a 2) b 3) c 4) d #? 如果输入的不是菜单描述的范围就会echo一个空行，如果直接输入回车，就会再显示一遍菜单本身。当然我们会发现这样一个菜单程序似乎没有什么意义，实际程序中，select大多数情况是跟case配合使用的。 #!/bin/bash select i in a b c d do case $i in a) echo &quot;Your choice is a&quot; ;; b) echo &quot;Your choice is b&quot; ;; c) echo &quot;Your choice is c&quot; ;; d) echo &quot;Your choice is d&quot; ;; *) echo &quot;Wrong choice! exit!&quot; ;; esac done执行结果 $ ./select.sh 1) a 2) b 3) c 4) d #? 1 Your choice is a #? 2 Your choice is b #? 3 Your choice is c #? 4 Your choice is d #? 5 Wrong choice! exit!]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Shell脚本中使用ls命令的注意事项]]></title>
    <url>%2F12994d1%2F</url>
    <content type="text"><![CDATA[请对比如下两个测试： $ for i in `ls /etc`;do echo $i;done adjtime adobe appstream.conf arch-release asound.conf avahi bash.bash_logout bash.bashrc bind.keys binfmt.d ...... $ for i in /etc/*;do echo $i;done /etc/adjtime /etc/adobe /etc/appstream.conf /etc/arch-release /etc/asound.conf /etc/avahi /etc/bash.bash_logout /etc/bash.bashrc /etc/bind.keys /etc/binfmt.d ......像ls这样的命令很多时候是设计给人用的，它的很多显示是有特殊设定的，可能并不是纯文本。 比如可能包含一些格式化字符，也可能包含可以让终端显示出颜色的标记字符等等。 当我们在程序里面使用类似这样的命令的时候要格外小心，说不定什么时候在什么不同环境配置的系统上， 你的程序就会有意想不到的异常出现，到时候排查起来非常麻烦。 所以这里我们应该尽量避免使用ls这样的命令来做类似的行为，用通配符可能更好。 当然，如果你要操作的是多层目录文件的话，那么ls就更不能帮你的忙了，它遇到目录之后显示成这样： $ ls /etc/* /etc/adobe: mms.cfg /etc/avahi: avahi-autoipd.action avahi-daemon.conf avahi-dnsconfd.action hosts services /etc/binfmt.d: /etc/bluetooth: main.conf /etc/ca-certificates: extracted trust-source所以遍历一个目录还是要用两个连续的**，如果不是bash 4.0之后的版本的话，可以使用find。 我推荐用find，因为它更通用。 有时候你会发现，使用find之后，绝大多数原来需要写脚本解决的问题可能都用不着了，一个find命令解决很多问题]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>ls</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[邮件发送原理]]></title>
    <url>%2Fb0248f59%2F</url>
    <content type="text"><![CDATA[SMTP(Simple Mail Transfer Protocol)是电子邮件从客户机传输到服务器或从某一个服务器传输到另一个服务器使用的传输协议。SMTP 是请求/响应协议，命令和响应都是基于 ASCII 文本，并以 CR 和 LF 符结束。响应包括一个表示返回状态的三位数字代码。在 TCP 协议 25 端口监听连接请求。其命令如下： SMTP命令 命令说明 HELO ＜domain＞＜CRLF＞ 识别发送方到接收SMTP的一个HELO命令 AUTH LOGIN 登陆服务器的命令。在这条命令之后，要发送用Base64编码后的用户名与密码进行登陆 MAIL FROM:＜reverse-path＞＜CRLF＞ ＜reverse-path＞为发送者地址。此命令告诉接收方一个新邮件发送的开始，并对所有的状态和缓冲区进行初始化。此命令开始一个邮件传输处理，最终完成将邮件数据传送到一个或多个邮箱中 RCPT TO:＜forward-path＞＜CRLF＞ ＜forward-path＞标识各个邮件接收者的地址 DATA ＜CRLF＞ 接收SMTP将把其后的行为看作邮件数据去处理，以＜CRLF＞.＜CRLF＞标识数据的结尾 REST ＜CRLF＞ 退出/复位当前的邮件传输 NOOP ＜CRLF＞ 要求接收SMTP仅做OK应答。（用于测试） QUIT ＜CRLF＞ 要求接收SMTP返回一个OK应答并关闭传输。 VRFY ＜string＞ ＜CRLF＞ 验证指定的邮箱是否存在，由于安全因素，服务器多禁止此命令。 EXPN ＜string＞ ＜CRLF＞ 验证给定的邮箱列表是否存在，扩充邮箱列表，也常禁止使用。 HELP ＜CRLF＞ 查询服务器支持什么命令 邮件交互图 A-&gt;B: 1. 建立TCP连接(host:port, 默认port为25) B-&gt;A: 220. Anti-spam GT for Coremail System Note over A: A-&gt;B: 2. 向服务器标识用户身份(HELO host\r\/n) B-&gt;A: 250 OK Note over A: A-&gt;B: 3. 登录服务器(AUTH LOGIN\r\/n) B-&gt;A: 334. username: (这里是解密后的信息) A-&gt;B: &lt;my_username&gt;(要用Base64加密) B-&gt;A: 334. password: (这里是解密后的信息) A-&gt;B: &lt;my_password&gt;(要用Base64加密) B-&gt;A: 235. Authentication successful Note over A: A-&gt;B: 4. 指定发信者(MAIL FROM: &lt;my_sender@gmail.com&gt;\r\/n) B-&gt;A: 250. Mail OK Note over A: A-&gt;B: 5. 指定收信者(RCPT TO: &lt;my_receiver@gmail.com&gt;\r\/n) B-&gt;A: 250. Mail OK Note over A: A-&gt;B: 6. 发送数据(DATA\r\/n) B-&gt;A: 354. End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt; Note over A: A-&gt;B: 7. to: &lt;my_receiver@gmail.com\r\/nsubject:&lt;my_subject&gt;\r\/nSome Context\r\/n.\r\/n&gt; B-&gt;A: 250. Mail OK Note over A: A-&gt;B: 8. QUIT\r\/n B-&gt;A: 221. Bye因markdown里不能打出”\n”, 因此使用”\/n” 代替”\n” SMTP发信操作及返回码 [crazywill@localhost crazywill]$ telnet smtp.163.com 25 #telnet登录25端口 Trying 202.108.5.81... Connected to smtp.163.com. Escape character is &apos;^]&apos;. 220 163.com Coremail SMTP(Anti Spam) System EHLO smtp.163.com # 握手 :) 250-mail 250-PIPELINING 250-AUTH LOGIN PLAIN 250-AUTH=LOGIN PLAIN 250 8BITMIME AUTH LOGIN # 开始认证登录 334 dXNlcm5hbWU6 crazywill 334 UGFzc3dvcmQ6 mypassword 535 Error: authentication failed # 直接用户名密码不能登录 AUTH LOGIN 334 dXNlcm5hbWU6 Y3Jhenl3aWxs 334 UGFzc3dvcmQ6 bXlwYXNzd29yZA== 235 Authentication successful # 使用Base64编码则成功登录 MAIL FROM:&lt;test@163.com&gt; # 邮件发送方 553 You are not authorized to send mail, authentication is required # 不可伪造发送邮件 MAIL FROM:&lt;crazywill@163.com&gt; # 邮件发送方 250 Mail OK RCPT TO:&lt;crazywill@163.com&gt; # 邮件的接收方，若有多个收件人，则重复这一语句多次。 250 Mail OK DATA # 邮件体内容 354 Please start mail input. TO: crazywill@163.com # 此处的TO，FROM，等内容，可以随便造假 :) 可以骗人但骗不了懂得查看邮件源码的。 FROM: cccc@163.com SUBJECT: test by telnet/smtp test, just a test. # 邮件正文内容，与Header部分空一行开始写 . # 邮件写完，以一个句点加回车结果。 250 Mail OK queued as smtp10,wKjADQ2ApxRnnqBE0CWaEw==.38326S3 # 返回250 表示发送成功。 NOOP # 空语句，不执行任何操作，一般用来保持和服务器连接，不要掉线 250 OK QUIT # 退出 221 Closing connection. Good bye. Connection closed by foreign host. [crazywill@localhost crazywill]$ 参考资料: 用c++发邮件 电子邮件发送的原理以及简易实现 邮件正文及其附件的发送的C++实现 C++通过SMTP发送邮件总结 C++实现向多人发送邮件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]How to Split a String in C++]]></title>
    <url>%2F9747854a%2F</url>
    <content type="text"><![CDATA[这个问题是说, 怎么得到组成一句话的各个单词, 或者得到CSV中的各个数据片段. 这在C++中是个很简单的问题, 却有很多种答案. 有3种方案, 每种有利有弊. 使用时请自己选择最佳方案. 这篇文章的目的是说明 迭代器的接口是如何优胜于简单的容器的, 并且阐明 design of the STL 是何等强大. 方案1使用的标准组件(虽然方案1.2 做了微调). 方案2相对好点但使用了boost. 而方案3 更好但使用了ranges. 所以到底应该用哪个, 取决于你需要什么和你能使用什么. Solution 1: Iterating on a streamStepping into the world of streams“流” 是一个 能生成 与源或希望连接的目标 的联系 的对象. 流可以从源中获取信息(std::istream), 或为目标提供信息(std::ostream), 或者两者皆可(std::iostream). 源和目标可以是标准输入(std::cin), 标准输出(std::cout), 一个文件, 或者一个字符串, 前提是方式得当. 对流的主要操作包括: - 对于输入流: 使用操作符&gt;&gt; 从里面读取信息 - 对于输出流: 使用操作符&lt;&lt;, 向它推入信息 一个指向字符串的输入流, std::istringstream, 有个有趣的特性: 它的操作符&gt;&gt; 在源字符串中制造出去向下一个空格的字符串. istream_iteratorstd::istream_iterator 是连接输入流的迭代器. 它代表了输入迭代器的普遍接口, 但它的操作符++ 更像是输入流. istream_iterator 以它从流里读取的类型为模板. 我们现在使用istream_iterator&lt;std::string&gt;, 它从流里读取字符串, 分离时为我们提供一个字符串. 当到达流的终点时, 流向它的迭代器发送信号, 然后迭代器被标记为结束. Solution 1.1现在, 我们可以借迭代器的接口使用算法, 这真切地证明了STL 设计的灵活性. 为了使用STL, 我们需要一个begin 和一个end (请参考Inserting several elements into an STL container efficiently). begin 是一个 还没开始着手分割的字符串的istreamstream 的迭代器: std::istream_iterator&lt;std::string&gt;(iss) . 按照惯例, end 的默认值也是个istream_iterator : std::istream_iterator&lt;string&gt;(). 代码如下: std::string text = &quot;Let me split this into words&quot;; std::istringstream iss(text); std::vector&lt;std::string&gt; results((std::istream_iterator&lt;std::string&gt;(iss)), std::istream_iterator&lt;std::string&gt;());第一个参数的额外的括号是为了避免与一个函数调用的歧义–请参考Scott Meyers的著作Effective STL 条目6 “most vexing parse” 优: - 仅使用标准组件 - 除字符串外, 对所有流都适用 劣: - 只能以空格为分隔符进行分割, 而且这在解析CSV时会是个至关重要的问题 - 在性能方面有待优化(但如果这不是影响你整个程序的瓶颈, 这也不是个大问题) - 很多人认为仅为了分割一个字符串, 写了太多代码 Solution1.2: Pimp my operator&gt;&gt;导致上面两条劣势的原因是同一个: istream_iterator 从流里读取字符串时调用的操作符&gt;&gt;. 这个操作符做了很多事: 在下一个空格处停止(这是我们的最初的需求, 但这个不能自定义), 格式化, 读取然后设置一些标志位, 构造对象, 等等. 而以上这些, 大部分我们是不需要的. 所以我们希望自己实现下面的函数: std::istream&amp; operator&gt;&gt;(std::istream&amp; is, std::string&amp; output) { // ...does lots of things... }实际上, 我们无法改变这些, 因为这是在标注库里的. 我们可以用另一个类型重载它, 但是这个类型需要是string 的一种. 所以现在的需求就是, 用另一种类型伪装成string. 有两种方案: 继承std::string 和 用显式转换封装string. 这里我们选择继承. 假如我们希望以逗号为分割符分割一个字符串: class WordDelimitedByCommas: pulic std::string {};我必须承认这是有争议的. 有人会说:”std::string 没有虚析构函数, 所以你不应该继承它!” 这可能, 大概, 也许是有一点点点点武断. 这里我要说的是, 继承本身不会产生问题. 诚然, 当一个指向WordDelimitedByCommas 的指针以std::string 的形式被delete 掉时, 会产生问题. 继续读, 你会发现, 我们不会这么做. 现在我们可以阻止写代码的人借WordDelimitedByCommas 突发冷箭破坏程序吗? 我们不能. 但是这个险值得我们冒吗? 请继续读, 然后你自己判断. 现在为了仅实现我们需要的功能, 我们可以重载操作符&gt;&gt; : 获取下一个逗号之前的所有字符. 这个可以借用getline 函数实现: std::istream&amp; operator&gt;&gt;(std::istream* is, std::WordDelimitedByCommas&amp;) { std::getline(is, output, &apos;,&apos;); return is; }返回值is 保证了可以连续调用操作符&gt;&gt; 现在我们可以写初级代码了: std::string text = &quot;Let,me,split,this,into,words&quot;; std::istringstream iss(text); std::vector&lt;std::string&gt; results((std::istream_iterator&lt;WordDelimitedByCommas&gt;(iss)), std::istream_iterator&lt;WordDelimitedByCommas&gt;());我们可以通过模板化WordDelimitedByCommas 泛华所有的分隔符: template&lt;char delemiter&gt; class WordDelimitedBy: pulic std::string {};现在以分号举例: std::string text = &quot;Let;me;split;this;into;words&quot;; std::istringstream iss(text); std::vector&lt;std::string&gt; results((std::istream_iterator&lt;WordDelimitedBy&lt;&apos;;&apos;&gt;&gt;(iss)), std::istream_iterator&lt;WordDelimitedBy&lt;&apos;;&apos;&gt;&gt;());优: - 编译时允许任何分隔符 - 不仅是字符串, 对任何流都可以操作 - 比方案1更快(快20%到30%) 劣: - 虽然可以很方便的复用, 但仍不是标准 - 仅仅为了分割一个字符串, 这个方案仍然使用了大量代码 Solution2: Using boost::split这个方案比方案1高级, 除非你需要对所有的流都进行操作. #include &lt;boost/algorithm/string.hpp&gt; std::string text = &quot;Let me split this into words&quot;; std::vector&lt;std::string&gt; result; boost::split&lt;results, text, [](char c){return &apos; &apos; == c;});传给boost::split 的第三个参数是一个函数或函数对象, 确定一个字符是不是分隔符. 上面的例子是使用lambda 表达式, 传入一个char, 返回这个char 是否是空格. boost::split 的实现很简单: 在到达字符串的结束位置之前, 重复地调用find_if . 优: - 非常直观的接口 - 允许任何分隔符, 甚至是多个 - 高效: 比方案1.1 快 60% 劣: - 暂不是标准: 需要用到boost Solution 3(未来): Usingranges虽然它们现在还没有像标准库甚至boost 里的组件一样被广泛使用, ranges 是future of the STL . 在未来几年, 会大量面世. Eric Neiber 的 range-v3 库 提供了非常友好的接口. 为了生成一个字符串的分割view, 代码如下: std::string text = &quot;Let me split this into words&quot;; auto splitText = text | view::split(&apos; &apos;);它有很多有趣的特性, 诸如 使用一个子字符串作为分隔符. ranges 会被C++20 引入, 所以我们应该能在几年之内就可以使用这个功能了. So, how do I split my string?如果你能使用boost, 务必使用方案2. 或者你可以自己写算法, 像boost 那样基于find_if 分割字符串. 如果你不想这么做, 你可以使用标准, 即方案1.1, 如果你需要自定义分隔符, 或者发现1.1是个瓶颈, 那么你可以选择方案1.2 . 如果你可以使用ranges , 那么就应该选择方案3. 翻译原文: http://www.fluentcpp.com/2017/04/21/how-to-split-a-string-in-c/]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Strict Weak Ordering]]></title>
    <url>%2F22f34ac7%2F</url>
    <content type="text"><![CDATA[A strict weak ordering is a binary relation &lt; on a set S that is a strict partial order (a transitive relation that is irreflexive, or equivalently, that is asymmetric) in which the relation neither a &lt; b nor b &lt; a is transitive. Therefore, a strict weak ordering has the following properties: For all x in S, it is not the case that x &lt; x (irreflexivity). For all x, y in S, if x &lt; y then it is not the case that y &lt; x (asymmetry). For all x, y, z in S, if x &lt; y and y &lt; z then x &lt; z (transitivity). For all x, y, z in S, if x is incomparable with y (neither x &lt; y nor y &lt; x hold), and y is incomparable with z, then x is incomparable with z (transitivity of incomparability). This list of properties is somewhat redundant, as asymmetry follows readily from irreflexivity and transitivity. 离散数学中的relation: Given a function f (which models a binary relation) over a domain D, and a, b ∈ D: Reflexivity: f (a, a) is true. Asymmetry: For a ≠ b, if f(a, b) is true, f(b,a) is false Anti-symmetry: If f(a, b) and f(b, a) are both true iff a ≡ b Transitivity: If f(a, b) and f(b, c) are true, then f(a, c) is true Incomparability: Neither f(a, b) nor f(b, a) is true Transitivity of incomparability: If a and b are incomparable, and so are b and c, then a and c are incomparable. 摘自WikiPedia]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 禁止普通用户Su切换root]]></title>
    <url>%2Fe5131675%2F</url>
    <content type="text"><![CDATA[一般情况下, 普通用户执行”su -“命令, 可以登录为root. 为了加强系统的安全性, 有必要建立一个管理员的组, 只允许这个组的用户执行”su -“ 命令登录为root, 而让其他组的用户即使执行”su -“ 输入了正确的密码, 也无法登录为root用户. 在Unix 和Linux 下, 这个组的名称通常为”wheel”. 1 添加一个用户, 把这个用户加入wheel组 2 修改/etc/pam.d/su #auth required pam_wheel.so use_uid 这行注释打开 3 修改/etc/login.defs 在文件末添加一行 SU_WHEEL_ONLY yes]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[禁止Root用户ssh登录机器]]></title>
    <url>%2Fd29ba5b9%2F</url>
    <content type="text"><![CDATA[1 修改 /etc/ssh/sshd_config #PermitRootLogin yes 取消注释并改为 PermitRootLogin no 2 重启ssh /etc/init.d/sshd restart]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Move Semantics of RapidJSON]]></title>
    <url>%2Fe5a4892c%2F</url>
    <content type="text"><![CDATA[RapidJSON 的设计有一个特性, 进行赋值操作时, 不是把源value复制(copy)到目的 value, 而是转移(move)到目的value. 例如 Value a(123); Value b(456); b = a; // a becomes a Null value, b becomes number 123. 这样的设计的目的是 为了提高性能. 对于固定大小的JSON类型(Number, True, False, Null), 复制很简单快捷. 而对于可变大小的类型(String, Array, Object), 复制时会产生大量不容易被察觉的开销. 尤其是当我们需要创建一个临时的值, 把它复制给另一个变量, 然后析构它. 若使用正常的复制 语义: Document d; Value o(kObjectType); { Value contacts(kArrayType); // Adding elements to contacts array. // ... o.AddMember(&quot;contacts&quot;, contacts, d.GetAllocator(); // deep clone contacts(may be with lots of allocations) // destruct contact } o 需要分配跟contacts 大小一样的缓冲区, 做深度复制, 然后析构contacts . 这样会产生大量不必要的内存分配/释放 和内存复制. 有一些方案可以避免实质的复制这些数据, 如引用计数, 垃圾回收等等. 为了使RapidJSON简单和快速, 我们选择使用转移语义来进行赋值. 这与std::auto_ptr类似, 都是在赋值时转移拥有权. 转移比复制简捷地多, 它只需 析构原来的值, 把源值memcpy() 到目的值, 最后再把源值 设为Null类型. 使用转移语义, 上面的例子变成: Document d; Value o(kObjectType); { Value contacts(kArraryType); // Adding elements to contacts array. o.AddMember(&quot;contacts&quot;, contacts, d.GetAllocator()); // Just memcpy() of contacts itself to the value of new member(16 bytes) // contacts became Null here. Its destructiong is trivial. } 转移语义和临时值 有时, 我们想直接构造一个临时变量传给”转移”函数, 如PushBack() , AddMember() . 由于临时对象不能直接转化成正常的值引用, 我们可以调用Move() 函数 Value a(kArrayType); Document::AllocatorType&amp; allocator = document.GetAllocator(); // a.PushBack(Value(42), allocator); // Compiling error a.PushBack(Value().SetInt(42), allocator); // fluent API a.PushBack(Value(42).Move(), allocator); // same as above翻译原文: http://rapidjson.org/md\_doc\_tutorial.html#MoveSemantics]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
        <category>JSON</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]C++11 Sort Using Function Objects]]></title>
    <url>%2Fe754bcbe%2F</url>
    <content type="text"><![CDATA[如果你用C++编码， 需要对容器内的元素进行排序， 这个容器提供任意访问的迭代器， 比如std::vector， 那么简单快捷的方法是使用里的std::sort 函数. Basic sorting std::sort 函数需要两个参数, 这两个参数分别指向你要排序的序列容器的开始(initial)和终点(final). 这个序列容易内除final指向的那个元素外 所有元素都会被排序. 下面是一个简单的排序例子: #include &lt;algorithm&gt; #include &lt;vector&gt;&lt;/vector&gt;&lt;/algorithm&gt; const int array[] {10, 20, 5, 15, 0}; std::vector&lt;int&gt; vec(array, array + 5);&lt;/int&gt; std::sort(vec.begin(), vec.end());输出: 0 5 10 15 20 More complex sorting 在某些时候, 根据数值升序排序已经足够解决问题了, 但是当我们需要按某个特定的参数进行排序, 或者降序排列时, 就需要一些其他的东西了. 对于这种需求, std::sort 需要引入第三个参数: 比较函数. 这个比较函数有两个参数, 分别是序列容器的两个元素, 返回值可以隐式地转为bool. 如果第一个参数应该排在第二个参数前面, 则返回true. 例: #include &lt;algorithm&gt; #include &lt;vector&gt;&lt;/vector&gt;&lt;/algorithm&gt; bool DescOrderInt(int a, int b); ... const int array[] = {10, 20, 5, 15, 0}; std::vector&lt;int&gt; vec(array, array + 5);&lt;/int&gt; std::sort(vec.begin(), vec.end(), DescOrderInt);DescOrderInt的实现: bool DescOrderInt(int a, int b) { return a &amp;gt; b; }输出: 20 15 10 5 0 C++11 sort using function objects 网上很多例子说, 为了排列元素, 可以使用std::binary_function 定义比较函数, 但不幸的是, std::binary_function 在C++11 中已经被标为 “将被弃用的”, 在C++17中会被完全移除, 所以写新的C++代码时, 最好不要用这个. 我们可以使用C++11中引入的std::function 来定义这个函数指针. 例: #include &lt;algorithm&gt; #include &lt;function&gt; #include &lt;vector&gt;&lt;/vector&gt;&lt;/function&gt;&lt;/algorithm&gt; struct StrDescOrderInt { bool operator()(int a, int b) const { return a &amp;gt; b; } }; ... const int array[] = {10, 20, 5, 15, 0}; std::vector&lt;int&gt; vec(array, array + 5);&lt;/int&gt; std::function&lt;bool(int, int)=&quot;&quot;&gt; sorter = StrDescOrderInt();&lt;/bool(int,&gt; std::sort(vec.begin(), vec.end(), sorter);输出: 20 15 10 5 0 A real-life example: providing multiple sorting options 我们假设有一队足球运动员, 我们想让用户按他们自己的意愿去排列这些运动员. 有一个图表的UI, 上面有几个按钮, 每个按钮对应不用的排序规则. Plaer 类的代码: // -- Player.h -- #include &lt;string&gt;&lt;/string&gt; class Player { public: Player(const char * name, int caps, int goals); const std::string &amp;amp; GetName() const; int GetCaps() const; int GetGoals() const; private: std::string mName; int mCaps; int mGoals; };现在我们新写一个类或结构体来列出所有的比较函数. 比较函数是一个结构体并实现操作符(), 操作符() 带有两个参数, 分别为两个指向Player的指针, 返回bool值. class Player; struct PlayerSorting { // name struct SortPlayerByNameAsc (bool operator()(Player* p1, Player* p2) const;); struct SortPlayerByNameDes (bool operator()(Player* p1, Player* p2) const;); // caps struct SortPlayerByCapsAsc (bool operator()(Player* p1, Player* p2) const;); struct SortPlayerByCapsDes (bool operator()(Player* p1, Player* p2) const;); // goals struct SortPlayerByGoalsAsc (bool operator()(Player* p1, Player* p2) const;); struct SortPlayerByGoalsDes (bool operator()(Player* p1, Player* p2) const;); }然后, 在调用它的地方, 我们可以先把所有的std::function 存在一个std::vector 里, 使用的时候, 用索引访问vector的元素. std::vector&amp;lt; std::function&lt;bool(player *,=&quot;&quot; player=&quot;&quot; *)=&quot;&quot;&gt; &amp;gt; sorters; sorters.push_back(PlayerSorting::SortPlayerByNameAsc()); sorters.push_back(PlayerSorting::SortPlayerByCapsAsc()); sorters.push_back(PlayerSorting::SortPlayerByGoalsAsc()); sorters.push_back(PlayerSorting::SortPlayerByNameDes()); sorters.push_back(PlayerSorting::SortPlayerByCapsDes()); sorters.push_back(PlayerSorting::SortPlayerByGoalsDes());&lt;/bool(player&gt;例如, 根据得分降序排列: std::vector&lt;player *=&quot;&quot;&gt; players;&lt;/player&gt; // ...init players... std::sort(players.begin(), players.end(), sorters[5]);输出: NAME CAPS GOALS Lionel Messi 21 20 David Villa 13 16 Asamoah Gyan 22 15 Arjen Robben 11 12 Mesut Oezil 19 10 Diego Forlan 20 10 Andres Iniesta 15 9 Wesley Sneijder 24 6 Xavi 17 5 Bastian Schweinsteiger 23 4假如需要实现一种新的排序方式, 我们只需要在PlayerSorting类中添加一个新的仿函数即可. 原文地址: http://blog.davidecoppola.com/2015/01/cpp11-sort-using-function-objects/]]></content>
      <categories>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]String&#039;s Interface]]></title>
    <url>%2F4c3249cd%2F</url>
    <content type="text"><![CDATA[考虑以下代码: bool fun(const string&amp;amp; code) { assert(code.length() &amp;gt;= 2); if (code.substr(0, 2) == string(&quot;XX&quot;)) { // ... } // ... }有没有发现什么问题? 不要纠结于assert(), 它只是为了保证 string “code” 长度大于2而已. 很显然, 这段代码用来检查string是否以”XX”开头. 基于它长度大于2 的前提, 这段代码能正常运行. 我们的关心的问题是, 表达式能否达到正确的结果. 绝大多数情况下, 我们之所以使用C++, 是希望能使我们的程序达到最优的性能. 基于这个目标, 上面的代码看起来就不是很正确了. 为了检查”code”是否以”XX”开头, 我们生成了两个临时的string, 每个string都可能潜在地申请堆上的内存. 有人可能会为此辩解: std::string应该能为一个 2字母的序列实现 短字符串最优化(SSO). 就算这个辩解是正确的, 这段代码也已经 耗费了 一些不能被优化掉的开销, 更何况, 并不是所有的都会实现SSO. 例如, 我使用的GCC 4.4.7 就不会为string实现SSO. 类模板std::basic_string 的接口很复杂. 它提供了大量的成员函数, 似乎不用它们显得不领情, 同时开发者也不会有自己一遍遍重新解析的冲动. 因为开发者模糊地记得应用于NTBS(null-terminated byte strings)(可以被隐式地转为const char* )的 操作符 == 会使结果出错, 所以他通过 确保参与比较的两个值都是std::string 类型来避开这个错误. 他可能在想, 在运行操作符== 前文本”XX” 已经被显式地转成了std::string, 那么这么做也没有坏处. 但是, 这是错误的, 因为对于操作符==, 标准提供了两种版本: bool operator==(const std::string&amp;amp; lhs, const char* rhs); bool operator==(const char* lhs, const std::string&amp;amp; rhs);当然实际上他们是带有多个参数的函数模板, 远比这个复杂. std::string 可以直接跟NTBS比较, 没有必要生成临时的std::string. 我们开头的例子, 可以通过去除显式生成的临时副本 进行优化: if (code.substr(0, 2) == &quot;XX&quot;) 更进一步, 不可否认, 在有些地方使用操作符== 看起来很高雅, 但是仅仅为了检查一个string 本身的一部分而去新申请一部分资源(生成一个新的string) 这种做法是错误的. 开发者的初衷, 并不是要是程序看起来高雅. 实际上, 如果我们深入研究std::basic_string 的官方文档, 就会发现, std::basic_string提供了一种比较它的子字符串和NTBS的方法: if(code.compare(0, 2, &quot;XX&quot;) == 0) 这个比较是三方比较, 结果等于0表示相等. 它可以达到目的, 并且不需要生成任何临时的string. 尽管这个compare() 使性能达到了很大的优化, 但我并不满足于此. 虽然它做了正确的事情, 但如果我们是第一次遇到他, 很难抓住他的精髓. 如果你可以使用boost库, 我的建议性的解决方案是使用Boost String Algorithms Library 中的算法: #include &lt;boost algorithm=&quot;&quot; string=&quot;&quot; predicate.hpp=&quot;&quot;&gt;&lt;/boost&gt; bool func(const string&amp;amp; code) { if (boost::algorithm::starts_with(code, &quot;XX&quot;)) }这段代码很好地体现了我想说的意思, 没有任何多余的开销. 原文地址: https://akrzemi1.wordpress.com/2015/04/15/strings-interface/]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Custom Comparison, Equality and Equivalence With the STL]]></title>
    <url>%2F55f488ad%2F</url>
    <content type="text"><![CDATA[从一段代码引用开始: std::vector&amp;lt; std::pair&lt;int, std::string=&quot;&quot;&gt; &amp;gt; v1 = ... // v1 is filled with data std::vector&amp;lt; std::pair&lt;int, std::string=&quot;&quot;&gt; &amp;gt; v2 = ... // v2 is filled with data std::vector&amp;lt; std::pair&lt;int, std::string=&quot;&quot;&gt; &amp;gt; results;&lt;/int,&gt;&lt;/int,&gt;&lt;/int,&gt; std::sort(v1.begin(), v1.end()); std::sort(v2.begin(), v2.end()); std::set_difference(v1.begin(), v1.end(), v2.begin(), v2.end(), std::back_inserter(result), compareFirst);我们在两个排好序的vector v1 和 v2上调用std::set_difference. std::set_difference 把结果写入 result, std::back_inserter 确保输出的结果从result 的后面添入. 自定义的compareFirst 作为比较函数提供给std::set_difference 默认地, std::set_difference 通过 std::pair 默认的比较函数来比较里面的元素(比较pair的first和second), 我们自定义了compareFirst, 希望只比较pair的first. compareFirst不是STL的函数, 需要我们自己实现. std::set_difference 使用的前提是input已经排好序, 倘若我们自定义比较函数C, 而通过C我们能把元素排好序, 那么我们使用这个C代替sort的默认排序也是可以的. 在此例中, 我们使用std::set_difference 只对pair的first进行排序, 尽管它们已经通过”first + second”的方式排序完了. 下面来实现compareFirst. 初版: bool compareFirst(const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp;amp; p1, const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp;amp; p2) { return p1.first == p2.first; // not final code, bug lurking here! }实际上, 上面的代码不会得到我们预期的结果. 为什么? 毕竟std::set_difference 会检查元素跟另一个容器的元素是否相等(equal), 不是吗?&lt;/int,&gt;&lt;/int,&gt; 为了理解上面的内容, 我们把STL大概地分为两类: 操作排序元素的 和操作乱序元素的. Comparing elements C++中描述”a is the same as b” 有两种方法 - the natural way: a == b. This is called equality. Equality is based on operator==. - the other way: a is not smaller than b and b is not smaller than a, so !(a&lt;b) &amp;&amp;=&quot;&quot; !(b&lt;a).=&quot;&quot; this=&quot;&quot; is=&quot;&quot; called=&quot;&quot; equivalence.=&quot;&quot; equivalence=&quot;&quot; based=&quot;&quot; on=&quot;&quot; operator&lt;.=&quot;&quot; 12345678910111213141516171819 对于基本类型如int, 甚至实践中大多数类型, `equivalence` 和`quality` 是相通的. 但是正如*Scott Meyers* 在&amp;lt;&amp;lt; Effective STL&amp;gt;&amp;gt; 一书条目19中指出的, 对于有一些类型, 即使&quot;并非罕见&quot;, `equivalence` 和 `equality` 是不同的, 如 大小写不敏感的string类型. &lt;u&gt;Why such a far-fetched way to express a simple thing?&lt;/u&gt; 当我们使用算法对容器内元素进行排序时, 很容易理解必须有独一无二的排序方法(如有多种排序方法, 会很笨重, 并可能产生不一致的结果). 所以对于一个特定的容器, 排序时, &quot;==&quot; 和&quot;&amp;lt;&quot; 只能选一个. 对于STL中排序的部分, 我们别无选择: 排序时必须使用&quot;&amp;lt;&quot;; 而乱序部分, 则没有这个约束, 我们可以使用&quot;==&quot;. **Implementing the comparator** STL的乱序部分使用&quot;==&quot;, 而排序部分使用&quot;&amp;lt;&quot;. 我们自定义的比较函数也必须遵循这种逻辑. 现在我们可以理解怎么自定义实现`std::set_difference` 的比较函数`compareFirst` 了. bool compareFirst(const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp; p1, const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp; p2) &#123; return p1.first &lt; p2.first; // correct, STL-compatible code. &#125;原文地址: http://www.fluentcpp.com/2017/02/16/custom-comparison-equality-equivalence-stl/ &lt;/int,&gt;&lt;/int,&gt;]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]How to (Std::)find Something Efficiently With the STL]]></title>
    <url>%2F56dc57bb%2F</url>
    <content type="text"><![CDATA[本文分3部分: 1. 怎么使用STL进行高效的查找: 借用传统STL算法对元素进行范围搜索 2. 搜索STL容器: 当你有直接读取STL容器里元素的权限时, 怎么进行高效准确的搜索(与简单的范围搜索相比较) 3. STL搜索算法的秘密: 向公众展示不为人知的算法, 这些算法在已经学习过的人眼里确实是很有用的 STL根据查看方式的不同, 一共分为两种: 排序的和不排序的. * 排序集合的遍历, 通常需要对数时长, 而乱序集合的遍历, 需要线性时长 * 排序容器中比较元素大小的函数根据equivalence(comparing with &lt;), 而乱序容器中的函数根据equality(comparing with ==). 本文将展示对于在一个范围内搜索一个给定的值, C++怎么样去阐述下面3个问题: * 它存在否 * 它在哪 * 它应该在什么位置(排序容器) Is it there?乱序容器的元素这个问题可以用std::find来表达(需要和与范围的终点值的比较相结合): vector&lt;int&gt; v = ... // v filled with values if (std::find(v.begin(), v.end(), 42) != v.end()) { ...“Is it there”这个问题也可以用std::count来表达: vector&lt;int&gt; v = ... // v filled with values if (std::count(v.begin(), v.end(), 42)) { ...std::count()的返回值会被隐式地转换成if条件里的bool值: 如果该范围里有至少一个值为42, 则返回true. 与std::find相比, std::count的优劣: 优势: std::count避免了与范围的end值相比较 弊端: std::count遍历整个集合, 而std::find在第一个与要查找的值相等的位置停下 可以证明, 对于”想要查找某个值”这件事, std::find 表达得更明确 基于以上, std::find用得更多. Note 若要确认某个值存在而非是与要搜索的值相等, 请使用std::count_if, std::find_if, std::find_if_not 排序容器的元素使用的算法是std::binary_search, 此函数返回一个bool值, 此bool值表示在集合中是否存在与搜索的值相等的元素. std::set&lt;int&gt; numbers = // sorted elements bool is42InThere = std::binary_search(numbers.begin(), numbers.end(), 42); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 ### Where is it? (当确定了要搜索的值存在后,) 我们想更进一步, 得到指向那个元素的迭代器. #### 乱序容器的元素 使用std::find. 返回指向第一个与搜索的值相等的元素的迭代器, 如果找不到, 则返回集合的终点. std::vector numbers = ... auto searchResult = std::find(numbers.begin(), numbers.end(), 42);if (searchResult != numbers.end()) &#123; ... #### 排序容器的元素 对于排序集合, STL并没有像std::find一样直接的算法. std::find并不是为排序容器设计的, 因为它依据的是&quot;==&quot;而不是&quot;&amp;lt;&quot;, 消耗的时间为线性时长而不是对数时长. 对于一个给定的容器, 如果容器内元素的&quot;equality&quot;和&quot;equivalence&quot;是相同的, 且你能接受消耗的线性时长, 那么std::find会为你返回正确的结果, 你也能从它简单直接的接口中获益. **但是,** 不能忘记, std::find并不是为排序容器设计的. 这里推荐使用`std::equal_range`. (并非`std::lower_bound`) 函数原型: template&lt; class ForwardIt, class T &gt; std::pair&lt;forwardit,forwardit&gt; equal_range( ForwardIt first, ForwardIt last, const T&amp; value ); `std::equal_range` 返回与搜索值相等的元素的范围, 这个范围用一对集合内的迭代器表示. 这两个迭代器分别指向 与搜索值相等的范围里第一个元素和最后一个元素的下一个位置.&lt;/forwardit,forwardit&gt; 然而, 它的接口有些笨重: 例A: std::vector v = &#123;3, 7, 3, 11, 3, 3, 2&#125;; sort(v.begin(), v.end());// equal_range, attempt 1: natively clumsy std::pair&lt;std::vector::iterator, std::vector::iterator&gt; range1 = equal\_range(v.begin(), v.end(), 3); std::for\_each(range1.first, range1.second, doSomething); 用一个`typedef` 或者`using`让它更简洁: 例B: std::vector v = &#123;3, 7, 3, 11, 3, 3, 2&#125;; sort(v.begin(), v.end());&lt;/std::vectorusing IteratorPair = std::pair&lt;std::vector::iterator, std::vector::iterator&gt;;&lt;/std::vector// equal\_range, attempt 2: with the classical typedef IteratorPair range2 = equal\_range(v.begin(), v.end(), 3); std::for_each(range2.first, range2.second, doSomething); 例B确实简洁了很多, 但是仍有一个根本问题: 没有考虑 抽象等级. 尽管返回的是一个范围, 但这对迭代器强迫我们在操作返回的范围时必须按照&quot;第一&quot;&quot;第二&quot;这种方式来写代码. 范围就应该用&quot;首&quot;&quot;尾&quot;这种方式来表达. 这不仅给我们在其他地方使用这个返回值时造成很大的麻烦, 而且使代码很别扭. 为了解决这个问题, 我么可以把`std::equal_range` 返回的迭代器对封装进一个有&quot;范围&quot;这种语义的`object` templateclass Range&#123;public:Range(std::pair range)m\_begin(range.first), m\_end(range.second) &#123;&#125; typename Container::iterator begin() &#123; return m\_begin; &#125; typename Container::iterator end() &#123; return m\_end; &#125;private: typename Container::iterator m\_begin; typename Container::iterator m\_end; &#125;; 注意: 尽管`std::equal_range` 返回的结果是一个&quot;范围&quot;, 但是`std::begin` 和 `std::end` 不能用在这个结果上. 而上面的封装解决了这个问题. 可以像下面这样使用: std::vector v = &#123;3, 7, 3, 11, 3, 3, 2&#125;; sort(v.begin(), v.end());// equal_range, attempt 3: natural al last Range&lt;std::vector\&gt; range3 = equal\_range(v.begin(), v.end(), 3); std::for\_each(range3.begin(), range3.end(), doSomething); 不管你使用上面的哪种方式, `std::equal_range` 都会返回一个范围, 要确定它是否为空, 可以通过检查那两个迭代器(是否相等)或者使用`std::distance` 检查它的大小. &lt;/std::vector&lt;int&gt; bool noElementFound = range3.begin() == range3.end(); size_t numberOfElementFound = std::distance(range3.begin(), range3.end())Where should it be?这个问题仅仅针对排序的范围, 因为对于乱序的范围, 某个元素可能会存在任何位置. 对于排序的范围, 这个问题可以简化为: 如果它存在, 那么它在哪儿? 如果它不存在, 那么它应该在哪儿? 这个问题可以用算法std::lower_bound 和std::upper_bound 来解释. 当你理解了std::equal_range 后, 上面这句话就很容易理解了: std::lower_bound 和std::upper_bound 都会返回 std::equal_range 返回的那个迭代器对的第一个和第二个迭代器. 要插入某个值x, 使用std::lower_bound 得到指向 在范围里与x相等的元素之前的位置的迭代器, 使用std::upper_bound 得到指向 在范围里与x相等的元素之后的位置的迭代器. 注意: 如果仅仅是搜索某个元素, 永远不要使用std::lower_bound 与std::find 相反, 你不能根据 判断std::lower_bound 返回的迭代器是否与终点的迭代器相等 来判断要搜索的值是否存在于这个集合. 事实上, 如果这个值在集合里不存在, 则std::lower_bound 返回它应该在的位置, 而不是终点的迭代器. 所以, 你不仅需要确认返回的迭代器不是终点的迭代器, 还要确认它指向的元素跟要搜索的值是相等的. 总结Question to express in C++ NOT SORTED SORTED Is it there? std::find != end std::binary_search Where is it? std::find std::equal_range Where should it be? - std::lower_bound / std::upper_bound 原文地址: http://www.fluentcpp.com/2017/01/16/how-to-stdfind-something-efficiently-with-the-stl/?hmsr=toutiao.io&amp;utm\_medium=toutiao.io&amp;utm\_source=toutiao.io]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Effective STL 9]]></title>
    <url>%2F978f4b48%2F</url>
    <content type="text"><![CDATA[条款9：在删除选项中仔细选择 假定你有一个标准STL容器，c，容纳int， Container&lt;int&gt; c; 而你想把c中所有值为1963的对象都去掉。令人吃惊的是，完成这项任务的方法因不同的容 器类型而不同：没有一种方法是通用的。 如果你有一个连续内存容器（vector、deque或string——参见条款1），最好的方法是erase-remove惯用法（参见条款32）： c.erase(remove(c.begin(), c.end(), 1963), // 当c是vector、string c.end()); // 或deque时， // erase-remove惯用法 // 是去除特定值的元素 // 的最佳方法这方法也适合于list，但是，正如条款44解释的，list的成员函数remove更高效： c.remove(1963); // 当c是list时， // remove成员函数是去除 // 特定值的元素的最佳方法当c是标准关联容器（即，set、multiset、map或multimap）时，使用任何叫做remove的东 西都是完全错误的。这样的容器没有叫做remove的成员函数，而且使用remove算法可能覆 盖容器值（参见条款32），潜在地破坏容器。（关于这样的破坏的细节，参考条款22，那 个条款也解释了为什么试图在map和multimap上使用remove肯定不能编译，而试图在set和 multiset上使用可能不能编译。） 不，对于关联容器，解决问题的适当方法是调用erase： c.erase(1963); // 当c是标准关联容器时 // erase成员函数是去除 // 特定值的元素的最佳方法这不仅是正确的，而且很高效，只花费对数时间。（序列容器的基于删除的技术需要线性 时间。）并且，关联容器的erase成员函数有基于等价而不是相等的优势，条款19解释了这 一区别的重要性。 让我们现在稍微修改一下这个问题。不是从c中除去每个有特定值的物体，让我们消除下面 判断式（参见条款39）返回真的每个对象： bool badValue(int x); // 返回x是否是“bad” 对于序列容器（vector、string、deque和list），我们要做的只是把每个remove()替换为remove_if()，然后就完成了： c.erase(remove_if(c.begin(), c.end(), badValue), // 当c是vector、string c.end()); // 或deque时这是去掉 // badValue返回真 // 的对象的最佳方法 c.remove_if(badValue); // 当c是list时这是去掉 // badValue返回真 // 的对象的最佳方法对于标准关联容器，它不是很直截了当。有两种方法处理该问题，一个更容易编码，另一 个更高效。“更容易但效率较低”的解决方案用remove_copy_if()把我们需要的值拷贝到一 个新容器中，然后把原容器的内容和新的交换： AssocContainer&lt;int&gt; c; // c现在是一种 ... // 标准关联容器 AssocContainer&lt;int&gt; goodValues; // 用于容纳不删除 // 的值的临时容器 remove_copy_if(c.begin(), c.end(), // 从c拷贝不删除 inserter(goodValues, // 的值到 goodValues.end()), // goodValues badValue); c.swap(goodValues); // 交换c和goodValues // 的内容这种方法的缺点是它拷贝了所有不删除的元素，而这样的拷贝开销可能大于我们期望的底线。 我们可以通过直接从原容器删除元素来避开拷贝的开销。不过，因为关联容器没有提供类似remove_if()的成员函数，所以我们必须写一个循环来迭代c中的元素，和原来一样删除元素. 看起来，这个任务很简单，而且实际上，代码也很简单。不幸的是，那些正确工作的代码 很少是跃出脑海的代码。例如，这是很多程序员首先想到的： AssocContainer&lt;int&gt; c; ... for (AssocContainer&lt;int&gt;::iterator i = c.begin(); // 清晰，直截了当 i!= c.end(); // 而漏洞百出的用于 ++i) { // 删除c中badValue返回真 if (badValue(*i)) c.erase(i); // 的每个元素的代码 } // 不要这么做！&lt;/int&gt;&lt;/int&gt;唉，这有未定义的行为。当容器的一个元素被删时，指向那个元素的所有迭代器都失效了 。当c.erase(i)返回时，i已经失效。那对于这个循环是个坏消息，因为在erase()返回后， i通过for循环的++i部分自增。 为了避免这个问题，我们必须保证在调用erase之前就得到了c中下一元素的迭代器。最容 易的方法是当我们调用时在i上使用后置递增： AssocContainer&lt;int&gt; c; ... for (AssocContainer&lt;int&gt;::iterator i = c.begin(); // for循环的第三部分 i != c.end(); // 是空的；i现在在下面 /*nothing*/ ){ // 自增 if (badValue(*i)) c.erase(i++); // 对于坏的值，把当前的 else ++i; // i传给erase，然后 } // 作为副作用增加i； // 对于好的值， // 只增加i这种调用erase()的解决方法可以工作，因为表达式i++的值是i的旧值，但作为副作用，i增 加了。因此，我们把i的旧值（没增加的）传给erase，但在erase开始执行前i已经自增了 。那正好是我们想要的。正如我所说的，代码很简单，只不过不是大多数程序员在第一次 尝试时想到的。 现在让我们进一步修改该问题。不仅删除badValue返回真的每个元素，而且每当一个元素 被删掉时，我们也想把一条消息写到日志文件中。 对于关联容器，这说多容易就有多容易，因为只需要对我们刚才开发的循环做一个微不足 道的修改就行了： ofstream logFile; // 要写入的日志文件 AssocContainer&lt;int&gt; c; ... for (AssocContainer&lt;int&gt;::iterator i = c.begin(); // 循环条件和前面一样 i !=c.end();){ if (badValue(*i)){ logFile &amp;lt;&amp;lt; &quot;Erasing &quot; &amp;lt;&amp;lt; *i &amp;lt;&amp;lt;&apos;\n&apos;; // 写日志文件 c.erase(i++); // 删除元素 } else ++i; }现在是vector、string和deque给我们带来麻烦。我们不能再使用erase-remove惯用法，因为没有办法让erase()或remove()写日志文件。而且，我们不能使用刚刚为关联容器开发的循环, 因为它为vector、string和deque产生未定义的行为！要记得对于那样的容器，调用erase不仅使所有指向被删元素的迭代器失效，也使被删元素之后的所有迭代器失效。在我们的情况里，那包括所有i之后的迭代器。我们写i++，++i或你能想起的其它任何东西都没有用，因为没有能导致迭代器有效的。 我们必须对vector、string和deque采用不同的战略。特别是，我们必须利用erase()的返回值。那个返回值正是我们需要的：一旦删除完成，它就是指向紧接在被删元素之后的元素的有效迭代器。换句话说，我们这么写： for (SeqContainer&lt;int&gt;::iterator i = c.begin(); i != c.end();){ if (badValue(*i)){ logFile &amp;lt;&amp;lt; &quot;Erasing &quot; &amp;lt;&amp;lt; *i &amp;lt;&amp;lt; &apos;\n&apos;; i = c.erase(i); // 通过把erase的返回值 } // 赋给i来保持i有效 else ++i; }这可以很好地工作，但只用于标准序列容器。由于论证一个可能的问题（条款5做了），标准关联容器的erase()的返回类型是void[1]。对于那些容器，你必须使用“后置递增你要传给erase()的迭代器”技术。（顺便说说，在为序列容器编码和为关联容器编码之间的这种差别是为什么写容器无关代码一般缺乏考虑的一个例子——参见条款2。) 为了避免你奇怪list的适当方法是什么，事实表明对于迭代和删除，你可以像vector/str ing/deque一样或像关联容器一样对待list；两种方法都可以为list工作。 如果我们观察在本条款中提到的所有东西，我们得出下列结论： 去除一个容器中有特定值的所有对象： 如果容器是vector、string或deque，使用erase-remove惯用法。 如果容器是list，使用list::remove。 如果容器是标准关联容器，使用它的erase成员函数。 去除一个容器中满足一个特定判定式的所有对象： 如果容器是vector、string或deque，使用erase-remove_if惯用法。 如果容器是list，使用list::remove_if。 如果容器是标准关联容器，使用remove_copy_if和swap，或写一个循环来遍历容器元素， 当你把迭代器传给erase时记得后置递增它。 在循环内做某些事情（除了删除对象之外）： 如果容器是标准序列容器，写一个循环来遍历容器元素，每当调用erase时记得都用它的返回值更新你的迭代器。 如果容器是标准关联容器，写一个循环来遍历容器元素，当你把迭代器传给erase时记得后置递增它。 如你所见，与仅仅调用erase相比，有效地删除容器元素有更多的东西。解决问题的最好方法取决于你是怎样鉴别出哪个对象是要被去掉的，储存它们的容器的类型，和当你删除它们的时候你还想要做什么（如果有的话）。只要你小心而且注意了本条款的建议，你将毫不费力。如果你不小心，你将冒着产生不必要低效的代码或未定义行为的危险。 ------------------------------------------------------------------------------[1] 这仅对带有迭代器实参的erase()形式是正确的。关联容器也提供一个带有一个值的实参 的erase()形式，而那种形式返回被删掉的元素个数。但这里，我们只关心通过迭代器删除东 西。 参考地址]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[STL 的 Erase( ) 陷阱-迭代器失效总结]]></title>
    <url>%2Fb12cd95a%2F</url>
    <content type="text"><![CDATA[STL中的容器按存储方式分为两类，一类是按以数组形式存储的容器（如：vector 、deque)；另一类是以不连续的节点形式存储的容器（如：list、set、map）。在使用erase方法来删除元素时，需要注意一些问题。 1.list,set,map容器在使用 list、set 或 map遍历删除某些元素时可以这样使用： 1.1 正确写法 1 std::list&lt;int&gt; list; std::list&lt;int&gt;::iterator it_list; for (it_list = list.begin(); it_list != list.end();) { if (willDelete(*it_list)) { it_list = list.erase(it_list); } else { ++it_list; } }Note: 以上方法仅适用于standard sequence container, 因为对于standard associative container, erase()的返回类型为void. (查阅Effective STL Item 9)以下为原文: This works wonderfully, but only for the standard sequence containers. Due to reasoning one might question, erase()&apos;s return type for the standard associative containers is void. For those containers, you have to use the postincrement-the-iterator-you-pass-to-erase technique.1.2 正确写法2 查阅原版Effctive STL Item 9, 证实, 下面这种写法不能用于标准序列容器, 而适用于标准关联容器, 而List也可以使用这种方法. std::list&lt;int&gt; list; std::list&lt;int&gt;::iterator it_list; for (it_list = list.begin(); it_list != list.end();) { if (willDelete(*it_list)) { list.erase(it_list++); // 必须使用后缀自增, 不能使用前缀自增 } else { ++it_list; } } 123456789101112131415161718192021222324252627282930 **1.3 错误写法 1** std::list&lt; int&gt; List; std::list&lt; int&gt;::iterator itList; for( itList = List.begin(); itList != List.end(); itList++) &#123; if( WillDelete( *itList) ) &#123; List.erase( itList); &#125; &#125; **1.4 错误写法 2** std::list&lt; int&gt; List; std::list&lt; int&gt;::iterator itList; for( itList = List.begin(); itList != List.end(); ) &#123; if( WillDelete( *itList) ) &#123; itList = List.erase( ++itList); &#125; else itList++; &#125; **1.5 分析** 正确方法1: 通过erase()方法的返回值来获取下一个元素的位置; 正确方法2: 在调用erase()方法之前先使用&quot;++&quot; 来获取下一个元素的位置; 错误使用方法1: 在调用erase()方法之后使用&quot;++&quot; 来获取下一个元素的位置, 由于在调用erase()方法之后, 该元素的位置已经被删除, 如果再根据这个旧的位置来获取下一个位置, 则会出现异常; 错误使用方法2: 同上 ####**2. vector,deque 容器** 在使用 vector、deque遍历删除元素时，也可以通过erase的返回值来获取下一个元素的位置： **2.1 正确写法:** std::vector vec; std::vector::iterator it\_vec; for (it\_vec = vec.begin(); it\_vec != vec.end();) &#123; if (willDelete(*it\_vec)) &#123; it\_vec = vec.erase(it\_vec); &#125; else &#123; ++it_vec; &#125; &#125;2.2 注意 vector, deque 不能像上面的”正确方法2” 的办法来遍历删除. 原因请参考Effective STL条款9。摘录到下面： 1) 对于关联容器(如map, set, multimap, multiset)，删除当前的iterator，仅仅会使当前的iterator失效，只要在erase时，递增当前iterator即可。这是因为map之类的容器，使用了红黑树来实现，插入、删除一个结点不会对其他结点造成影响。 for (iter = cont.begin(); it != cont.end();) { (*iter)-&amp;gt;doSomething(); if (shouldDelete(*iter)) cont.erase(iter++); else ++iter; }因为iter传给erase方法的是一个副本，iter++会指向下一个元素。 2) 对于序列式容器(如vector, deque)，删除当前的iterator会使后面所有元素的iterator都失效。这是因为vetor, deque使用了连续分配的内存，删除一个元素导致后面所有的元素会向前移动一个位置。还好erase()方法可以返回下一个有效的iterator。 for (iter = cont.begin(); iter != cont.end();) { (*it)-&amp;gt;doSomething(); if (shouldDelete(*iter)) iter = cont.erase(iter); else ++iter; }3)对于list来说，它使用了不连续分配的内存，并且它的erase()方法也会返回下一个有效的iterator，因此上面两种方法都可以使用。 3. 其他set 键和值相等。 键唯一。 元素默认按升序排列。 如果迭代器所指向的元素被删除，则该迭代器失效。其它任何增加、删除元素的操作都不会使迭代器失效 map 键唯一。 元素默认按键的升序排列。 如果迭代器所指向的元素被删除，则该迭代器失效。其它任何增加、删除元素的操作都不会使迭代器失效。 作成参考地址]]></content>
      <categories>
        <category>C++</category>
        <category>En</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[用GDB调试程序]]></title>
    <url>%2F13b68d49%2F</url>
    <content type="text"><![CDATA[使用GDB一般来说GDB主要调试的是C/C++的程序。要调试C/C++的程序，首先在编译时，我们必须要把调试信息加到可执行文件中。使用编译器（cc/gcc/g++）的 -g 参数可以做到这一点。如： $gcc -g -Wall hello.c -o hello $g++ -g -Wall hello.cpp -o hello如果没有-g，你将看不见程序的函数名、变量名，所代替的全是运行时的内存地址。当你用-g把调试信息加入之后，并成功编译目标代码以后，让我们来看看如何用gdb来调试他。 启动GDB的方法有以下几种： gdb &lt;program&gt; program也就是你的执行文件，一般在当前目录下。 gdb &lt;program&gt; core 用gdb同时调试一个运行程序和core文件，core是程序非法执行后core dump后产生的文件。 gdb &lt;program&gt; &lt;pid&gt; 如果你的程序是一个服务程序，那么你可以指定这个服务程序运行时的进程ID。gdb会自动attach上去，并调试他。program应该在PATH环境变量中搜索得到。 以上三种都是进入gdb环境和加载被调试程序同时进行的。也可以先进入gdb环境，在加载被调试程序，方法如下： *在终端输入：gdb *在gdb环境中：file &lt;program&gt; 这两步等价于：gdb &lt;program&gt;GDB启动时，可以加上一些GDB的启动开关，详细的开关可以用gdb -help查看。我在下面只例举一些比较常用的参数： -symbols &lt;file&gt; -s &lt;file&gt; 从指定文件中读取符号表。 -se file 从指定文件中读取符号表信息，并把他用在可执行文件中。 -core &lt;file&gt; -c &lt;file&gt; 调试时core dump的core文件。 -directory &lt;directory&gt; -d &lt;directory&gt; 加入一个源文件的搜索路径。默认搜索路径是环境变量中PATH所定义的路径。 ```&lt;/directory&gt;&lt;/directory&gt;&lt;/file&gt;&lt;/file&gt;&lt;/file&gt;&lt;/file&gt;&lt;/program&gt;&lt;/program&gt;&lt;/pid&gt;&lt;/program&gt;&lt;/program&gt;&lt;/program&gt; ###GDB的命令概貌 启动gdb后，你就被带入gdb的调试环境中，就可以使用gdb的命令开始调试程序了，gdb的命令可以使用help命令来查看，如下所示： ```bash $ gdb GNU gdb 6.7.1-debian Copyright (C) 2007 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http: gnu.org=&quot;&quot; licenses=&quot;&quot; gpl.html=&quot;&quot;&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot; and &quot;show warranty&quot; for details. This GDB was configured as &quot;i486-linux-gnu&quot;. (gdb) help List of classes of commands:&lt;/http:&gt; aliases -- Aliases of other commands breakpoints -- Making program stop at certain points data -- Examining data files -- Specifying and examining files internals -- Maintenance commands obscure -- Obscure features running -- Running the program stack -- Examining the stack status -- Status inquiries support -- Support facilities tracepoints -- Tracing of program execution without stopping the program user-defined -- User-defined commands Type &quot;help&quot; followed by a class name for a list of commands in that class. Type &quot;help all&quot; for the list of all commands. Type &quot;help&quot; followed by command name for full documentation. Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;. Command name abbreviations are allowed if unambiguous. (gdb)gdb 的命令很多，gdb把之分成许多个种类。help命令只是例出gdb的命令种类，如果要看种类中的命令，可以使用help &lt;class&gt;命令，如：help breakpoints，查看设置断点的所有命令。也可以直接help &lt;command&gt;&lt;/command&gt;来查看命令的帮助。 gdb中，输入命令时，可以不用打全命令，只用打命令的前几个字符就可以了，当然，命令的前几个字符应该要标志着一个唯一的命令，在Linux下，你可以敲击两次TAB键来补齐命令的全称，如果有重复的，那么gdb会把其列出来。 示例一：在进入函数func时，设置一个断点。可以敲入break func，或是直接就是b func (gdb) b func Breakpoint 1 at 0x804837a: file tst.c, line 5.示例二：敲入b按两次TAB键，你会看到所有b打头的命令： (gdb) b backtrace break bt (gdb)示例三：只记得函数的前缀，可以这样： (gdb) b make_ &amp;lt;按TAB键&amp;gt; （再按下一次TAB键，你会看到:） make_a_section_from_file make_environ make_abs_section make_function_type make_blockvector make_pointer_type make_cleanup make_reference_type make_command make_symbol_completion_list (gdb) b make_ GDB把所有make开头的函数全部列出来给你查看。示例四：调试C++的程序时，有可以函数名一样。如： (gdb) b &apos;bubble( M-? bubble(double,double) bubble(int,int) (gdb) b &apos;bubble(你可以查看到C++中的所有的重载函数及参数。（注：M-?和“按两次TAB键”是一个意思） 要退出gdb时，只用发quit或命令简称q就行了 GDB中运行UNIX的shell程序在gdb环境中，你可以执行UNIX的shell的命令，使用gdb的shell命令来完成： shell &lt;command string=&quot;&quot;&gt;&lt;/command&gt; 调用UNIX的shell来执行&lt;command string=&quot;&quot;&gt;&lt;/command&gt;，环境变量SHELL中定义的UNIX的shell将会被用来执行&lt;command string=&quot;&quot;&gt;&lt;/command&gt;，如果SHELL没有定义，那就使用UNIX的标准shell：/bin/sh。（在Windows中使用Command.com或cmd.exe） 还有一个gdb命令是make： make &lt;make-args&gt; 可以在gdb中执行make命令来重新build自己的程序。这个命令等价于shell make &lt;make-args&gt;。 在GDB中运行程序当以gdb &lt;program&gt;方式启动gdb后，gdb会在PATH路径和当前目录中搜索&lt;program&gt;的源文件。如要确认gdb是否读到源文件，可使用l或list命令，看看gdb是否能列出源代码。 在gdb中，运行程序使用r或是run命令。程序的运行，你有可能需要设置下面四方面的事。 1、程序运行参数。 set args 可指定运行时参数。（如：set args 10 20 30 40 50） show args 命令可以查看设置好的运行参数。 2、运行环境。 `path 可设定程序的运行路径。 show paths 查看程序的运行路径。 set environment varname [=value] 设置环境变量。如：set env USER=hchen show environment [varname] 查看环境变量。 **3、工作目录。**cd ` 相当于shell的cd命令。 pwd 显示当前的所在目录。 4、程序的输入输出。 info terminal 显示你程序用到的终端的模式。 使用重定向控制程序输出。如：run &gt; outfile tty命令可以指写输入输出的终端设备。如：tty /dev/ttyb 调试已运行的程序两种方法： 1. 在UNIX下用ps查看正在运行的程序的PID（进程ID），然后用gdb &lt;program&gt; PID格式挂接正在运行的程序。 2. 先用gdb &lt;program&gt;关联上源代码，并进行gdb，在gdb中用attach命令来挂接进程的PID。并用detach来取消挂接的进程。 暂停/恢复程序运行调试程序中，暂停程序运行是必须的，GDB可以方便地暂停程序的运行。你可以设置程序的在哪行停住，在什么条件下停住，在收到什么信号时停往等等。以便于你查看运行时的变量，以及运行时的流程。 当进程被gdb停住时，你可以使用info program 来查看程序的是否在运行，进程号，被暂停的原因。 在gdb中，我们可以有以下几种暂停方式：断点（BreakPoint）、观察点（Watch Point）、捕捉点（Catch Point）、信号（Signals）、线程停止（Thread Stops）。如果要恢复程序运行，可以使用c或是 continue命令。 下面为重要的使用步骤, 只摘抄了部分必要的信息, 如设置断点, 查看栈信息, 其余操作, 可以在wiki.ubuntu查看 设置断点（Break Points） 我们用break命令来设置断点。下面有几点设置断点的方法： break &lt;function&gt; 在进入指定函数时停住。C++中可以使用class::function或function(type,type)格式来指定函数名。 break &lt;linenum&gt; 在指定行号停住。 break +offset break -offset 在当前行号的前面或后面的offset行停住。offiset为自然数。 break filename：linenum 在源文件filename的linenum行处停住。 break filename：function 在源文件filename的function函数的入口处停住。 break *address 在程序运行的内存地址处停住。 break break命令没有参数时，表示在下一条指令处停住。 break ... if &lt;condition&gt; …可以是上述的参数，condition表示条件，在条件成立时停住。比如在循环体中，可以设置break if i==100，表示当i为100时停住程序。 查看断点时，可使用info命令，如下所示：（注：n表示断点号） info breakpoints [n] info break [n] 维护停止点 上面说了如何设置程序的停止点，GDB中的停止点也就是上述的三类。在GDB中，如果你觉得已定义好的停止点没有用了，你可以使用delete、clear、disable、enable这几个命令来进行维护。 clear 清除所有的已定义的停止点。 clear &lt;function&gt; clear &lt;filename：function&gt; 清除所有设置在函数上的停止点。 clear &lt;linenum&gt; clear &lt;filename：linenum&gt; 清除所有设置在指定行上的停止点。 delete [breakpoints] [range...] 删除指定的断点，breakpoints为断点号。如果不指定断点号，则表示删除所有的断点。range 表示断点号的范围（如：3-7）。其简写命令为d。&lt;/filename：linenum&gt;&lt;/filename：function&gt; 比删除更好的一种方法是disable停止点，disable了的停止点，GDB不会删除，当你还需要时，enable即可，就好像回收站一样。 disable [breakpoints] [range...] disable所指定的停止点，breakpoints为停止点号。如果什么都不指定，表示disable所有的停止点。简写命令是dis. enable [breakpoints] [range...] enable所指定的停止点，breakpoints为停止点号。 enable [breakpoints] once range... enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动disable。 enable [breakpoints] delete range... enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动删除。 恢复程序运行和单步调试 当程序被停住了，你可以用continue命令恢复程序的运行直到程序结束，或下一个断点到来。也可以使用step或next命令单步跟踪程序。 continue [ignore-count] c [ignore-count] fg [ignore-count] 恢复程序运行，直到程序结束，或是下一个断点到来。ignore-count表示忽略其后的断点次数。continue，c，fg三个命令都是一样的意思。 step &lt;count&gt; 单步跟踪，如果有函数调用，他会进入该函数。进入函数的前提是，此函数被编译有debug信息。很像VC等工具中的step in。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。 next &lt;count&gt; 同样单步跟踪，如果有函数调用，他不会进入该函数。很像VC等工具中的step over。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。 set step-mode set step-mode on 打开step-mode模式，于是，在进行单步跟踪时，程序不会因为没有debug信息而不停住。这个参数很有利于查看机器码。 set step-mode off 关闭step-mode模式。 finish 运行程序，直到当前函数完成返回。并打印函数返回时的堆栈地址和返回值及参数值等信息。 until 或 u 当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。 stepi 或 si nexti 或 ni 单步跟踪一条机器指令！一条程序代码有可能由数条机器指令完成，stepi和nexti可以单步执行机器指令。与之一样有相同功能的命令是“display/i $pc” ，当运行完这个命令后，单步跟踪会在打出程序代码的同时打出机器指令（也就是汇编代码） 查看栈信息 当程序被停住了，你需要做的第一件事就是查看程序是在哪里停住的。当你的程序调用了一个函数，函数的地址，函数参数，函数内的局部变量都会被压入“栈”（Stack）中。你可以用GDB命令来查看当前的栈中的信息。 下面是一些查看函数调用栈信息的GDB命令： backtrace bt 打印当前的函数调用栈的所有信息。如： (gdb) bt #0 func (n=250) at tst.c:6 #1 0x08048524 in main (argc=1, argv=0xbffff674) at tst.c:30 #2 0x400409ed in __libc_start_main () from /lib/libc.so.6从上可以看出函数的调用栈信息：__libc_start_main --&amp;gt; main() --&amp;gt; func() backtrace &lt;n&gt; bt &lt;n&gt; n是一个正整数，表示只打印栈顶上n层的栈信息。 backtrace &amp;lt;-n&amp;gt; bt &amp;lt;-n&amp;gt; -n表一个负整数，表示只打印栈底下n层的栈信息。 如果你要查看某一层的信息，你需要切换当前栈，一般来说，程序停止时，最顶层的栈就是当前栈，如果你要查看栈下面层的详细信息，首先要做的是切换当前栈。 frame &lt;n&gt; f &lt;n&gt; n是一个从0开始的整数，是栈中的层编号。比如：frame 0，表示栈顶，frame 1，表示栈的第二层。 up &lt;n&gt; 表示向栈的上面移动n层，可以不打n，表示向上移动一层。 down &lt;n&gt; 表示向栈的下面移动n层，可以不打n，表示向下移动一层。 上面的命令，都会打印出移动到的栈层的信息。如果你不想让其打出信息。你可以使用这三个命令： select-frame &lt;n&gt; 对应于 frame 命令。 up-silently &lt;n&gt;对应于 up 命令。 down-silently &lt;n&gt; 对应于 down 命令。 查看当前栈层的信息，你可以用以下GDB命令： frame 或 f 会打印出这些信息：栈的层编号，当前的函数名，函数参数值，函数所在文件及行号，函数执行到的语句。 info frame info f 这个命令会打印出更为详细的当前栈层的信息，只不过，大多数都是运行时的内存地址。比如：函数地址，调用函数的地址，被调用函数的地址，目前的函数是由什么样的程序语言写成的、函数参数地址及值、局部变量的地址等等。如： bash (gdb) info f Stack level 0, frame at 0xbffff5d4: eip = 0x804845d in func (tst.c:6); saved eip 0x8048524 called by frame at 0xbffff60c source language c. Arglist at 0xbffff5d4, args: n=250 Locals at 0xbffff5d4, Previous frame&#39;s sp is 0x0 Saved registers: ebp at 0xbffff5d4, eip at 0xbffff5d8 info args 打印出当前函数的参数名及其值。 info locals 打印出当前函数中所有局部变量及其值。 info catch 打印出当前的函数中的异常处理信息。]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译器工作过程]]></title>
    <url>%2F49fab9fa%2F</url>
    <content type="text"><![CDATA[代码要运行，必须先转成二进制的机器码。这是编译器的任务。 比如，下面这段源码（假定文件名叫做test.c）。 #include &lt;stdio.h&gt; int main(void) { fputs(&quot;Hello, world!\n&quot;, stdout); return 0; } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980 要先用编译器处理一下，才能运行。 ```bash $ gcc test.c $ ./a.out Hello, world! 对于复杂的项目，编译过程还必须分成三步。 $ ./configure $ make $ make install 这些命令到底在干什么？大多数的书籍和资料，都语焉不详，只说这样就可以编译了，没有进一步的解释。本文将介绍编译器的工作过程，也就是上面这三个命令各自的任务。我主要参考了Alex Smith的文章《Building C Projects》。需要声明的是，本文主要针对gcc编译器，也就是针对C和C++，不一定适用于其他语言的编译。![这里写图片描述](http://img.blog.csdn.net/20170105233325494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbHhid29sZg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)#### 第一步 配置（configure）编译器在开始工作之前，需要知道当前的系统环境，比如标准库在哪里、软件的安装位置在哪里、需要安装哪些组件等等。这是因为不同计算机的系统环境不一样，通过指定编译参数，编译器就可以灵活适应环境，编译出各种环境都能运行的机器码。这个确定编译参数的步骤，就叫做”配置”（configure）。这些配置信息保存在一个配置文件之中，约定俗成是一个叫做configure的脚本文件。通常它是由autoconf工具生成的。编译器通过运行这个脚本，获知编译参数。configure脚本已经尽量考虑到不同系统的差异，并且对各种编译参数给出了默认值。如果用户的系统环境比较特别，或者有一些特定的需求，就需要手动向configure脚本提供编译参数。`$ ./configure --prefix=/www --with-mysql`上面代码是php源码的一种编译配置，用户指定安装后的文件保存在www目录，并且编译时加入mysql模块的支持。#### 第二步 确定标准库和头文件的位置源码肯定会用到标准库函数（standard library）和头文件（header）。它们可以存放在系统的任意目录中，编译器实际上没办法自动检测它们的位置，只有通过配置文件才能知道。编译的第二步，就是从配置文件中知道标准库和头文件的位置。一般来说，配置文件会给出一个清单，列出几个具体的目录。等到编译时，编译器就按顺序到这几个目录中，寻找目标。#### 第三步 确定依赖关系对于大型项目来说，源码文件之间往往存在依赖关系，编译器需要确定编译的先后顺序。假定A文件依赖于B文件，编译器应该保证做到下面两点。（1）只有在B文件编译完成后，才开始编译A文件。 （2）当B文件发生变化时，A文件会被重新编译。编译顺序保存在一个叫做makefile的文件中，里面列出哪个文件先编译，哪个文件后编译。而makefile文件由configure脚本运行生成，这就是为什么编译时configure必须首先运行的原因。在确定依赖关系的同时，编译器也确定了，编译时会用到哪些头文件。#### 第四步 头文件的预编译（precompilation）不同的源码文件，可能引用同一个头文件（比如stdio.h）。编译的时候，头文件也必须一起编译。为了节省时间，编译器会在编译源码之前，先编译头文件。这保证了头文件只需编译一次，不必每次用到的时候，都重新编译了。不过，并不是头文件的所有内容，都会被预编译。用来声明宏的#define命令，就不会被预编译。#### 第五步 预处理（Preprocessing）预编译完成后，编译器就开始替换掉源码中bash的头文件和宏。以本文开头的那段源码为例，它包含头文件stdio.h，替换后的样子如下。 extern int fputs(const char *, FILE *); extern FILE *stdout; int main(void) &#123; fputs(&quot;Hello, world!\n&quot;, stdout); return 0; &#125; 为了便于阅读，上面代码只截取了头文件中与源码相关的那部分，即fputs和FILE的声明，省略了stdio.h的其他部分（因为它们非常长）。另外，上面代码的头文件没有经过预编译，而实际上，插入源码的是预编译后的结果。编译器在这一步还会移除注释。这一步称为”预处理”（Preprocessing），因为完成之后，就要开始真正的处理了。#### 第六步 编译（Compilation）预处理之后，编译器就开始生成机器码。对于某些编译器来说，还存在一个中间步骤，会先把源码转为汇编码（assembly），然后再把汇编码转为机器码。下面是本文开头的那段源码转成的汇编码。```` .file &quot;test.c&quot; .section .rodata .LC0: .string &quot;Hello, world!\\n&quot; .text .globl main .type main, @function main: .LFB0: .cfi\_startproc pushq %rbp .cfi\_def\_cfa\_offset 16 .cfi\_offset 6, -16 movq %rsp, %rbp .cfi\_def\_cfa\_register 6 movq stdout(%rip), %rax movq %rax, %rcx movl $14, %edx movl $1, %esi movl $.LC0, %edi call fwrite movl $0, %eax popq %rbp .cfi\_def\_cfa 7, 8 ret .cfi_endproc .LFE0: .size main, .-main .ident &quot;GCC: (Debian 4.9.1-19) 4.9.1&quot; .section .note.GNU-stack,&quot;&quot;,@progbits这种转码后的文件称为对象文件（object file）。 第七步 连接（Linking）对象文件还不能运行，必须进一步转成可执行文件。如果你仔细看上一步的转码结果，会发现其中引用了stdout函数和fwrite函数。也就是说，程序要正常运行，除了上面的代码以外，还必须有stdout和fwrite这两个函数的代码，它们是由C语言的标准库提供的。 编译器的下一步工作，就是把外部函数的代码（通常是后缀名为.lib和.a的文件），添加到可执行文件中。这就叫做连接（linking）。这种通过拷贝，将外部函数库添加到可执行文件的方式，叫做静态连接（static linking），后文会提到还有动态连接（dynamic linking）。 make命令的作用，就是从第四步头文件预编译开始，一直到做完这一步。 第八步 安装（Installation）上一步的连接是在内存中进行的，即编译器在内存中生成了可执行文件。下一步，必须将可执行文件保存到用户事先指定的安装目录。 表面上，这一步很简单，就是将可执行文件（连带相关的数据文件）拷贝过去就行了。但是实际上，这一步还必须完成创建目录、保存文件、设置权限等步骤。这整个的保存过程就称为”安装”（Installation）。 第九步 操作系统连接可执行文件安装后，必须以某种方式通知操作系统，让其知道可以使用这个程序了。比如，我们安装了一个文本阅读程序，往往希望双击txt文件，该程序就会自动运行。 这就要求在操作系统中，登记这个程序的元数据：文件名、文件描述、关联后缀名等等。Linux系统中，这些信息通常保存在/usr/share/applications目录下的.desktop文件中。另外，在Windows操作系统中，还需要在Start启动菜单中，建立一个快捷方式。 这些事情就叫做”操作系统连接”。make install命令，就用来完成”安装”和”操作系统连接”这两步。 第十步 生成安装包写到这里，源码编译的整个过程就基本完成了。但是只有很少一部分用户，愿意耐着性子，从头到尾做一遍这个过程。事实上，如果你只有源码可以交给用户，他们会认定你是一个不友好的家伙。大部分用户要的是一个二进制的可执行程序，立刻就能运行。这就要求开发者，将上一步生成的可执行文件，做成可以分发的安装包。 所以，编译器还必须有生成安装包的功能。通常是将可执行文件（连带相关的数据文件），以某种目录结构，保存成压缩文件包，交给用户。 第十一步 动态连接（Dynamic linking）正常情况下，到这一步，程序已经可以运行了。至于运行期间（runtime）发生的事情，与编译器一概无关。但是，开发者可以在编译阶段选择可执行文件连接外部函数库的方式，到底是静态连接（编译时连接），还是动态连接（运行时连接）。所以，最后还要提一下，什么叫做动态连接。 前面已经说过，静态连接就是把外部函数库，拷贝到可执行文件中。这样做的好处是，适用范围比较广，不用担心用户机器缺少某个库文件；缺点是安装包会比较大，而且多个应用程序之间，无法共享库文件。动态连接的做法正好相反，外部函数库不进入安装包，只在运行时动态引用。好处是安装包会比较小，多个应用程序可以共享库文件；缺点是用户必须事先安装好库文件，而且版本和安装位置都必须符合要求，否则就不能正常运行。 现实中，大部分软件采用动态连接，共享库文件。这种动态共享的库文件，Linux平台是后缀名为.so的文件，Windows平台是.dll文件，Mac平台是.dylib文件。]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Unordered_map笔记]]></title>
    <url>%2Fd97ca7%2F</url>
    <content type="text"><![CDATA[unordered_map与map的区别 boost::unordered_map， 它与 stl::map的区别就是，stl::map是按照operator&lt;比较判断元素是否相同，以及比较元素的大小，然后选择合适的位置插入到树中。所以，如果对map进行遍历（中序遍历）的话，输出的结果是有序的。顺序就是按照operator&lt; 定义的大小排序。 而boost::unordered_map是计算元素的Hash值，根据Hash值判断元素是否相同。所以，对unordered_map进行遍历，结果是无序的。 用法的区别就是，stl::map 的key需要定义operator&lt; 。 而boost::unordered_map需要定义hash_value函数并且重载operator==。对于内置类型，如string，这些都不用操心。对于自定义的类型做key，就需要自己重载operator== 或者hash_value()了。 最后，说，当不需要结果排好序时，最好用unordered_map。 linux下使用 普通的key就不说了和map一样 看一下用sockaddr_in 作为key的方法 #ifndef CSESSION_H #define CSESSION_H #include &lt;netinet in.h=&quot;&quot;&gt; #include &lt;time.h&gt; #include &lt;/time.h&gt;&lt;/netinet&gt; &lt;map&gt; #include &lt;string.h&gt; #include &lt;tr1 unordered_map=&quot;&quot;&gt; //头文件 #include &lt;iostream&gt; using namespace std; using namespace std::tr1; struct Terminal { int nid ; //id the key for terminal sockaddr_in addr; //ip the key for Client time_t tm; //last alive time enTerminalStat enStat;//status Terminal(); ~Terminal(); Terminal &amp;amp;operator =(const Terminal&amp;amp; term); }; struct hash_func //hash 函数 { size_t operator()(const sockaddr_in &amp;amp;addr) const { return addr.sin_port*9999 + addr.sin_addr.s_addr; } }; struct cmp_fun //比较函数 == { bool operator()(const sockaddr_in &amp;amp;addr1, const sockaddr_in &amp;amp;addr2) const { return memcmp(&amp;amp;addr1, &amp;amp;addr2, sizeof(sockaddr_in)) == 0 ? true:false; } }; //typedef unordered_map&lt;int,terminal*&gt; MapTerminal; // Terminal socket 作为key //typedef unordered_map&lt;int,terminal*&gt;::iterator MapTerminal_It; // &lt;/int,terminal*&gt;&lt;/int,terminal*&gt; typedef unordered_map&lt;sockaddr_in, terminal*,hash_func,=&quot;&quot; cmp_fun=&quot;&quot;&gt; MapClientSession; // sockaddr_in作为key typedef unordered_map&lt;sockaddr_in, terminal*,hash_func,=&quot;&quot; cmp_fun=&quot;&quot;&gt;::iterator MapClientSession_It; // &lt;/sockaddr_in,&gt;&lt;/sockaddr_in,&gt; #endif // CSESSION_Hoperator==有两种方式 一种是 struct st { bool operator==(const st &amp;amp;s) const ... }；另一种就是自定义函数体，代码中 struct cmp_fun { bool operator()(...) ... }必须要自定义operator==和hash_value。 重载operator==是因为，如果两个元素的hash_value的值相同，并不能断定这两个元素就相同，必须再调用operator==。 当然，如果hash_value的值不同，就不需要调用operator==了。 &lt;/string.h&gt;]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Unordered_set笔记]]></title>
    <url>%2F14decfad%2F</url>
    <content type="text"><![CDATA[http://www.cplusplus.com/reference/unordered\_set/unordered\_set/ unordered_set 模板原型: [cpp] template &amp;lt; class Key, class Hash = hash&lt;key&gt;, class Pred = equal_to&lt;key&gt;, class Alloc = allocator&lt;key&gt; &amp;gt; class unordered_set; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 当比较unordered_set中某两个元素时, 先调用`hash&lt;key&gt;`, 如果`hash&lt;key&gt;` 不相等, 说明两个元素不同, 如果`hash&lt;key&gt;` 值相等, 则调用`equal_to&lt;key&gt;`, 判断两个元素是否完全相等. (Hash函数和Compare函数都可以自定义)&lt;/key&gt;&lt;/key&gt;&lt;/key&gt;&lt;/key&gt; C++ 11中对unordered_set描述大体如下：无序集合容器（unordered_set）是一个存储唯一(unique，即无重复）的关联容器（Associative container），容器中的元素无特别的秩序关系，该容器允许基于值的快速元素检索，同时也支持正向迭代。 在一个unordered_set内部，元素不会按任何顺序排序，而是通过元素值的hash值将元素分组放置到各个槽(Bucker，也可以译为“桶”），这样就能通过元素值快速访问各个对应的元素（均摊耗时为O（1））。 原型中的Key代表要存储的类型，而hash&lt;key&gt;也就是你的hash函数，equal_to&lt;key&gt;用来判断两个元素是否相等，allocator&lt;key&gt;是内存的分配策略。一般情况下，我们只关心hash&lt;key&gt;和equal_to&lt;key&gt;参数，下面将介绍这两部分。&lt;/key&gt;&lt;/key&gt;&lt;/key&gt;&lt;/key&gt;&lt;/key&gt; **`hash&lt;key&gt;`** ` hash&lt;key&gt;`通过相应的hash函数，将传入的参数转换为一个size_t类型值，然后用该值对当前hashtable的bucket取模算得其对应的hash值。而C++标准库，为我们提供了基本数据类型的hash函数：&lt;/key&gt;&lt;/key&gt; \[cpp\] /// Primary class template hash. template struct hash;/// Partial specializations for pointer types. template struct hash&lt;\_Tp*&gt; : public \_\_hash\_base&lt;size\_t, \_tp*=&quot;&quot;&gt; &#123; size\_t operator()(\_Tp* \_\_p) const noexcept &#123; return reinterpret_cast(__p); &#125; &#125;; &lt;/size_t,&gt;// Explicit specializations for integer types.define \_Cxx\_hashtable\_define\_trivial\_hash(\_Tp) \======================================================template&lt;&gt; \ struct hash&lt;\_Tp&gt; : public \_\_hash\_base&lt;size\_t, \_tp=&quot;&quot;&gt; \ &#123; \ size\_t \ operator()(\_Tp \_\_val) const noexcept \ &#123; return static_cast(__val); &#125; \ &#125;; &lt;/size_t,&gt;/// Explicit specialization for bool. \_Cxx\_hashtable\_define\_trivial_hash(bool)/// Explicit specialization for char. \_Cxx\_hashtable\_define\_trivial_hash(char)/// Explicit specialization for signed char. \_Cxx\_hashtable\_define\_trivial_hash(signed char)/// Explicit specialization for unsigned char. \_Cxx\_hashtable\_define\_trivial_hash(unsigned char)/// Explicit specialization for wchar\_t. \_Cxx\_hashtable\_define\_trivial\_hash(wchar_t)/// Explicit specialization for char16\_t. \_Cxx\_hashtable\_define\_trivial\_hash(char16_t)/// Explicit specialization for char32\_t. \_Cxx\_hashtable\_define\_trivial\_hash(char32_t)/// Explicit specialization for short. \_Cxx\_hashtable\_define\_trivial_hash(short)/// Explicit specialization for int. \_Cxx\_hashtable\_define\_trivial_hash(int)/// Explicit specialization for long. \_Cxx\_hashtable\_define\_trivial_hash(long)/// Explicit specialization for long long. \_Cxx\_hashtable\_define\_trivial_hash(long long)/// Explicit specialization for unsigned short. \_Cxx\_hashtable\_define\_trivial_hash(unsigned short)/// Explicit specialization for unsigned int. \_Cxx\_hashtable\_define\_trivial_hash(unsigned int)/// Explicit specialization for unsigned long. \_Cxx\_hashtable\_define\_trivial_hash(unsigned long)/// Explicit specialization for unsigned long long. \_Cxx\_hashtable\_define\_trivial_hash(unsigned long long) 对于指针类型，标准库只是单一将地址转换为一个size_t值作为hash值，这里特别需要注意的是`char *`类型的指针，其标准库提供的hash函数只是将指针所指地址转换为一个sieze_t值，如果，你需要用`char *`所指的内容做hash，那么，你需要自己写hash函数或者调用系统提供的`hash&lt;string&gt;`。 标准库为string类型对象提供了一个hash函数，即：Murmur hash，。对于float、double、long double标准库也有相应的hash函数，这里，不做过多的解释，相应的可以参看functional_hash.h头文件。 上述只是介绍了基本数据类型，而在实际应用中，有时，我们需要使用自己写的hash函数，那怎么自定义hash函数？参考标准库基本数据类型的hash函数，我们会发现这些hash函数有个共同的特点：通过定义函数对象，实现相应的hash函数，这也就意味我们可以通过自定义相应的函数对象，来实现自定义hash函数。比如：已知平面上有N，每个点的x轴、y轴范围为[0，100]，现在需要统计有多少个不同点？hash函数设计为：将每个点的x、y值看成是101进制，如下所示:&lt;/string&gt; \[cpp\]include&lt;bits\\stdc++.h&gt;=======================using namespace std; struct myHash &#123; size\_t operator()(pair&lt;int, int=&quot;&quot;&gt; \_\_val) const &#123; return static_cast(\_\_val.first * 101 + \_\_val.second); &#125; &#125;; int main() &#123; unordered\_set&lt;pair&lt;int, int=&quot;&quot;&gt;, myHash&gt; S; int x, y; while (cin &gt;&gt; x &gt;&gt; y) S.insert(make\_pair(x, y)); for (auto it = S.begin(); it != S.end(); ++it) cout &lt;&lt; it-&gt;first &lt;&lt; &quot; &quot; &lt;&lt; it-&gt;second &lt;&lt; endl; return 0; &#125; **`equal_to&lt;key&gt;`** 该参数用于实现比较两个关键字是否相等，至于为什么需要这个参数？这里做点解释，前面我们说过，当不同关键字，通过hash函数，可能会得到相同的关键字值，每当我们在unordered_set里面做数据插入、删除时，由于unordered_set关键字唯一性，所以我们得确保唯一性。标准库定义了基本类型的比较函数，而对于自定义的数据类型，我们需要自定义比较函数。这里有两种方法:重载==操作符和使用函数对象，下面是STL中实现`equal_to&lt;key&gt;`的源代码：&lt;/key&gt;&lt;/key&gt;&lt;/pair&lt;int,&gt;&lt;/size_t&gt;&lt;/int,&gt;&lt;/bits\stdc++.h&gt; \[cpp\] template struct unary\_function &#123; /// @c argument\_type is the type of the argument typedef \_Arg argument\_type;/// @c result\_type is the return type typedef \_Result result_type; &#125;; template struct equal\_to : public binary\_function&lt;\_Tp, \_Tp, bool&gt; &#123; bool operator()(const \_Tp&amp; \_\_x, const \_Tp&amp; \_\_y) const &#123; return \_\_x == \_\_y; &#125; &#125;;扩容与缩容 在vector中，每当我们插入一个新元素时，如果当前的容量（capacity)已不足，需要向系统申请一个更大的空间，然后将原始数据拷贝到新空间中。这种现象在unordered_set中也存在，比如当前的表长为100，而真实存在表中的数据已经大于1000个元素，此时，每个bucker均摊有10个元素，这样就会影响到unordered_set的存取效率，而标准库通过采用某种策略来对当前空间进行扩容，以此来提高存取效率。当然，这里也存在缩容，原理和扩容类似，不过，需要注意的是，每当unordered_set内部进行一次扩容或者缩容，都需要对表中的数据重新计算，也就是说，扩容或者缩容的时间复杂度至少为。 code： [cpp] // unordered_set::find #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;unordered_set&gt; &lt;/unordered_set&gt;&lt;/string&gt;&lt;/iostream&gt; int main () { std::unordered_set&lt;std::string&gt; myset = { &quot;red&quot;,&quot;green&quot;,&quot;blue&quot; }; &lt;/std::string&gt; std::string input; std::cout &amp;lt;&amp;lt; &quot;color? &quot;; getline (std::cin,input); std::unordered_set&lt;std::string&gt;::const_iterator got = myset.find (input); &lt;/std::string&gt; if ( got == myset.end() ) std::cout &amp;lt;&amp;lt; &quot;not found in myset&quot;; else std::cout &amp;lt;&amp;lt; *got &amp;lt;&amp;lt; &quot; is in myset&quot;; std::cout &amp;lt;&amp;lt; std::endl; return 0; }]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++代码优化建议]]></title>
    <url>%2F8e72ff9a%2F</url>
    <content type="text"><![CDATA[记住阿姆达尔定律： Ahmdal’s rule $$Speedup =\dfrac{ time_{old}}{time_{new}} = \dfrac{1}{(1-func_{cost})+func_{cost}/func_{speedup}}$$ func_cost是函数func运行时间百分比，func_speedup是你优化函数的运行的系数。 所以，如果你优化了函数TriangleIntersect执行40%的运行时间，使它运行快了近两倍，而你的程序会运行快25%。 这意味着不经常使用的代码不需要做较多优化考虑（或者完全不优化）。 这里有句俗语：让经常执行的路径运行更加高效，而运行稀少的路径正确运行。 代码先保证正确，然后再考虑优化 这并不意味着用8周时间写一个全功能的射线追踪算法，然后用8周时间去优化它。 分多步来做性能优化。 先写正确的代码，当你意识到这个函数可能会被经常调用，进行明显的优化。 然后再寻找算法的瓶颈，并解决（通过优化或者改进算法）。通常，改进算法能显著地改进瓶颈——也许是采用一个你还没有预想到的方法。所有频繁调用的函数，都需要优化。 我所了解的那些写出非常高效代码的人说，他们优化代码的时间，是写代码时间的两倍。 跳转和分支执行代价高，如果可能，尽量少用。 函数调用需要两次跳转，外加栈内存操作。 优先使用迭代而不是递归。 使用内联函数处理短小的函数来消除函数调用开销。 将循环内的函数调用移动到循环外(例如，将for(i=0;i&lt;100;i++) DoSomething();改为DoSomething() { for(i=0;i&lt;100;i++) { … }})。 if…else if…else if…else if…很长的分支链执行到最后的分支需要很多的跳转。如果可能，将其转换为一个switch声明语句，编译器有时候会将其转换为一个表查询单次跳转。如果switch声明不可行，将最常见的场景放在if分支链的最前面。 5. 仔细思考函数下标的顺序。 两阶或更高阶的数组在内存中还是以一维的方式在存储在内存中，这意味着（对于C/C++数组）array[i][j] 和 array[i][j+1]是相邻的，但是array[i][j] 和array[i+1][j]可能相距很远。以适当的方式访问存储实际内存中的数据，可以显著地提升你代码的执行效率（有时候可以提升一个数量级甚至更多）。 现代处理器从主内存中加载数据到处理器cache，会加载比单个值更多的数据。该操作会获取请求数据和相邻数据（一个cache行大小）的整块数据。这意味着，一旦array[i][j]已经在处理器cache中，array[i][j+1]很大可能也已经在cache中了，而array[i+1][j]可能还在内存中。 6. 使用指令层的并行机制 尽管许多程序还是依赖单线程的执行，现代处理器在单核中也提供了不少的并行性。例如：单个CPU可以同时执行4个浮点数乘，等待4个内存请求并执行一个分支预判。为了最大化利用这种并行性，代码块（在跳转之间的）需要足够的独立指令来允许处理器被充分利用。 考虑展开循环来改进这一点。 这也是使用内联函数的一个好理由。 7. 避免或减少使用本地变量。 本地变量通常都存储在栈上。不过如果数量比较少，它们可以存储在CPU寄存器中。在这种情况下，函数不但得到了更快访问存储在寄存器中的数据的好处，也避免了初始化一个栈帧的开销。不要将大量数据转换为全局变量。 8. 减少函数参数的个数。 和减少使用本地变量的理由一样——它们也是存放在栈上。9. 通过引用传递结构体而不是传值 我在射线追踪中还找不到一个场景需要将结构体使用传值方式（包括一些简单结构如：Vector，Point和Color）。10. 如果你的函数不需要返回值，不要定义。 尽量避免数据转换。 整数和浮点数指令通常操作不同的寄存器，所以转换需要进行一次拷贝操作。 短整型（char和short）仍然使用一整个寄存器，并且它们需要被填充为32/64位，然后在存储回内存时需要再次转换为小字节（不过，这个开销一定比一个更大的数据类型的内存开销要多一点）。 定义C++对象时需要注意。 使用类初始化而不是使用赋值（Color c(black); 比Color c; c = black;更快） 使类构造函数尽可能轻量。 尤其是常用的简单类型（比如，color，vector，point等等），这些类经常被复制。 这些默认构造函数通常都是在隐式执行的，这或许不是你所期望的。 使用类初始化列表(Use Color::Color() : r(0), g(0), b(0) {}，而不是初始化函数Color::Color() { r= g = b = 0; } .) 如果可以的话，使用位移操作&gt;&gt;和&lt;&lt;来代替整数乘除法 小心使用表查找函数 许多人都鼓励将复杂的函数（比如：三角函数）转化为使用预编译的查找表。对于射线追踪功能来说，这通常导致了不必要的内存查找，这很昂贵（并不断增长），并且这和计算一个三角函数并从内存中获取值一样快（尤其你考虑到三角查找打乱了cpu的cache存取）。 在其他情况下，查找表会很有用。对于GPU编程通常优先使用表查找而不是复杂函数。 对大多数类，优先使用+= 、 -= 、 *= 和 /=，而不是使用+ 、 – 、 * 、 和?/ 这些简单操作需要创建一个匿名临时中间变量。 例如：Vector v = Vector(1,0,0) + Vector(0,1,0) + Vector(0,0,1);?创建了五个匿名临时Vector: Vector(1,0,0), Vector(0,1,0), Vector(0,0,1), Vector(1,0,0) + Vector(0,1,0), 和 Vector(1,0,0) + Vector(0,1,0) + Vector(0,0,1). 对上述代码进行简单转换：Vector v(1,0,0); v+= Vector(0,1,0); v+= Vector(0,0,1);仅仅创建了两个临时Vector: Vector(0,1,0) 和 Vector(0,0,1)。这节约了6次函数调用（3次构造函数和3次析构函数）。 对于基本数据类型，优先使用+?、?-?、??、?和?/，而不是+=?、?-=?、?= 和 /= 推迟定义本地变量 定义一个对象变量通常需要调用一次函数（构造函数）。 如果一个变量只在某些情况下需要（例如在一个if声明语句内），仅在其需要的时候定义，这样，构造函数仅在其被使用的时候调用。 对于对象，使用前缀操作符（++obj），而不是后缀操作符（obj++） 这在你的射线追踪算法中可能不是一个问题 使用后缀操作符需要执行一次对象拷贝（这也导致了额外的构造和析构函数调用），而前缀的构造函数不需要一个临时的拷贝。 小心使用模板 对不同的是实例实现进行不同的优化。 标准模板库已经经过良好的优化，不过我建议你在实现一个交互式射线追踪算法时避免使用它。 使用自己的实现，你知道它如何使用算法，所以你知道如何最有效的实现它。 最重要的是，我的经历告诉我：调试STL库非常低效。通常这也不是一个问题，除非你使用debug版本做性能分析。你会发现STL的构造函数，迭代器和其他一些操作，占用了你15%的运行时间，这会导致你分析性能输出更加费劲。 避免在计算时进行动态内存分配 动态内存对于存储场景和运行期间其他数据都很有用。 但是，在许多（大多数）的系统动态内存分配需要获取控制访问分配器的锁。对于多线程应用程序，现实中使用动态内存由于额外的处理器导致了性能下降，因为需要等待分配器锁和释放内存。 即便对于单线程应用，在堆上分配内存也比在栈上分配内存开销大得多。操作系统还需要执行一些操作来计算并找到适合尺寸的内存块。 找到你系统内存cache的信息并利用它们 如果一个是数据结构正好适合一个cache行，处理整个类从内存中只需要做一次获取操作。 确保所有的数据结构都是cache行大小对齐（如果你的数据结构和一个cache行大小都是128字节，仍有可能因为你的结构体中的一个字节在一个cache行中，而其他127字节在另外一个cahce行中）。 避免不需要的数据初始化 如果你需要初始化一大段的内存，考虑使用memset。 尽早结束循环和尽早返回函数调用 考虑一个射线和三角形交叉，通常的情况是射线会越过三角，所以这里可以优化。 如果你决定将射线和三角面板交叉。如果射线和面板交叉t值是负数，你可以立即返回。这允许你跳过射线三角交叉一大半的质心坐标计算。这是一个大的节约，一旦你知道这个交叉不存在，你就应该立即返回交叉计算函数。 同样的，一些循环也应该尽早结束。例如，当设置阴影射线，对于近处的交叉通常都是不必须的，一旦有类似的的交叉，交叉计算就应该尽早返回。（这里的交叉含义不太明白，可能是专业词汇，译者注） 在稿纸上简化你的方程式 许多方程式中，通常都可以或者在某些条件中取消计算。 编译器不能发现这些简化，但是你可以。取消一个内部循环的一些昂贵操作可以抵消你在其他地方的好几天的优化工作。 整数、定点数、32位浮点数和64位双精度数字的数学运算差异，没有你想象的那么大 在现代CPU，浮点数运算和整数运算差不多拥有同样的效率。在计算密集型应用（比如射线追踪），这意味这可以忽略整数和浮点数计算的开销差异。这也就是说，你不必要对算数进行整数处理优化。 双精度浮点数运算也不比单精度浮点数运算更慢，尤其是在64位机器上。我在同一台机器测试射线追踪算法全部使用double比全部使用floats运行有时候更快，反过来测试也看到了一样的现象（这里的原文是：I have seen ray tracers run faster using all doubles than all floats on the same machine. I have also seen the reverse）。 不断改进你的数学计算，以消除昂贵的操作 sqrt()经常可以被优化掉，尤其是在比较两个值的平方根是否一致时。 如果你重复地需要处理 除x 操作，考虑计算1/x的值，乘以它。这在向量规范化（3次除法）运算中赢得了大的改进，不过我最近发现也有点难以确定的。不过，这仍然有所改进，如果你要进行三次或更多除法运算。 如果你在执行一个循环，那些在循环中执行不发生变化的部分，确保提取到循环外部。 考虑看看你的计算值是否可以在循环中修改得到（而不每次都重新开始循环计算）。]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[X == X]]></title>
    <url>%2F72da8a18%2F</url>
    <content type="text"><![CDATA[C的表达式 x == x，何时为假呢？即下面的代码： if (x == x) { printf(&quot;Equal\n&quot;); } else { printf(&quot;Not equal\n&quot;); }什么时候输出为”Not equal”呢？ #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;string.h&gt;&lt;/string.h&gt;&lt;/stdio.h&gt;&lt;/stdlib.h&gt; int main(void) { float x = 0xffffffff; if (x == x) { printf(&quot;Equal\n&quot;); } else { printf(&quot;Not equal\n&quot;); } if (x &amp;gt;= 0) { printf(&quot;x(%f) &amp;gt;= 0\n&quot;, x); } else if (x &amp;lt; 0) { printf(&quot;x(%f) &amp;lt; 0\n&quot;, x); } int a = 0xffffffff; memcpy(&amp;amp;x, &amp;amp;a, sizeof(x)); if (x == x) { printf(&quot;Equal\n&quot;); } else { printf(&quot;Not equal\n&quot;); } if (x &amp;gt;= 0) { printf(&quot;x(%f) &amp;gt;= 0\n&quot;, x); } else if (x &amp;lt; 0) { printf(&quot;x(%f) &amp;lt; 0\n&quot;, x); } else { printf(&quot;Surprise x(%f)!!!\n&quot;, x); } return 0; }编译gcc -g -Wall test.c，看执行结果： $ ./a.out Equal x(4294967296.000000) &amp;gt;= 0 Not equal Surprise x(-nan)!!!最后两行输出是不是有点surprise啊。 下面先简单解释一下： 1. 当float x = 0xffffffff：这时将整数赋给一个浮点数，由于float和int的size都是4，而浮点数的存储格式与整数不同，其需要将某些位作为小数位，所以float的范围要小于int的范围。因此这里涉及到了整数转换浮点的规定。因为这个转换其实很少用到，我也就不查了。但是总之，这个转换是合法的。但是最终的值很可能不是你想要的结果——尤其是当浮点的范围小于整数的范围时。 2. 即使整数转换成浮点，数值再不是期望值，但它也一定是一个合法的浮点数值。所以第一个x == x，一定为true，且x不是大于0，就是小于0。这时x存的并不是0xffffffff。 3. 当使用memcpy将0xff填充到x的地址时，这时x存的保证为0xffffffff。但是这个不是一个合法的float的值。因此奇怪的现象发生了，x并不等于x。原因则是与cpu的浮点指令相关. 4. 作为一个非法的float值，当它与其它任何数值比较时，都会返回false。这也就造成了，后面惊奇的结果，x既不大于等于0，也不小于0。 总结一下：一般来说，浮点类型很少被使用，也不应该在程序中鼓励使用。不仅其效率比整数低，且由于浮点类型特殊的存储格式，很容易造成一些意想不到的错误。如果真的无法避免时，一定要小心小心再小心。特别要注意今天的主题，这种非法的浮点值，会导致任何比较判断都失败。而判断这种浮点值的方法也很简单，如果x != x，那么该浮点即为非法浮点值。]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++在重载operator=为带模板的函数的时候的陷阱]]></title>
    <url>%2Fa25c4e07%2F</url>
    <content type="text"><![CDATA[原文地址 https://segmentfault.com/a/1190000004467381 最近被一个语法问题缠了半天，终于找到了原因。不仔细思考一下写的时候真的很容易忽略。先看代码： template class A { public: const T t = 0; template A&amp; operator=(const A&amp; a) { return *this; } }; int main() { A a, b; b = a; // error } 这会带来一个编译错误，然而横睇掂睇都看不出问题。于是我就试了一下这样的代码：A c; b = c;居然通过了编译。F**k，这个模板居然胳膊肘往外拐。 其实我在写这个代码的时候忽略了一点，就是default assignment operator，它是你在定义类的时候编译器默认给你加上去的，行为是对所有成员变量赋值。它的声明是A&amp; operator=(const A&amp; a);，跟我们自己定义的放在一起： template A&amp; operator=(const A&amp; a) { return *this; } A&amp; operator=(const A&amp; a) /= delete/; 恰好构成了模板特化，这就糟了。一旦构成了特化，OtherT可以匹配的类型就会除去int，用A赋值时只能调用系统给我们定义的那个。然而它也不起作用，因为成员里面有常量（这样它就会被标记为= delete，留意delete并不会令OtherT可以匹配到int，反而令它匹配不到）。 知道了原因之后，解决就很方便了，只要重新定义这个默认赋值运算符就好： A&amp; operator=(const A&amp; a) { /…/ }]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Makefile学习笔记]]></title>
    <url>%2F31dc5dc1%2F</url>
    <content type="text"><![CDATA[关于Makefile怎么写,参考http://blog.csdn.net/haoel/article/details/2886 一 关于编译和链接 一般来说，无论是C、C++、还是pas，首先要把源文件编译成中间代码文件，在Windows下是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做编译（compile）。然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常是你需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。 链接时，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。 总结一下，源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。你需要指定函数的Object File.二 Makefile的规则 三条: 1）如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。 2）如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。 3）如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。 target ... : prerequisites ... command ... ... target也就是一个目标文件，可以是Object File，也可以是执行文件,还可以是一个标签（Label）. prerequisites就是，要生成那个target所需要的东西(文件或是目标)。 command也就是make需要执行的命令。（任意的Shell命令） 这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在command中。说白一点就是说，prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。这就是Makefile的规则。也就是Makefile中最核心的内容。For example: edit : main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o main.o : main.c defs.h cc -c main.c kbd.o : kbd.c defs.h command.h cc -c kbd.c command.o : command.c defs.h command.h cc -c command.c display.o : display.c defs.h buffer.h cc -c display.c insert.o : insert.c defs.h buffer.h cc -c insert.c search.o : search.c defs.h buffer.h cc -c search.c files.o : files.c defs.h buffer.h command.h cc -c files.c utils.o : utils.c defs.h cc -c utils.c clean : rm edit main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o 在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个\[Tab\]键作为开头(在Makefile中的命令，必须要以\[Tab\]键开始)。make并不管命令是怎么工作的，它只管执行所定义的命令。make会比较targets文件和prerequisites文件的修改日期，如果prerequisites文件的日期要比targets文件的日期要新，或者target不存在的话，make就会执行后续定义的命令。 clean不是一个文件，它只不过是一个动作名字，有点像C语言中的lable一样，如果其冒号后什么也没有，那么make就不会自动去找文件的依赖性，也就不会自动执行其后所定义的命令。要执行其后的命令，就要在make命令后明显得指出这个lable的名字。这样的方法非常有用，我们可以在一个makefile中定义不用的编译或是和编译无关的命令，比如程序的打包，程序的备份，等等。三 Makefile里有什么 1 显示规则 2 隐晦规则 3 变量的定义 4 文件指示: 其包括了三个部分，一个是在一个Makefile中引用另一个Makefile，就像C语言中的include一样；另一个是指根据某些情况指定Makefile中的有效部分，就像C语言中的预编译#if一样；还有就是定义一个多行的命令。 5 注释: Makefile中只有行注释，和UNIX的Shell脚本一样，其注释是用“#”字符，这个就像C/C++中的“//”一样。如果你要在你的Makefile中使用“#”字符，可以用反斜框进行转义，如：“#”。 四 Makefile的文件名 默认情况下，make命令会在当前目录下按顺序找寻文件名为“GNUmakefile”、“makefile”、“Makefile”的文件，找到了就解释这个文件。在这三个文件名中，最好使用“Makefile”这个文件名，因为这个文件名第一个字符为大写，这样有一种显目的感觉。最好不要用“GNUmakefile”，这个文件是GNU的make识别的。有另外一些make只对全小写的“makefile”文件名敏感，但是基本上来说，大多数的make都支持“makefile”和“Makefile”这两种默认文件名。当然，也可以使用别的文件名来书写Makefile，比如：“Make.Linux”，“Make.Solaris”，“Make.AIX”等，如果要指定特定的Makefile，可以使用make的“-f”和“–file”参数，如：make -f Make.Linux或make –file Make.AIX。 五 引用其它的Makefile include的语法是： includefilename可以是当前操作系统Shell的文件模式（可以保含路径和通配符） 在include前面可以有一些空字符，但是绝不能是\[Tab\]键开始。include和可以用一个或多个空格隔开。例如,有这样几个Makefile：a.mk、b.mk、c.mk，还有一个文件叫foo.make，以及一个变量$(bar)，其包含了e.mk和f.mk，那么，下面的语句： include foo.make *.mk $(bar) 等价于： include foo.make a.mk b.mk c.mk e.mk f.mkmake命令开始时，会把找寻include所指出的其它Makefile，并把其内容安置在当前的位置。就好像C/C++的#include指令一样。如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，如果当前目录下没有找到，那么，make还会在下面的几个目录下找： 1、如果make执行时，有“-I”或“--include-dir”参数，那么make就会在这个参数所指定的目录下去寻找。 2、如果目录/include（一般是：/usr/local/bin或/usr/include）存在的话，make也会去找。如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成makefile的读取，make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。如果你想让make不理那些无法读取的文件，而继续执行，你可以在include前加一个减号“-”。如： -include 其表示，无论include过程中出现什么错误，都不要报错继续执行。和其它版本make兼容的相关命令是sinclude，其作用和这一个是一样的。六 环境变量 MAKEFILES 如果当前环境中定义了环境变量MAKEFILES，那么，make会把这个变量中的值做一个类似于include的动作。这个变量中的值是其它的Makefile，用空格分隔。只是，它和include不同的是，从这个环境变量中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。 但是在这里还是建议不要使用这个环境变量，因为只要这个变量一被定义，那么当使用make时，所有的Makefile都会受到它的影响，这绝不是想看到的。在这里提这个事，只是为了告诉大家，也许有时候你的Makefile出现了怪事，那么你可以看看当前环境中有没有定义这个变量。 当make嵌套调用时（参见前面的“嵌套调用”章节），上层Makefile中定义的变量会以系统环境变量的方式传递到下层的Makefile中。当然，默认情况下，只有通过命令行设置的变量会被传递。而定义在文件中的变量，如果要向下层Makefile传递，则需要使用exprot关键字来声明.七 关于命令 通常，make会把其要执行的命令行在命令执行前输出到屏幕上。当我们用“@”字符在命令行前，那么，这个命令将不被make显示出来，最具代表性的例子是，我们用这个功能来像屏幕显示一些信息。如： @echo 正在编译XXX模块......当make执行时，会输出“正在编译XXX模块……”字串，但不会输出命令，如果没有“@”，那么，make将输出： echo 正在编译XXX模块...... 正在编译XXX模块......如果make执行时，带入make参数“-n”或“–just-print”，那么其只是显示命令，但不会执行命令，这个功能很有利于调试Makefile，看看书写的命令执行起来是什么样子的或是什么顺序的,而make参数“-s”或“–slient”则是全面禁止命令的显示。 如果要让上一条命令的结果应用在下一条命令时，应该使用分号分隔这两条命令。比如第一条命令是cd，希望第二条命令在cd之后的基础上运行，那么就不能把这两条命令写在两行上，而应该把这两条命令写在一行上，用分号分隔。如： 示例一： exec: cd /home/hchen pwd 示例二： exec: cd /home/hchen; pwd当执行“make exec”时，第一个例子中的cd没有作用，pwd会打印出当前的Makefile目录，而第二个例子中，cd就起作用了，pwd会打印出“/home/hchen”。 每当命令运行完后，make会检测每个命令的返回码，如果命令返回成功，那么make会执行下一条命令，当规则中所有的命令成功返回后，这个规则就算是成功完成了。如果一个规则中的某个命令出错了（命令退出码非零），那么make就会终止执行当前规则，这将有可能终止所有规则的执行。 有些时候，命令的出错并不表示就是错误的。例如mkdir，如果目录不存在，那么mkdir就成功执行，万事大吉，如果目录存在，那么就出错了。之所以使用mkdir的意思就是一定要有这样的一个目录，只要这个目录存在了,就不希望mkdir出错而终止规则的运行。为了做到这一点，忽略命令的出错，可以在Makefile的命令行前加一个减号“-”（在Tab键之后），标记为不管命令出不出错都认为是成功的。如： clean: -rm -f *.o 还有一个全局的办法是，给make加上“-i”或是“–ignore-errors”参数，那么，Makefile中所有命令都会忽略错误。而如果一个规则是以“.IGNORE”作为目标的，那么这个规则中的所有命令将会忽略错误。还有一个要提一下的make的参数的是“-k”或是“–keep-going”，这个参数的意思是，如果某规则中的命令出错了，那么就终止该规则的执行，但继续执行其它规则。 在一些大的工程中，会把不同模块或是不同功能的源文件放在不同的目录中，这种情况可以在每个目录中都书写一个该目录的Makefile，例如，有一个子目录叫subdir，这个目录下有个Makefile文件，来指明了这个目录下文件的编译规则。那么总控的Makefile可以这样书写： subsystem: cd subdir &amp;&amp; $(MAKE)其等价于： subsystem: $(MAKE) -C subdir这两个例子的意思都是先进入“subdir”目录，然后执行make命令。总控Makefile的变量可以传递到下级的Makefile中，但是不会覆盖下层的Makefile中所定义的变量，除非指定了“-e”参数。 如果要传递变量到下级Makefile中，那么可以使用这样的声明： export如果不想让某些变量传递到下级Makefile中，那么可以这样声明： unexport如： 示例一： export variable = value 其等价于： variable = value export variable 其等价于： export variable := value 其等价于： variable := value export variable 示例二： export variable += value 其等价于： variable += value export variable如果要传递所有的变量，那么，只要一个export就行了,后面什么也不用跟，表示传递所有的变量。 八 变量 两种高级用法: 我们可以替换变量中的共有的部分，其格式是“$(var:a=b)”或是“${var:a=b}”，其意思是，把变量“var”中所有以“a”字串“结尾”的“a”替换成“b”字串。这里的“结尾”意思是“空格”或是“结束符”。示例： foo := a.o b.o c.o bar := $(foo:.o=.c)这个示例中，我们先定义了一个“$(foo)”变量，而第二行的意思是把“$(foo)”中所有的“.o”字串“结尾”全部替换成“.c”，所以“$(bar)”的值就是“a.c b.c c.c”。 第二种高级用法是——“把变量的值再当成变量”。先看一个例子： x = y y = z a := $($(x))在这个例子中，$(x)的值是“y”，所以$($(x))就是$(y)，于是$(a)的值就是“z”。（注意，是“x=y”，而不是“x=$(y)”） 我们还可以使用更多的层次： x = y y = z z = u a := $($($(x)))这里的$(a)的值是“u”. 还有一种设置变量值的方法是使用define关键字。使用define关键字设置变量的值可以有换行，这有利于定义一系列的命令.定义是以endef关键字结束,其工作方式和“=”操作符一样。变量的值可以包含函数、命令、文字，或是其它变量。命令需要以\[Tab\]键开头，define定义的命令也不例外.下面的这个示例展示了define的用法： define two-lines echo foo echo $(bar) endef 九 目标变量 前面所有的在Makefile中定义的变量都是“全局变量”，在整个文件，都可以访问这些变量。当然，“自动化变量”除外，如“$&lt;”等这种类量的自动化变量就属于“规则型变量”，这种变量的值依赖于规则的目标和依赖目标的定义。 当然，我们同样可以为某个目标设置局部变量，这种变量被称为“Target-specific Variable”，它可以和“全局变量”同名，因为它的作用范围只在这条规则以及连带规则中，所以其值也只在作用范围内有效,而不会影响规则链以外的全局变量的值。其语法是： : : overide可以是各种赋值表达式，如“=”、“:=”、“+=”或是“？=”。第二个语法是针对于make命令行带入的变量，或是系统环境变量。 这个特性非常的有用，当我们设置了这样一个变量，这个变量会作用到由这个目标所引发的所有的规则中去。如： prog : CFLAGS = -g prog : prog.o foo.o bar.o $(CC) $(CFLAGS) prog.o foo.o bar.o prog.o : prog.c $(CC) $(CFLAGS) prog.c foo.o : foo.c $(CC) $(CFLAGS) foo.c bar.o : bar.c $(CC) $(CFLAGS) bar.c在这个示例中，不管全局的$(CFLAGS)的值是什么，在prog目标，以及其所引发的所有规则中（prog.o foo.o bar.o的规则），$(CFLAGS)的值都是“-g”. 十 条件判断语法 条件表达式的语法为： else endif其中表示条件关键字，如“ifeq”。这个关键字有四个。 第一个是我们前面所见过的“ifeq” ifeq (, ) ifeq &apos;&apos; &apos;&apos; ifeq &quot;&quot; &quot;&quot; ifeq &quot;&quot; &apos;&apos; ifeq &apos;&apos; &quot;&quot;比较参数“arg1”和“arg2”的值是否相同。当然，参数中我们还可以使用make的函数。如： ifeq ($(strip $(foo)),) endif这个示例中使用了“strip”函数，如果这个函数的返回值是空（Empty），那么就生效。 第二个条件关键字是“ifneq”。 第三个条件关键字是“ifdef”。语法是： ifdef如果变量的值非空，那到表达式为真。否则，表达式为假。当然，同样可以是一个函数的返回值。注意，ifdef只是测试一个变量是否有值，其并不会把变量扩展到当前位置。还是来看两个例子： 示例一： bar = foo = $(bar) ifdef foo frobozz = yes else frobozz = no endif 示例二： foo = ifdef foo frobozz = yes else frobozz = no endif第一个例子中，“$(frobozz)”值是“yes”，第二个则是“no”。 第四个条件关键字是“ifndef”。 在这一行上，多余的空格是被允许的，但是不能以\[Tab\]键做为开始（不然就被认为是命令）。而注释符“#”同样也是安全的。“else”和“endif”也一样，只要不是以\[Tab\]键开始就行了。 特别注意的是，make是在读取Makefile时就计算条件表达式的值，并根据条件表达式的值来选择语句，所以，最好不要把自动化变量（如“$@”等）放入条件表达式中，因为自动化变量是在运行时才有的。十一 foreach 函数 foreach函数和别的函数非常的不一样。因为这个函数是用来做循环用的，它的语法是： $(foreach , &lt;list&gt;,&lt;text&gt;)这个函数的意思是，把参数 &lt;list&gt;中的单词逐一取出放到参数&lt;var&gt;所指定的变量中，然后再执行&lt;text&gt;所包含的表达式。每一次&lt;text&gt;会返回一个字符串，循环过程中，&lt;text&gt;的所返回的每个字符串会以空格分隔，最后当整个循环结束时，&lt;text&gt;所返回的每个字符串所组成的整个字符串（以空格分隔）将会是foreach函数的返回值。&amp;nbsp; &amp;nbsp;所以，&lt;var&gt;最好是一个变量名，&lt;/var&gt; &lt;list&gt;可以是一个表达式，而&lt;text&gt;中一般会使用&lt;var&gt;这个参数来依次枚举 &lt;list&gt;中的单词。举个例子：&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;names := a b c d files := $(foreach n,$(names),$(n).o) $(name)中的单词会被挨个取出，并存到变量“n”中，“$(n).o”每次根据“$(n)”计算出一个值，这些值以空格分隔，最后作为foreach函数的返回，所以，$(files)的值是“a.o b.o c.o d.o”。 十二 检查规则 有时候，我们不想让我们的makefile中的规则执行起来，只想检查一下命令，或是执行的序列。可以使用make命令的下述参数： “-n” “--just-print” “--dry-run”------------------ “-t” “--touch” 这个参数的意思就是把目标文件的时间更新，但不更改目标文件。也就是说，make假装编译目标，但不是真正的编译目标，只是把目标变成已编译过的状态。 “-W ” 这个参数需要指定一个文件。一般是是源文件（或依赖文件），make会根据规则推导来运行依赖于这个文件的命令，一般来说，可以和“-n”参数一同使用，来查看这个依赖文件所发生的规则命令。假定目标需要更新，如果和“-n”选项使用，那么这个参数会输出该目标更新时的运行动作。十三 make的其他参数 “-B” “–always-make” 认为所有的目标都需要更新（重编译）。 “-C ” “–directory= ” 指定读取makefile的目录。如果有多个“-C”参数，make的解释是后面的路径以前面的作为相对路径，并以最后的目录作为被指定目录。如：“make –C ~hchen/test –C prog”等价于“make –C ~hchen/test/prog”。 “-i” “–ignore-errors” 在执行时忽略所有的错误。 “-k” “–keep-going” 出错也不停止运行。如果生成一个目标失败了，那么依赖于其上的目标就不会被执行了。 十四 隐含规则 在使用Makefile时，有一些会经常使用，而且使用频率非常高的东西，比如，编译C/C++的源程序为中间目标文件（Unix下是\[.o\]文件，Windows下是\[.obj\]文件）,这些就是早先约定了的，不需要我们再写出来的规则。 “隐含规则”也就是一种惯例，make会按照这种“惯例”心照不喧地来运行，即使Makefile中没有书写这样的规则。例如，把\[.c\]文件编译成\[.o\]文件这一规则，根本就不用写出来，make会自动推导出这种规则，并生成需要的\[.o\]文件。 “隐含规则”会使用一些系统变量，我们可以改变这些系统变量的值来定制隐含规则的运行时的参数。如系统变量“CFLAGS”可以控制编译时的编译器参数。1 使用隐含规则 如果要使用隐含规则生成你需要的目标，你所需要做的就是不要写出这个目标的规则。那么，make会试图去自动推导产生这个目标的规则和命令，如果make可以自动推导生成这个目标的规则和命令，那么这个行为就是隐含规则的自动推导。当然，隐含规则是make事先约定好的一些东西。例如，我们有下面的一个Makefile： foo : foo.o bar.o cc –o foo foo.o bar.o $(CFLAGS) $(LDFLAGS)这个Makefile中并没有写下如何生成foo.o和bar.o这两目标的规则和命令。因为make的“隐含规则”功能会自动为我们自动去推导这两个目标的依赖目标和生成命令。 make会在自己的“隐含规则”库中寻找可以用的规则，如果找到，那么就会使用。如果找不到，那么就会报错。在上面的那个例子中，make调用的隐含规则是，把\[.o\]的目标的依赖文件置成\[.c\]，并使用C的编译命令“cc –c $(CFLAGS) \[.c\]”来生成\[.o\]的目标。也就是说，我们完全没有必要写下下面的两条规则： foo.o : foo.c cc –c foo.c $(CFLAGS) bar.o : bar.c cc –c bar.c $(CFLAGS)因为，这已经是“约定”好了的事了，make和我们约定好了用C编译器“cc”生成[.o]文件的规则，这就是隐含规则。 当然，如果我们为\[.o\]文件书写了自己的规则，那么make就不会自动推导并调用隐含规则，它会按照我们写好的规则忠实地执行。 还有，在make的“隐含规则库”中，每一条隐含规则都在库中有其顺序，越靠前的则是越被经常使用的，所以，这会导致我们有些时候即使我们显示地指定了目标依赖，make也不会管。如下面这条规则（没有命令）： foo.o : foo.p依赖文件“foo.p”（Pascal程序的源文件）有可能变得没有意义。如果目录下存在了“foo.c”文件，那么我们的隐含规则一样会生效，并会通过“foo.c”调用C的编译器生成foo.o文件。因为，在隐含规则中，Pascal的规则出现在C的规则之后，所以，make找到可以生成foo.o的C的规则就不再寻找下一条规则了。如果你确实不希望任何隐含规则推导，那么，你就不要只写出“依赖规则”，而不写命令。当然，我们也可以使用make的参数“-r”或“–no-builtin-rules”选项来取消所有的预设置的隐含规则。 1 编译C程序的隐含规则 “.o”的目标的依赖目标会自动推导为“.c”，并且其生成命令是“$(CC) –c $(CPPFLAGS) $(CFLAGS)” 2 编译C++程序的隐含规则 “.o”的目标的依赖目标会自动推导为“.cc”或是“.C”，并且其生成命令是“$(CXX) –c $(CPPFLAGS) $(CFLAGS)”。（建议使用“.cc”作为C++源文件的后缀，而不是“.C”） 3 链接Object文件的隐含规则 “”目标依赖于“.o”，通过运行C的编译器来运行链接程序生成（一般是“ld”），其生成命令是：“$(CC) $(LDFLAGS) .o $(LOADLIBES) $(LDLIBS)”。这个规则对于只有一个源文件的工程有效，同时也对多个Object文件（由不同的源文件生成）的也有效。例如如下规则： x : y.o z.o并且“x.c”、“y.c”和“z.c”都存在时，隐含规则将执行如下命令： cc -c x.c -o x.o cc -c y.c -o y.o cc -c z.c -o z.o cc x.o y.o z.o -o x rm -f x.o rm -f y.o rm -f z.o如果没有一个源文件（如上例中的x.c）和你的目标名字（如上例中的x）相关联，那么，你最好写出自己的生成规则，不然，隐含规则会报错的。]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
</search>
