{"meta":{"title":"Gate","subtitle":null,"description":"我总在抱怨自己没鞋子穿，直到有一天，我遇到了一个没有脚的人。","author":"Xiaobin.Liu","url":"https://lxb.wiki","root":"/"},"pages":[{"title":"示例页面","date":"2018-06-19T16:00:46.000Z","updated":"2019-09-26T15:41:04.078Z","comments":false,"path":"sample-page/index.html","permalink":"https://lxb.wiki/sample-page/index.html","excerpt":"","text":"这是一个范例页面。它和博客文章不同，因为它的页面位置是固定的，同时会显示于您的博客导航栏（大多数主题中）。大多数人会新增一个“关于”页面向访客介绍自己。它可能类似下面这样： 我是一个很有趣的人，我创建了工厂和庄园。并且，顺便提一下，我的妻子也很好。 ……或下面这样： XYZ装置公司成立于1971年，公司成立以来，我们一直向市民提供高品质的装置。我们位于北京市，有超过2,000名员工，对北京市有着相当大的贡献。 作为一个新的WordPress用户，您可以前往您的仪表盘删除这个页面，并建立属于您的全新内容。祝您使用愉快！"},{"title":"categories","date":"2019-09-26T16:04:42.000Z","updated":"2019-09-28T02:59:12.136Z","comments":true,"path":"categories/index.html","permalink":"https://lxb.wiki/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-09-28T03:19:12.000Z","updated":"2019-09-28T03:19:37.388Z","comments":true,"path":"about/index.html","permalink":"https://lxb.wiki/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-09-26T16:03:52.000Z","updated":"2019-09-28T02:55:51.738Z","comments":true,"path":"tags/index.html","permalink":"https://lxb.wiki/tags/index.html","excerpt":"","text":""},{"title":"repository","date":"2019-09-26T16:06:23.000Z","updated":"2019-09-26T16:06:36.445Z","comments":true,"path":"repository/index.html","permalink":"https://lxb.wiki/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"Go 1.14 中接口的菱形组合","slug":"Go-1-14-中接口的菱形组合","date":"2020-06-01T13:00:20.000Z","updated":"2020-06-01T13:03:28.408Z","comments":true,"path":"ace0b2ab/","link":"","permalink":"https://lxb.wiki/ace0b2ab/","excerpt":"","text":"按照部分重叠的接口提议，Go 1.14 现在允许嵌入有部分方法重叠的接口。本文是一篇解释这次修改的简要说明。 我们先来看 io 包中的三个关键接口：io.Reader、io.Writer 和 io.Closer： 12345678910111213package iotype Reader interface &#123; Read([]byte) (int, error)&#125;type Writer interface &#123; Write([]byte) (int, error)&#125;type Closer interface &#123; Close() error&#125; 在结构体中嵌入类型时，如果在结构体中声明了被嵌入的类型，那么该类型的字段和方法允许被访问^1，对于接口来说这个处理也成立。因此下面两种方式：显式声明 1234type ReadCloser interface &#123; Read([]byte) (int, error) Close() error&#125; 和使用嵌入来组成接口 1234type ReadCloser interface &#123; Reader Closer&#125; 没有区别。 你甚至可以混合使用： 1234type WriteCloser interface &#123; Write([]byte) (int, error) Closer&#125; 然而，在 Go 1.14 之前，如果你用这种方式来声明接口，你可能会得到类似这样的结果： 1234type ReadWriteCloser interface &#123; ReadCloser WriterCloser&#125; 编译错误： 123% Go build interfaces.gocommand-line-arguments./interfaces.go:27:2: duplicate method Close 幸运的是，在 Go 1.14 中这不再是一个限制了，因此这个改动解决了在菱形嵌入时出现的问题。 然而，在我向本地的用户组解释这个特性时也陷入了麻烦 — 只有 Go 编译器使用 1.14（或更高版本）语言规范时才支持这个特性。 我理解的编译过程中 Go 语言规范所使用的版本的规则似乎是这样的： 如果你的源码是在 GOPATH 下（或者你用 GO111MODULE=off 关闭了 module），那么 Go 语言规范会使用你编译器的版本来编译。换句话说，如果安装了 Go 1.13，那么你的 Go 版本就是 1.13。如果你安装了 Go 1.14，那么你的版本就是 1.14。这里符合认知。 如果你的源码保存在 GOPATH 外（或你用 GO111MODULE=on 强制开启了 module），那么 Go tool 会从 go.mod 文件中获取 Go 版本。 如果 go.mod 中没有列出 Go 版本，那么语言规范会使用安装的 Go 的版本。这跟第 1 点是一致的。 如果你用的是 Go module 模式，不管是源码在 GOPATH 外还是设置了 GO111MODULE=on，但是在当前目录或所有父目录中都没有 go.mod 文件，那么 Go 语言规范会默认用 Go 1.13 版本来编译你的代码。 我曾经遇到过第 4 点的情况。 via: https://dave.cheney.net/2020/05/24/diamond-interface-composition-in-go-1-14 作者：Dave Cheney 译者：lxbwolf 校对：polaris1119 本文由 GCTT 原创编译，Go 中文网 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"树莓派安装TT-RSS","slug":"树莓派安装TT-RSS","date":"2020-05-30T14:05:48.000Z","updated":"2020-05-30T14:07:10.012Z","comments":true,"path":"afdc720c/","link":"","permalink":"https://lxb.wiki/afdc720c/","excerpt":"","text":"Tiny Tiny RSS（TT-RSS）是一个自由开源的基于 Web 的新闻馈送feed（RSS/Atom）阅读器和聚合工具。它非常适合那些注重隐私，并且仍然依赖 RSS 来获取日常新闻的人。TT-RSS 是自行托管的软件，因此你可以 100% 的掌控你的服务器、数据以及你的全部隐私。它还支持大量的插件、扩展和主题。你喜欢黑暗模式的界面？没问题。想基于关键词过滤发来的消息？TT-RSS 也能让你得偿所愿。 现在你知道 TT-RSS 是什么了，那么为什么你可能会想用它。我会讲述要把它安装到树莓派或 Debian 10 服务器上你需要了解的所有的东西。 安装和配置 TT-RSS要把 TT-RSS 安装到树莓派上，你还需要安装和配置最新版本的 PHP（本文撰写时 PHP 最新版本是 7.3）、后端数据库 PostgreSQL、Nginx web 服务器、Git，最后才是 TT-RSS。 1、安装 PHP 7安装 PHP 7 是整个过程中最复杂的部分。幸运的是，它并不像看起来那样困难。从安装下面的支持包开始： 1$ sudo apt install -y ca-certificates apt-transport-https 现在，添加存储库 PGP 密钥： 1$ wget -q https://packages.sury.org/php/apt.gpg -O- | sudo apt-key add - 下一步，把 PHP 库添加到你的 apt 源： 1$ echo &quot;deb https://packages.sury.org/php/ buster main&quot; | sudo tee /etc/apt/sources.list.d/php.list 然后更新你的存储库索引： 1$ sudo apt update 最后，安装 PHP 7.3（或最新版本）和一些通用组件： 1$ sudo apt install -y php7.3 php7.3-cli php7.3-fpm php7.3-opcache php7.3-curl php7.3-mbstring php7.3-pgsql php7.3-zip php7.3-xml php7.3-gd php7.3-intl 上面的命令默认你使用的后端数据库是 PostgreSQL，会安装 php7.3-pgsql。如果你想用 MySQL 或 MariaDB，你可以把命令参数改为 php7.3-mysql。 下一步，确认 PHP 已安装并在你的树莓派上运行着： 1$ php -v 现在是时候安装和配置 Web 服务器了。 2、安装 Nginx可以用下面的命令安装 Nginx： 1$ sudo apt install -y nginx 修改默认的 Nginx 虚拟主机配置，这样 Web 服务器才能识别 PHP 文件以及知道如何处理它们。 1$ sudo nano /etc/nginx/sites-available/default 你可以安全地删除原文件中的所有内容，用下面的内容替换： 123456789101112131415161718server &#123; listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.html index.htm index.php; server_name _; location / &#123; try_files $uri $uri/ =404; &#125; location ~ \\.php$ &#123; include snippets/fastcgi-php.conf; fastcgi_pass unix:/run/php/php7.3-fpm.sock; &#125;&#125; 按 Ctrl+O 保存修改后的配置文件，然后按 Ctrl+X 退出 Nano。你可以用下面的命令测试你的新配置文件： 1$ nginx -t 如果没有报错，重启 Nginx 服务： 1$ systemctl restart nginx 3、安装 PostgreSQL接下来是安装数据库服务器。在树莓派上安装 PostgreSQL 超级简单： 1$ sudo apt install -y postgresql postgresql-client postgis 输入下面的命令看一下数据库服务器安装是否成功： 1$ psql --version 4、创建 Tiny Tiny RSS 数据库在做其他事之前，你需要创建一个数数据库，用来给 TT-RSS 软件保存数据。首先，登录 PostgreSQL 服务器： 1sudo -u postgres psql 下一步，新建一个用户，设置密码： 1CREATE USER username WITH PASSWORD &apos;your_password&apos; VALID UNTIL &apos;infinity&apos;; 然后创建一个给 TT-RSS 用的数据库： 1CREATE DATABASE tinyrss; 最后，给新建的用户赋最高权限： 1GRANT ALL PRIVILEGES ON DATABASE tinyrss to user_name; 这是安装数据库的步骤。你可以输入 \\q 来退出 psql 程序。 5、安装 Git安装 TT-RSS 需要用 Git，所以输入下面的命令安装 Git： 1$ sudo apt install git -y 现在，进入到 Nginx 服务器的根目录： 1$ cd /var/www/html 下载 TT-RSS 最新源码： 1$ git clone https://git.tt-rss.org/fox/tt-rss.git tt-rss 注意，这一步会创建一个 tt-rss 文件夹。 6、安装和配置Tiny Tiny RSS现在是安装和配置你的新 TT-RSS 服务器的最后时刻了。首先，确认下你在浏览器中能打开 http://your.site/tt-rss/install/index.php。如果浏览器显示 403 Forbidden，那么就证明 /var/www/html 文件夹的权限没有设置正确。下面的命令通常能解决这个问题： 1$ chmod 755 /var/www/html/ -v 如果一切正常，你会看到 TT-RSS 安装页面，它会让你输入一些数据的信息。你只需要输入前面你创建的数据库用户名和密码；数据库名；主机名填 localhost；端口填 5432。 点击“Test Configuration”。如果一切正常，你会看到一个标记着“Initialize Database”的红色按钮。点击它来开始安装。结束后，你会看到一个配置文件，你可以把它复制到 TT-RSS 的目录，另存为 config.php。 安装过程结束后，浏览器输入 http://yoursite/tt-rss/ 打开 TT-RSS，使用默认的凭证登录（用户名：admin，密码：password）。登录后系统会提示你修改密码。我强烈建议你尽快修改密码。 配置 TT-RSS如果一切正常，你现在就可以开始使用 TT-RSS 了。建议你新建一个非管理员用户，使用新用户名登录，并开始导入你的馈送、订阅，按照你的意愿来配置它。 最后，并且是超级重要的事，不要忘了阅读 TT-RSS 维基上的 Updating Feeds 部分。它讲述了如何创建一个简单的 systemd 服务来更新馈送。如果你跳过了这一步，你的 RSS 馈送就不会自动更新。 总结呵！工作量不小，但是你做完了！你现在有自己的 RSS 聚合服务器了。想了解 TT-RSS 更多的知识？我推荐你去看官方的 FAQ、支持论坛，和详细的安装笔记。如果你有任何问题，尽情地在下面评论吧。 via: https://opensource.com/article/20/2/ttrss-raspberry-pi 作者：Patrick H. Mullins选题：lujun9972译者：lxbwolf校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出","categories":[{"name":"RPI","slug":"RPI","permalink":"https://lxb.wiki/categories/RPI/"}],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"https://lxb.wiki/tags/树莓派/"}]},{"title":"通过禁止比较让 Go 二进制文件变小","slug":"通过禁止比较让-Go-二进制文件变小","date":"2020-05-23T04:24:34.000Z","updated":"2020-05-23T04:41:28.498Z","comments":true,"path":"27d7ea27/","link":"","permalink":"https://lxb.wiki/27d7ea27/","excerpt":"","text":"大家常规的认知是，Go 程序中声明的类型越多，生成的二进制文件就越大。这个符合直觉，毕竟如果你写的代码不去操作定义的类型，那么定义一堆类型就没有意义了。然而，链接器的部分工作就是检测没有被程序引用的函数（比如说它们是一个库的一部分，其中只有一个子集的功能被使用），然后把它们从最后的编译产出中删除。常言道，“类型越多，二进制文件越大”，对于多数 Go 程序还是正确的。 本文中我会深入讲解在 Go 程序的上下文中“相等”的意义，以及为什么像这样的修改会对 Go 程序的大小有重大的影响。 定义两个值相等Go 的语法定义了“赋值”和“相等”的概念。赋值是把一个值赋给一个标识符的行为。并不是所有声明的标识符都可以被赋值，如常量和函数就不可以。相等是通过检查标识符的内容是否相等来比较两个标识符的行为。 作为强类型语言，“相同”的概念从根源上被植入标识符的类型中。两个标识符只有是相同类型的前提下，才有可能相同。除此之外，值的类型定义了如何比较该类型的两个值。 例如，整型是用算数方法进行比较的。对于指针类型，是否相等是指它们指向的地址是否相同。映射和通道等引用类型，跟指针类似，如果它们指向相同的地址，那么就认为它们是相同的。 上面都是按位比较相等的例子，即值占用的内存的位模式是相同的，那么这些值就相等。这就是所谓的 memcmp，即内存比较，相等是通过比较两个内存区域的内容来定义的。 记住这个思路，我过会儿再来谈。 结构体相等除了整型、浮点型和指针等标量类型，还有复合类型：结构体。所有的结构体以程序中的顺序被排列在内存中。因此下面这个声明： 123type S struct &#123; a, b, c, d int64&#125; 会占用 32 字节的内存空间；a 占用 8 个字节，b 占用 8 个字节，以此类推。Go 的规则说如果结构体所有的字段都是可以比较的，那么结构体的值就是可以比较的。因此如果两个结构体所有的字段都相等，那么它们就相等。 123a := S&#123;1, 2, 3, 4&#125;b := S&#123;1, 2, 3, 4&#125;fmt.Println(a == b) // 输出 true 编译器在底层使用 memcmp 来比较 a 的 32 个字节和 b 的 32 个字节。 填充和对齐然而，在下面的场景下过分简单化的按位比较的策略会返回错误的结果： 123456789101112type S struct &#123; a byte b uint64 c int16 d uint32&#125;func main() a := S&#123;1, 2, 3, 4&#125; b := S&#123;1, 2, 3, 4&#125; fmt.Println(a == b) // 输出 true&#125; 编译代码后，这个比较表达式的结果还是 true，但是编译器在底层并不能仅依赖比较 a 和 b 的位模式，因为结构体有填充。 Go 要求结构体的所有字段都对齐。2 字节的值必须从偶数地址开始，4 字节的值必须从 4 的倍数地址开始，以此类推。编译器根据字段的类型和底层平台加入了填充来确保字段都对齐。在填充之后，编译器实际上看到的是： 12345678type S struct &#123; a byte _ [7]byte // 填充 b uint64 c int16 _ [2]int16 // 填充 d uint32&#125; 填充的存在保证了字段正确对齐，而填充确实占用了内存空间，但是填充字节的内容是未知的。你可能会认为在 Go 中 填充字节都是 0，但实际上并不是 — 填充字节的内容是未定义的。由于它们并不是被定义为某个确定的值，因此按位比较会因为分布在 s 的 24 字节中的 9 个填充字节不一样而返回错误结果。 Go 通过生成所谓的相等函数来解决这个问题。在这个例子中，s 的相等函数只比较函数中的字段略过填充部分，这样就能正确比较类型 s 的两个值。 类型算法呵，这是个很大的设置，说明了为什么，对于 Go 程序中定义的每种类型，编译器都会生成几个支持函数，编译器内部把它们称作类型的算法。如果类型是一个映射的键，那么除相等函数外，编译器还会生成一个哈希函数。为了维持稳定，哈希函数在计算结果时也会像相等函数一样考虑诸如填充等因素。 凭直觉判断编译器什么时候生成这些函数实际上很难，有时并不明显，（因为）这超出了你的预期，而且链接器也很难消除没有被使用的函数，因为反射往往导致链接器在裁剪类型时变得更保守。 通过禁止比较来减小二进制文件的大小现在，我们来解释一下 Brad 的修改。向类型添加一个不可比较的字段，结构体也随之变成不可比较的，从而强制编译器不再生成相等函数和哈希函数，规避了链接器对那些类型的消除，在实际应用中减小了生成的二进制文件的大小。作为这项技术的一个例子，下面的程序： 123456789101112131415package mainimport &quot;fmt&quot;func main() &#123; type t struct &#123; // _ [0][]byte // 取消注释以阻止比较 a byte b uint16 c int32 d uint64 &#125; var a t fmt.Println(a)&#125; 用 Go 1.14.2（darwin/amd64）编译，大小从 2174088 降到了 2174056，节省了 32 字节。单独看节省的这 32 字节似乎微不足道，但是考虑到你的程序中每个类型及其传递闭包都会生成相等和哈希函数，还有它们的依赖，这些函数的大小随类型大小和复杂度的不同而不同，禁止它们会大大减小最终的二进制文件的大小，效果比之前使用 -ldflags=&quot;-s -w&quot; 还要好。 最后总结一下，如果你不想把类型定义为可比较的，可以在源码层级强制实现像这样的奇技淫巧，会使生成的二进制文件变小。 via: https://dave.cheney.net/2020/05/09/ensmallening-go-binaries-by-prohibiting-comparisons 作者：Dave Cheney选题：lujun9972译者：lxbwolf校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"},{"name":"编译器","slug":"编译器","permalink":"https://lxb.wiki/tags/编译器/"},{"name":"翻译","slug":"翻译","permalink":"https://lxb.wiki/tags/翻译/"}]},{"title":"Go：异步抢占","slug":"Go：异步抢占","date":"2020-05-05T07:24:46.000Z","updated":"2020-05-16T07:31:16.920Z","comments":true,"path":"5698ca18/","link":"","permalink":"https://lxb.wiki/5698ca18/","excerpt":"","text":"ℹ️ 本文基于 Go 1.14。 抢占是调度器的重要部分，基于抢占调度器可以在各个协程中分配运行的时间。实际上，如果没有抢占机制，一个长时间占用 CPU 的协程会阻塞其他的协程被调度。1.14 版本引入了一项新的异步抢占的技术，赋予了调度器更大的能力和控制力。 我推荐你阅读我的文章”Go：协程和抢占“来了解更多之前的特性和它的弊端。 工作流我们以一个需要抢占的例子来开始。下面一段代码开启了几个协程，在几个循环中没有其他的函数调用，意味着调度器没有机会抢占它们： 然而，当把这个程序的追踪过程可视化后，我们清晰地看到了协程间的抢占和切换： 我们还可以看到表示协程的每个块儿的长度都相等。所有的协程运行时间相同（约 10 到 20 毫秒）。 异步抢占是基于一个时间条件触发的。当一个协程运行超过 10ms 时，Go 会尝试抢占它。 抢占是由线程 sysmon 初始化的，该线程专门用于监控包括长时间运行的协程在内的运行时。当某个协程被检测到运行超过 10ms 后，sysmon 向当前的线程发出一个抢占信号。 之后，当信息被信号处理器接收到时，线程中断当前的操作来处理信号，因此不会再运行当前的协程，在我们的例子中是 G7。取而代之的是，gsignal 被调度为管理发送来的信号。当它发现它是一个抢占指令后，在程序处理信号后恢复时它准备好指令来中止当前的协程。下面是这第二个阶段的示意图： 如果你想了解更多关于 gsignal 的信息，我推荐你读一下我的文章”Go：gsignal，信号的掌控者“。 实现我们在被选中的信号 SIGURG 中第一次看到了实现的细节。这个选择在提案”提案：非合作式协程抢占“中有详细的解释： 它应该是调试者默认传递过来的一个信号。 它不应该是 Go/C 混合二进制中 libc 内部使用的信号。 它应该是一个可以伪造而没有其他后果的信号。 我们需要在没有实时信号时与平台打交道。然后，当信号被注入和接收时，Go 需要一种在程序恢复时能终止当前协程的方式。为了实现这个过程，Go 会把一条指令推进程序计数器，这样看起来运行中的程序调用了运行时的函数。该函数暂停了协程并把它交给了调度器，调度器之后还会运行其他的协程。 我们应该注意到 Go 不能做到在任何地方终止程序；当前的指令必须是一个安全点。例如，如果程序现在正在调用运行时，那么抢占协程并不安全，因为运行时很多函数不应该被抢占。 这个新的抢占机制也让垃圾回收器受益，可以用更高效的方式终止所有的协程。诚然，STW 现在非常容易，Go 仅需要向所有运行的线程发出一个信号就可以了。下面是垃圾回收器运行时的一个例子： 然后，所有的线程都接收到这个信号，在垃圾回收器重新开启全局之前会暂停执行。 如果你想了解更多关于 STW 的信息，我建议你阅读我的文章”Go：Go 怎样实现 STW？“。 最后，这个特性被封装在一个参数中，你可以用这个参数关闭异步抢占。你可以用 GODEBUG=asyncpreemptoff=1 来运行你的程序，如果你因为升级到了 Go 1.14 发现了不正常的现象就可以调试你的程序，或者观察你的程序有无异步抢占时的不同表现。 via: https://medium.com/a-journey-with-go/go-asynchronous-preemption-b5194227371c 作者：Vincent Blanchon译者：lxbwolf校对：polaris1119 本文由 GCTT 原创编译，Go 中文网 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"Inlining Optimisations in Go","slug":"Inlining-optimisations-Go","date":"2020-04-29T13:05:10.000Z","updated":"2020-05-05T11:39:31.603Z","comments":true,"path":"6ce34c49/","link":"","permalink":"https://lxb.wiki/6ce34c49/","excerpt":"","text":"Go 中的内联优化 本文讨论 Go 编译器是如何实现内联的，以及这种优化方法如何影响你的 Go 代码。 请注意：本文重点讨论 gc，这是来自 golang.org 的事实标准的 Go 编译器。讨论到的概念可以广泛适用于其它 Go 编译器，如 gccgo 和 llgo，但它们在实现方式和功效上可能有所差异。 内联是什么？内联inlining就是把简短的函数在调用它的地方展开。在计算机发展历程的早期，这个优化是由程序员手动实现的。现在，内联已经成为编译过程中自动实现的基本优化过程的其中一步。 为什么内联很重要？有两个原因。第一个是它消除了函数调用本身的开销。第二个是它使得编译器能更高效地执行其他的优化策略。 函数调用的开销在任何语言中，调用一个函数 [^1] 都会有消耗。把参数编组进寄存器或放入栈中（取决于 ABI），在返回结果时的逆反过程都会有开销。引入一次函数调用会导致程序计数器从指令流的一点跳到另一点，这可能导致管道滞后。函数内部通常有前置处理preamble，需要为函数执行准备新的栈帧，还有与前置相似的后续处理epilogue，需要在返回给调用方之前释放栈帧空间。 在 Go 中函数调用会消耗额外的资源来支持栈的动态增长。在进入函数时，goroutine 可用的栈空间与函数需要的空间大小进行比较。如果可用空间不同，前置处理就会跳到运行时runtime的逻辑中，通过把数据复制到一块新的、更大的空间的来增长栈空间。当这个复制完成后，运行时就会跳回到原来的函数入口，再执行栈空间检查，现在通过了检查，函数调用继续执行。这种方式下，goroutine 开始时可以申请很小的栈空间，在有需要时再申请更大的空间。[^2] 这个检查消耗很小，只有几个指令，而且由于 goroutine 的栈是成几何级数增长的，因此这个检查很少失败。这样，现代处理器的分支预测单元可以通过假定检查肯定会成功来隐藏栈空间检查的消耗。当处理器预测错了栈空间检查，不得不放弃它在推测性执行所做的操作时，与为了增加 goroutine 的栈空间运行时所需的操作消耗的资源相比，管道滞后的代价更小。 虽然现代处理器可以用预测性执行技术优化每次函数调用中的泛型和 Go 特定的元素的开销，但那些开销不能被完全消除，因此在每次函数调用执行必要的工作过程中都会有性能消耗。一次函数调用本身的开销是固定的，与更大的函数相比，调用小函数的代价更大，因为在每次调用过程中它们做的有用的工作更少。 因此，消除这些开销的方法必须是要消除函数调用本身，Go 的编译器就是这么做的，在某些条件下通过用函数的内容来替换函数调用来实现。这个过程被称为内联，因为它在函数调用处把函数体展开了。 改进的优化机会Cliff Click 博士把内联描述为现代编译器做的优化措施，像常量传播（LCTT 译注：此处作者笔误，原文为 constant proportion，修正为 constant propagation）和死代码消除一样，都是编译器的基本优化方法。实际上，内联可以让编译器看得更深，使编译器可以观察调用的特定函数的上下文内容，可以看到能继续简化或彻底消除的逻辑。由于可以递归地执行内联，因此不仅可以在每个独立的函数上下文处进行这种优化决策，也可以在整个函数调用链中进行。 实践中的内联下面这个例子可以演示内联的影响： 123456789101112131415161718192021package mainimport \"testing\"//go:noinlinefunc max(a, b int) int &#123; if a &gt; b &#123; return a &#125; return b&#125;var Result intfunc BenchmarkMax(b *testing.B) &#123; var r int for i := 0; i &lt; b.N; i++ &#123; r = max(-1, i) &#125; Result = r&#125; 运行这个基准，会得到如下结果：[^3] 12% go test -bench=. BenchmarkMax-4 530687617 2.24 ns/op 在我的 2015 MacBook Air 上 max(-1, i) 的耗时约为 2.24 纳秒。现在去掉 //go:noinline 编译指令，再看下结果： 12% go test -bench=. BenchmarkMax-4 1000000000 0.514 ns/op 从 2.24 纳秒降到了 0.51 纳秒，或者从 benchstat 的结果可以看出，有 78% 的提升。 123% benchstat &#123;old,new&#125;.txtname old time/op new time/op deltaMax-4 2.21ns ± 1% 0.49ns ± 6% -77.96% (p=0.000 n=18+19) 这个提升是从哪儿来的呢？ 首先，移除掉函数调用以及与之关联的前置处理 [^4] 是主要因素。把 max 函数的函数体在调用处展开，减少了处理器执行的指令数量并且消除了一些分支。 现在由于编译器优化了 BenchmarkMax，因此它可以看到 max 函数的内容，进而可以做更多的提升。当 max 被内联后，BenchmarkMax 呈现给编译器的样子，看起来是这样的： 1234567891011func BenchmarkMax(b *testing.B) &#123; var r int for i := 0; i &lt; b.N; i++ &#123; if -1 &gt; i &#123; r = -1 &#125; else &#123; r = i &#125; &#125; Result = r&#125; 再运行一次基准，我们看一下手动内联的版本和编译器内联的版本的表现： 123% benchstat &#123;old,new&#125;.txtname old time/op new time/op deltaMax-4 2.21ns ± 1% 0.48ns ± 3% -78.14% (p=0.000 n=18+18) 现在编译器能看到在 BenchmarkMax 里内联 max 的结果，可以执行以前不能执行的优化措施。例如，编译器注意到 i 初始值为 0，仅做自增操作，因此所有与 i 的比较都可以假定 i 不是负值。这样条件表达式 -1 &gt; i 永远不是 true。[^5] 证明了 -1 &gt; i 永远不为 true 后，编译器可以把代码简化为： 1234567891011func BenchmarkMax(b *testing.B) &#123; var r int for i := 0; i &lt; b.N; i++ &#123; if false &#123; r = -1 &#125; else &#123; r = i &#125; &#125; Result = r&#125; 并且因为分支里是个常量，编译器可以通过下面的方式移除不会走到的分支： 1234567func BenchmarkMax(b *testing.B) &#123; var r int for i := 0; i &lt; b.N; i++ &#123; r = i &#125; Result = r&#125; 这样，通过内联和由内联解锁的优化过程，编译器把表达式 r = max(-1, i)) 简化为 r = i。 内联的限制本文中我论述的内联称作叶子内联leaf inlining：把函数调用栈中最底层的函数在调用它的函数处展开的行为。内联是个递归的过程，当把函数内联到调用它的函数 A 处后，编译器会把内联后的结果代码再内联到 A 的调用方，这样持续内联下去。例如，下面的代码： 1234567func BenchmarkMaxMaxMax(b *testing.B) &#123; var r int for i := 0; i &lt; b.N; i++ &#123; r = max(max(-1, i), max(0, i)) &#125; Result = r&#125; 与之前的例子中的代码运行速度一样快，因为编译器可以对上面的代码重复地进行内联，也把代码简化到 r = i 表达式。 下一篇文章中，我会论述当 Go 编译器想要内联函数调用栈中间的某个函数时选用的另一种内联策略。最后我会论述编译器为了内联代码准备好要达到的极限，这个极限 Go 现在的能力还达不到。 [^1]: 在 Go 中，一个方法就是一个有预先定义的形参和接受者的函数。假设这个方法不是通过接口调用的，调用一个无消耗的函数所消耗的代价与引入一个方法是相同的。[^2]: 在 Go 1.14 以前，栈检查的前置处理也被垃圾回收器用于 STW，通过把所有活跃的 goroutine 栈空间设为 0，来强制它们切换为下一次函数调用时的运行时状态。这个机制最近被替换为一种新机制，新机制下运行时可以不用等 goroutine 进行函数调用就可以暂停 goroutine。[^3]: 我用 //go:noinline 编译指令来阻止编译器内联 max。这是因为我想把内联 max 的影响与其他影响隔离开，而不是用 -gcflags=&#39;-l -N&#39; 选项在全局范围内禁止优化。关于 //go: 注释在这篇文章中详细论述。[^4]: 你可以自己通过比较 go test -bench=. -gcflags=-S 有无 //go:noinline 注释时的不同结果来验证一下。[^5]: 你可以用 -gcflags=-d=ssa/prove/debug=on 选项来自己验证一下。 相关文章： 使 Go 变快的 5 件事 为什么 Goroutine 的栈空间会无限增长？ Go 中怎么写基准测试 Go 中隐藏的编译指令 via: https://dave.cheney.net/2020/04/25/inlining-optimisations-in-go 作者：Dave Cheney选题：lujun9972译者：lxbwolf校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"},{"name":"编译器","slug":"编译器","permalink":"https://lxb.wiki/tags/编译器/"}]},{"title":"树莓派做BT下载器","slug":"树莓派做BT下载器","date":"2020-03-11T14:29:00.000Z","updated":"2020-05-10T19:40:07.694Z","comments":true,"path":"fb1d9193/","link":"","permalink":"https://lxb.wiki/fb1d9193/","excerpt":"","text":"可以用 transmission，它提供了 Web 界面 1. 安装 transmission：1sudo apt-get install transmission-daemon 2. 创建下载目录，一个是下载完成的目录，一个是未完成的目录12mkdir Public/bt_completemkdir Public/bt_incomplete 3. 配置目录权限12345sudo usermod -a -G debian-transmission pisudo chgrp debian-transmission bt_completesudo chgrp debian-transmission bt_incompletesudo chmod 770 bt_completesudo chmod 770 bt_incomplete 4. 修改配置文件 /etc/transmission-daemon/settings.json123&quot;download-dir&quot;:&quot;/home/pi/Public/bt_complete&quot;&quot;incomplete-dir&quot;:&quot;/home/pi/Public/bt_incomplete&quot;&quot;rpc-host-whitelist&quot;: &quot;192.168.1.*&quot;, 5. 重启 transmission12sudo service transmission-daemon reloadsudo service transmission-daemon restart 两个命令按顺序执行，单独 restart 的话配置不会保存： 浏览器中输入 http://192.168.1.8:9091/，默认用户名密码：transmission 修改 transmission 用户名和密码的方法： 先停止服务： sudo service transmission-daemon stop 修改配置文件，看到这个是加密的密码，直接把密码改为密码明文就可以： 12“rpc-username”: “明文”,“rpc-password”: “密文”, 再此启动服务 ：sudo service transmission-daemon start 启动的时候 transmission 会自动把新密码加密。 transmission 默认监听 51413 端口，最好在路由器上做个端口转发，把这个端口转到它的 IP 地址","categories":[{"name":"RPI","slug":"RPI","permalink":"https://lxb.wiki/categories/RPI/"}],"tags":[{"name":"torrent","slug":"torrent","permalink":"https://lxb.wiki/tags/torrent/"}]},{"title":"树莓派搭建GitHub镜像服务","slug":"树莓派搭建GitHub镜像服务","date":"2020-03-10T11:03:42.000Z","updated":"2020-05-20T15:37:42.070Z","comments":true,"path":"2073ae8b/","link":"","permalink":"https://lxb.wiki/2073ae8b/","excerpt":"","text":"1. 树莓派上创建 git 账号，创建用于存放代码的目录/srv/ 2. GitHub 库 clone 到树莓派git clone git@github.com:user/XXXX.git /srv/ 3. 添加 remotegit remote add upstream https://github.com/abcd/XXXX 4. 修改 hook1234#.git/hooks/post-updateparam=\"$1\"push_branch=$&#123;param##refs/heads/&#125; #获取到更新的分支名git push origin $push_branch 5. 添加定时任务5,35 * * * * cd /srv/XXXX &amp;&amp; git pull upstream master 6. 在本地代码添加 remote6.1 有多个项目时，为避免修改每个项目的 remote，直接添加 host192.168.1.8 gitsrv 6.2 在每个项目在添加一次 remotegit remote add pi git@192.168.1.8:/srv/XXXX 这样即使以后地址改变，只需要改一次 host 就可以了 7. 推拉代码时，从 pi 推拉git pull pi branch git push pi branch","categories":[{"name":"RPI","slug":"RPI","permalink":"https://lxb.wiki/categories/RPI/"}],"tags":[{"name":"GitHub","slug":"GitHub","permalink":"https://lxb.wiki/tags/GitHub/"},{"name":"树莓派","slug":"树莓派","permalink":"https://lxb.wiki/tags/树莓派/"}]},{"title":"非实时信号表","slug":"非实时信号表","date":"2020-02-15T11:19:45.000Z","updated":"2020-05-05T14:53:40.668Z","comments":true,"path":"c77ce28f/","link":"","permalink":"https://lxb.wiki/c77ce28f/","excerpt":"","text":"信号类型Linux系统共定义了64种信号，分为两大类：可靠信号与不可靠信号，前32种信号为不可靠信号，后32种为可靠信号。 概念不可靠信号： 也称为非实时信号，不支持排队，信号可能会丢失, 比如发送多次相同的信号, 进程只能收到一次. 信号值取值区间为1~31；可靠信号： 也称为实时信号，支持排队, 信号不会丢失, 发多少次, 就可以收到多少次. 信号值取值区间为32~64 信号表在终端，可通过kill -l查看所有的signal信号 取值 名称 解释 默认动作 1 SIGHUP 挂起 2 SIGINT 中断 3 SIGQUIT 退出 4 SIGILL 非法指令 5 SIGTRAP 断点或陷阱指令 6 SIGABRT abort发出的信号 7 SIGBUS 非法内存访问 8 SIGFPE 浮点异常 9 SIGKILL kill信号 不能被忽略、处理和阻塞 10 SIGUSR1 用户信号1 11 SIGSEGV 无效内存访问 12 SIGUSR2 用户信号2 13 SIGPIPE 管道破损，没有读端的管道写数据 14 SIGALRM alarm发出的信号 15 SIGTERM 终止信号 16 SIGSTKFLT 栈溢出 17 SIGCHLD 子进程退出 默认忽略 18 SIGCONT 进程继续 19 SIGSTOP 进程停止 不能被忽略、处理和阻塞 20 SIGTSTP 进程停止 21 SIGTTIN 进程停止，后台进程从终端读数据时 22 SIGTTOU 进程停止，后台进程想终端写数据时 23 SIGURG I/O有紧急数据到达当前进程 默认忽略 24 SIGXCPU 进程的CPU时间片到期 25 SIGXFSZ 文件大小的超出上限 26 SIGVTALRM 虚拟时钟超时 27 SIGPROF profile时钟超时 28 SIGWINCH 窗口大小改变 默认忽略 29 SIGIO I/O相关 30 SIGPWR 关机 默认忽略 31 SIGSYS 系统调用异常","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[{"name":"signal","slug":"signal","permalink":"https://lxb.wiki/tags/signal/"}]},{"title":"关于 CGo 的字符串函数的解释","slug":"关于-CGo-的字符串函数的解释","date":"2020-02-09T12:55:26.000Z","updated":"2020-05-05T11:39:59.173Z","comments":true,"path":"8c45788a/","link":"","permalink":"https://lxb.wiki/8c45788a/","excerpt":"","text":"cgo 的大量文档都提到过，它提供了四个用于转换 Go 和 C 类型的字符串的函数，都是通过复制数据来实现。在 CGo 的文档中有简洁的解释，但我认为解释得太简洁了，因为文档只涉及了定义中的某些特定字符串，而忽略了两个很重要的注意事项。我曾经踩过这里的坑，现在我要详细解释一下。 四个函数分别是： 1234func C.CString(string) *C.charfunc C.GoString(*C.char) stringfunc C.GoStringN(*C.char, C.int) stringfunc C.GoBytes(unsafe.Pointer, C.int) []byte C.CString() 等价于 C 的 strdup()，像文档中提到的那样，把 Go 的字符串复制为可以传递给 C 函数的 C 的 char *。很讨厌的一件事是，由于 Go 和 CGo 类型的定义方式，调用 C.free 时需要做一个转换： 12cs := C.CString(\"a string\")C.free(unsafe.Pointer(cs)) 请留意，Go 字符串中可能嵌入了 \\0 字符，而 C 字符串不会。如果你的 Go 字符串中有 \\0 字符，当你调用 C.CString() 时，C 代码会从 \\0 字符处截断你的字符串。这往往不会被注意到，但有时文本并不保证不含 null 字符。 C.GoString() 也等价于 strdup()，但与 C.CString() 相反，是把 C 字符串转换为 Go 字符串。你可以用它定义结构体的字段，或者是声明为 C 的 char *（在 Go 中叫 *C.cahr） 的其他变量，抑或其他的一些变量（我们后面会看到）。 C.GoStringN() 等价于 C 的 memmove()，与 C 中普通的字符串函数不同。它把整个 N 长度的 C buffer 复制为一个 Go 字符串，不单独处理 null 字符。再详细点，它也通过复制来实现。如果你有一个定义为 char feild[64] 的结构体的字段，然后调用了 C.GoStringN(&amp;field, 64)，那么你得到的 Go 字符串一定是 64 个字符，字符串的末尾有可能是一串 \\0 字符。 (我认为这是 cgo 文档中的一个 bug。它宣称 GoStringN 的入参是一个 C 的字符串，但实际上很明显不是，因为 C 的字符串不能以 null 字符结束，而 GoStringN 不会在 null 字符处结束处理。) C.GoBytes() 是 C.GoStringN() 的另一个版本，不返回 string 而是返回 []byte。它没有宣称以 C 字符串作为入参，它仅仅是对整个 buffer 做了内存拷贝。 如果你要拷贝的东西不是以 null 字符结尾的 C 字符串，而是固定长度的 memory buffer，那么 C.GoString() 正好能满足需求；它避开了 C 中传统的问题处理不是 C 字符串的 ’string‘。然而，如果你要处理定义为 char field[N] 的结构体字段这种限定长度的 C 字符串时，这些函数都不能满足需求。 传统语义的结构体中固定长度的字符串变量，定义为 char field[N] 的字段，以及“包含一个字符串”等描述，都表示当且仅当字符串有足够空间时以 null 字符结尾，换句话说，字符串最多有 N-1 个字符。如果字符串正好有 N 个字符，那么它不会以 null 字符结尾。这是 C 代码中诸多 bug 的根源，也不是一个好的 API，但我们却摆脱不了这个 API。每次我们遇到这样的字段，文档不会明确告诉你字段的内容并不一定是 null 字符结尾的，你需要自己假设你有这种 API。 C.GoString() 或 C.GoStringN() 都不能正确处理这些字段。使用 GoStringN() 相对来说出错更少；它仅仅返回一个末尾有一串 \\0 字符长度为 N 的 Go 字符串（如果你仅仅是把这些字段打印出来，那么你可能不会留意到；我经常干这种事）。使用有诱惑力的 GoString() 更是引狼入室，因为它内部会对入参做 strlen()；如果字符末尾没有 null 字符，strlen() 会访问越界的内存地址。如果你走运，你得到的 Go 字符串末尾会有大量的垃圾。如果你不走运，你的 Go 程序出现段错误，因为 strlen() 访问了未映射的内存地址。 （总的来说，如果字符串末尾出现了大量垃圾，通常意味着在某处有不含结束符的 C 字符串。） 你需要的是与 C 的 strndup() 等价的 Go 函数，以此来确保复制不超过 N 个字符且在 null 字符处终止。下面是我写的版本，不保证无错误： 12345678func strndup(cs *C.char, len int) string &#123; s := C.GoStringN(cs, C.int(len)) i := strings.IndexByte(s, 0) if i == -1 &#123; return s &#125; return C.GoString(cs)&#125; 由于有 Go 的字符串怎样占用内存的问题，这段代码做了些额外的工作来最小化额外的内存占用。你可能想用另一种方法，返回一个 GoStringN() 字符串的切片。你也可以写复杂的代码，根据 i 和 len 的不同来决定选用哪种方法。 更新：Ian Lance Taylor 给我展示了份更好的代码： 123func strndup(cs *C.char, len int) string &#123; return C.GoStringN(cs, C.int(C.strnlen(cs, C.size_t(len))))&#125; 是的，这里有大量的转换。这篇文章就是你看到的 Go 和 Gco 类型的结合。 via: https://utcc.utoronto.ca/~cks/space/blog/programming/GoCGoStringFunctions 作者：ChrisSiebenmann译者：lxbwolf校对：polaris1119 本文由 GCTT 原创编译，Go 中文网 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"Go 字符串中的潜在问题","slug":"Go-字符串中的潜在问题","date":"2020-01-16T16:12:58.000Z","updated":"2020-05-05T11:38:40.291Z","comments":true,"path":"10e5e8ba/","link":"","permalink":"https://lxb.wiki/10e5e8ba/","excerpt":"","text":"在我之前的文章 Go 中我喜欢的东西中提到过，我喜欢的 Go 的东西其中之一就是它的字符串（通常还有切片）。从一个 Python 开发者的角度看，它们之所以伟大，是因为创建它们时开销很少，因为它们通常不需要复制。在 Python 中，任何时候操作字符串都需要复制一部分或全部字符串，而 这很容易对性能造成影响。想要写高性能的 Python 代码需要谨慎考虑复制的问题。在 Go 中，几乎所有的字符串操作都是不复制的，仅仅是从原字符串取一个子集（例如去除字符串首尾的空白字符），因此你可以更自由地操作字符串。这个机制可以非常直接地解决你的问题，并且非常高效。 （当然，不是所有的字符串操作都不复制。例如，把一个字符串转换成大写需要复制，尽管 Go 中的实现已经足够智能，在不需要改变原字符串时 — 例如由于它已经是一个全大写的字符串 — 可以规避掉复制。） 但是这个优势也带来了潜在的坏处，那些没有开销的子字符串使原来的整个字符串一直存在于内存中。Go 中的字符串（和切片）操作之所以内存开销很少，是因为它们只是底层存储（字符串或切片底层的数组的真实数据）的一些部分的引用；创建一个字符串做的操作就是创建了一个新的引用。但是 Go（目前）不会对字符串数据或数组进行部分的垃圾回收，所以即使它一个很小的 bit 被其它元素引用，整个对象也会一直保持在内存中。换句话说，一个单字符的字符串（目前）足够让一个巨大的字符串不被 GC 回收。 当然，不会有很多人遇到这个问题。为了遇到它，你需要处理一个非常庞大的原字符串，或造成大量的内存消耗（或者两者都做），在这个基础上，你必须创建那些不持久的字符串的持久的小子字符串（好吧，你是多么希望它是非持久的）。很多使用场景不会复现这个问题；要么你的原字符串不够大，要么你的子集获取了大部分原字符串（例如你把原字符串进行了分词处理），要么子字符串生命周期不够长。简而言之，如果你是一个普通的 Go 开发者，你可以忽略这个问题。处理长字符串并且长时间维持原字符串的很小部分的人才会关注这个问题。 （我之所以关注到这个问题，是因为一次我花了大量精力用尽可能少的内存写 Python 程序，尽管它是从一个大的配置文件解析结果然后分块储存。这让我联想到了一些其他的事，如字符串的生命周期、限制字符串只复制一次，等等。然后我用 Go 语言写了一个解析器，这让我由重新考虑了一下这些问题，我意识到由于我的解析器截取出和维持的 bit 一直存在于内存中，从输入文件解析出的庞大字符串也会一直存在与内存中。） 顺便说一下，我认为这是 Go 做了权衡之后的正确结果。大部分使用字符串的开发者不会遇到这个问题，而且截取子字符串开销很小对于开发者来说用处很大。这种低开销的截取操作也减轻了 GC 的负担；当代码使用大量的子字符串截取（像 Python 中那样）时，你只需要处理固定长度的字符串引用就可以了，而不是需要处理长度变化的字符串。 当你的代码遇到这个问题时，当然有明显的解决方法：创建一个函数，通过把字符串转换成 []byte 来 ”最小化“ 字符串，然后返回。这种方法生成了一个最小化的字符串，内存开销是理论上最完美实现的只复制一次，而 Go 现在很容易就可以实现。 附加问题：strings.ToUpper() 等怎样规避没有必要的复制所有的主动转换函数像 ToUpper() 和 ToTitle() 是用 strings.Map() 和 unicode 包 中的函数实现的。Map() 足够智能，在映射的函数返回一个与已存在的 rune 不同的结果之前不会创建新的字符串。因此，你代码中所有类似的直接使用 Map() 的地方都不会有内存开销。 via: https://utcc.utoronto.ca/~cks/space/blog/programming/GoStringsMemoryHolding 作者：Chris Siebenmann译者：lxbwolf校对：校对者ID 本文由 GCTT 原创编译，Go 中文网 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"Go GC 怎么标记内存","slug":"Go-GC-怎么标记内存","date":"2020-01-13T16:06:31.000Z","updated":"2020-05-05T11:37:17.990Z","comments":true,"path":"b3baee92/","link":"","permalink":"https://lxb.wiki/b3baee92/","excerpt":"","text":"ℹ️ 本文基于 Go 1.13。关于内存管理的概念的讨论在我的文章 Go 中的内存管理和分配 中有详细的解释。 Go GC 的作用是回收不再使用的内存。实现的算法是并发的三色标记和清除回收法。本中文，我们研究三色标记法，以及各个颜色的不同用处。 你可以在 Ken Fox 的 解读垃圾回收算法 中了解更多关于不同垃圾回收机制的信息。 标记阶段这个阶段浏览内存来了解哪些块儿是在被我们的代码使用和哪些块儿应该被回收。 然而，因为 GC 和我们的 Go 程序并行，GC 扫描期间内存中某些对象的状态可能被改变，所以需要一个检测这种可能的变化的方法。为了解决这个潜在的问题，实现了 写屏障 算法，GC 可以追踪到任何的指针修改。使写屏障生效的唯一条件是短暂终止程序，又名 “Stop the World”。 在进程启动时，Go 也在每个 processor 起了一个标记 worker 来辅助标记内存。 然后，当 root 被加入到处理队列中后，标记阶段就开始遍历和用颜色标记内存。 为了了解在标记阶段的每一步，我们来看一个简单的程序示例： 1234567891011121314151617181920212223242526272829303132333435type struct1 struct &#123; a, b int64 c, d float64 e *struct2&#125;type struct2 struct &#123; f, g int64 h, i float64&#125;func main() &#123; s1 := allocStruct1() s2 := allocStruct2() func () &#123; _ = allocStruct2() &#125;() runtime.GC() fmt.Printf(\"s1 = %X, s2 = %X\\n\", &amp;s1, &amp;s2)&#125;//go:noinlinefunc allocStruct1() *struct1 &#123; return &amp;struct1&#123; e: allocStruct2(), &#125;&#125;//go:noinlinefunc allocStruct2() *struct2 &#123; return &amp;struct2&#123;&#125;&#125; struct2 不包含指针，因此它被储存在一个专门存放不被其他对象引用的对象的 span 中。 这减少了 GC 的工作，因为标记内存时不需要扫描这个 span。 分配工作结束后，我们的程序强迫 GC 重复前面的步骤。下面是流程图： GC 从栈开始，递归地顺着指针找指针指向的对象，遍历内存。扫描到被标记为 no scan 的 span 时，停止扫描。然而，这个工作是在多个协程中完成的，每个指针被加入到一个 work pool 中的队列。然后，后台运行的标记 worker 从这个 work pool 中拿到前面出列的 work，扫描这个对象然后把在这个对象里找到的指针加入到队列。 颜色标记worker 需要一种记录哪些内存需要扫描的方法。GC 使用一种 三色标记算法，工作流程如下： 开始时，所有对象都被认为是白色 root 对象（栈，堆，全局变量）被标记为灰色 这个初始步骤完成后，GC 会： 选择一个灰色的对象，标记为黑色 追踪这个对象的所有指针，把所有引用的对象标记为灰色 然后，GC 重复以上两步，直到没有对象可被标记。在这一时刻，对象非黑即白，没有灰色。白色的对象表示没有其他对象引用，可以被回收。 下面是前面例子的图示： 初始状态下，所有的对象被认为是白色的。然后，遍历到的且被其他对象引用的对象，被标记为灰色。如果一个对象在被标记为 no scan 的 span 中，因为它不需要被扫描，所以可以标记为黑色。 现在灰色的对象被加入到扫描队列并被标记为黑色： 对加入到扫描队列的所有对象重复做相同的操作，直到没有对象需要被处理： 处理结束时，黑色对象表示内存中在使用的对象，白色对象是要被回收的对象。我们可以看到，由于 struct2 的实例是在一个匿名函数中创建的且不再存在于栈上，因此它是白色的且可以被回收。 归功于每一个 span 中的名为 gcmarkBits 的 bitmap 属性，三色被原生地实现了，bitmap 对 scan 中相应的 bit 设为 1 来追踪 scan。 我们可以看到，黑色和灰色表示的意义相同。处理的不同之处在于，标记为灰色时是把对象加入到扫描队列，而标记为黑色时，不再扫描。 GC 最终 STW，清除每一次写屏障对 work pool 做的改变，继续后续的标记。 你可以在我的文章 Go GC 怎样监控你的应用 中找到关于并发处理和 GC 的标记阶段更详细的描述。 runtime 分析器Go 提供的工具使我们可以对每一步进行可视化，观察 GC 在我们的程序中的影响。开启 tracing 运行我们的代码，可以看到前面所有步骤的一个概览。下面是追踪结果： 标记 worker 的生命周期也可以在追踪结果中以协程等级可视化。下面是在启动之前先在后台等待标记内存的 goroutine #33 的例子。 via: https://medium.com/a-journey-with-go/go-how-does-the-garbage-collector-mark-the-memory-72cfc12c6976 作者：Vincent Blanchon 译者：lxbwolf 校对：校对者ID 本文由 GCTT 原创编译，Go 中文网 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"},{"name":"GC","slug":"GC","permalink":"https://lxb.wiki/tags/GC/"}]},{"title":"微服务中的熔断器和重试","slug":"微服务中的熔断器和重试","date":"2019-12-12T15:02:17.000Z","updated":"2020-05-05T11:41:08.979Z","comments":true,"path":"c9399f4/","link":"","permalink":"https://lxb.wiki/c9399f4/","excerpt":"","text":"今天我们来讨论微服务架构中的自我恢复能力。通常情况下，服务间会通过同步或异步的方式进行通信。我们假定把一个庞大的系统分解成一个个的小块能将各个服务解耦。管理服务内部的通信可能有点困难了。你可能听说过这两个著名的概念：熔断和重试。 熔断器 想象一个简单的场景：用户发出的请求访问服务 A 随后访问另一个服务 B。我们可以称 B 是 A 的依赖服务或下游服务。到服务 B 的请求在到达各个实例前会先通过负载均衡器。 后端服务发生系统错误的原因有很多，例如慢查询、network blip 和内存争用。在这种场景下，如果返回 A 的 response 是 timeout 和 server error，我们的用户会再试一次。在混乱的局面中我们怎样来保护下游服务呢？ 熔断器可以让我们对失败率和资源有更好的控制。熔断器的设计思路是不等待 TCP 的连接 timeout 快速且优雅地处理 error。这种 fail fast 机制会保护下游的那一层。这种机制最重要的部分就是立刻向调用方返回 response。没有被 pending request 填充的线程池，没有 timeout，而且极有可能烦人的调用链中断者会更少。此外，下游服务也有了充足的时间来恢复服务能力。完全杜绝错误很难，但是减小失败的影响范围是有可能的。 通过 hystrix 熔断器，我们可以采用降级方案，对上游返回降级后的结果。例如，服务 B 可以访问一个备份服务或 cache，不再访问原来的服务 C。引入这种降级方案需要集成测试，因为我们在 happy path（译注：所谓 happy path，即测试方法的默认场景，没有异常和错误信息。具体可参见 wikipedia）可能不会遇到这种网络模式。 状态 熔断器有三个主要的状态： Closed：让所有请求都通过的默认状态。在阈值下的请求不管成功还是失败，熔断器的状态都不会改变。可能出现的错误是 Max Concurrency（最大并发数）和 Timeout（超时）。 Open：所有的请求都会返回 Circuit Open 错误并被标记为失败。这是一种不等待处理结束的 timeout 时间的 fail-fast 机制。 Half Open：周期性地向下游服务发出请求，检查它是否已恢复。如果下游服务已恢复，熔断器切换到 Closed 状态，否则熔断器保持 Open 状态。 熔断器原理控制熔断的设置共有 5 个主要参数。 12345678// CommandConfig is used to tune circuit settings at runtimetype CommandConfig struct &#123; Timeout int `json:\"timeout\"` MaxConcurrentRequests int `json:\"max_concurrent_requests\"` RequestVolumeThreshold int `json:\"request_volume_threshold\"` SleepWindow int `json:\"sleep_window\"` ErrorPercentThreshold int `json:\"error_percent_threshold\"`&#125; 查看源码 可以通过根据两个服务的 SLA（‎ Service Level Agreement，服务级别协议）来定出阈值。如果在测试时把依赖的其他服务也涉及到了，这些值会得到很好的调整。 一个好的熔断器的名字应该能精确指出哪个服务连接出了问题。实际上，请求一个服务时可能会有很多个 API endpoint。每一个 endpoint 都应该有一个对应的熔断器。 生产上的熔断器熔断器通常被放在聚合点上。尽管熔断器提供了一种 fail-fast 机制，但我们仍然需要确保可选的降级方案可行。如果我们因为假定需要降级方案的场景出现的可能性很小就不去测试它，那（之前的努力）就是白费力气了。即使在最简单的演练中，我们也要确保阈值是有意义的。以我的个人经验，把参数配置在 log 中 print 出来对于 debug 很有帮助。 Demo这段实例代码用的是 hystrix-go 库，hystrix Netflix 库在 Golang 的实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package mainimport ( \"errors\" \"fmt\" \"log\" \"net/http\" \"os\" \"github.com/afex/hystrix-go/hystrix\")const commandName = \"producer_api\"func main() &#123; hystrix.ConfigureCommand(commandName, hystrix.CommandConfig&#123; Timeout: 500, MaxConcurrentRequests: 100, ErrorPercentThreshold: 50, RequestVolumeThreshold: 3, SleepWindow: 1000, &#125;) http.HandleFunc(\"/\", logger(handle)) log.Println(\"listening on :8080\") http.ListenAndServe(\":8080\", nil)&#125;func handle(w http.ResponseWriter, r *http.Request) &#123; output := make(chan bool, 1) errors := hystrix.Go(commandName, func() error &#123; // talk to other services err := callChargeProducerAPI() // err := callWithRetryV1() if err == nil &#123; output &lt;- true &#125; return err &#125;, nil) select &#123; case out := &lt;-output: // success log.Printf(\"success %v\", out) case err := &lt;-errors: // failure log.Printf(\"failed %s\", err) &#125;&#125;// logger is Handler wrapper function for loggingfunc logger(fn http.HandlerFunc) http.HandlerFunc &#123; return func(w http.ResponseWriter, r *http.Request) &#123; log.Println(r.URL.Path, r.Method) fn(w, r) &#125;&#125;func callChargeProducerAPI() error &#123; fmt.Println(os.Getenv(\"SERVER_ERROR\")) if os.Getenv(\"SERVER_ERROR\") == \"1\" &#123; return errors.New(\"503 error\") &#125; return nil&#125; demo 中分别测试了请求调用链 closed 和 open 两种情况： 12345678910111213/* Experiment 1: success path */// servergo run main.go// clientfor i in $(seq 10); do curl -x '' localhost:8080 ;done/* Experiment 2: circuit open */// serverSERVER_ERROR=1 Go run main.go// clientfor i in $(seq 10); do curl -x '' localhost:8080 ;done 查看源码 重试问题在上面的熔断器模式中，如果服务 B 缩容，会发生什么？大量已经从 A 发出的请求会返回 5xx error。可能会触发熔断器切换到 open 的错误报警。因此我们需要重试以防间歇性的 network hiccup 发生。 一段简单的重试代码示例： 123456789101112131415package mainfunc callWithRetryV1() (err error) &#123; for index := 0; index &lt; 3; index++ &#123; // call producer API err := callChargeProducerAPI() if err != nil &#123; return err &#125; &#125; // adding backoff // adding jitter return nil&#125; 查看源码 重试模式为了实现乐观锁，我们可以为不同的服务配置不同的重试次数。因为立即重试会对下游服务产生爆发性的请求，所以不能用立即重试。加一个 backoff 时间可以缓解下游服务的压力。一些其他的模式会用一个随机的 backoff 时间（或在等待时加 jitter）。 一起来看下列算法： Exponential: bash * 2attemp Full Jitter: sleep = rand(0, base * 2attempt) Equal Jitter: temp = base * 2attemp; sleep = temp/2+rand(0, temp/2) De-corredlated Jitter: sleep = rand(base, sleep*3) 【译注】关于这几个算法，可以参考这篇文章 。Full Jitter、 Equal Jitter、 De-corredlated 等都是原作者自己定义的名词。 客户端的数量与服务端的总负载和处理完成时间是有关联的。为了确定什么样的重试模式最适合你的系统，在客户端数量增加时很有必要运行基准测试。详细的实验过程可以在这篇文章中看到。我建议的算法是 de-corredlated Jitter 和 full jitter 选择其中一个。 两者结合 熔断器被广泛用在无状态线上事务系统中，尤其是在聚合点上。重试应该用于调度作业或不被 timeout 约束的 worker。经过深思熟虑后我们可以同时用熔断器和重试。在大型系统中，service mesh 是一种能更精确地编排不同配置的理想架构。 参考文章 https://github.com/afex/hystrix-go/ https://github.com/eapache/go-resiliency https://github.com/Netflix/Hystrix/wiki https://www.awsarchitectureblog.com/2015/03/backoff.html https://dzone.com/articles/go-microservices-part-11-hystrix-and-resilience via: https://scene-si.org/2019/12/01/introduction-to-protobuf-messages/ 作者：Tit Petric 译者：lxbwolf 校对：polaris1119 本文由 GCTT 原创编译，Go语言中文网 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"协程，操作系统线程和 CPU 管理","slug":"协程，操作系统线程和-CPU-管理","date":"2019-12-12T15:01:35.000Z","updated":"2020-05-05T11:40:54.072Z","comments":true,"path":"4e717bb5/","link":"","permalink":"https://lxb.wiki/4e717bb5/","excerpt":"","text":"ℹ️ 本文运行环境为 Go 1.13 对于一个程序来说，从内存和性能角度讲创建一个 OS 线程或切换线程花费巨大。Go 志在极尽所能地充分利用内核资源。从第一天开始，它就是为并发而生的。 M, P, G 编排为了解决这个问题，Go 有它自己的在线程间调度协程的调度器。这个调度器定义了三个主要概念，如源码中解释的这样： 12345The main concepts are:G - goroutine.M - worker thread, or machine.P - processor, a resource that is required to execute Go code. M must have an associated P to execute Go code[...]. P, M, G 模型图解： 每个协程（G）运行在与一个逻辑 CPU（P）相关联的 OS 线程（M）上。我们一起通过一个简单的示例来看 Go 是怎么管理他们的： 12345678910111213141516func main() &#123; var wg sync.WaitGroup wg.Add(2) go func() &#123; println(`hello`) wg.Done() &#125;() go func() &#123; println(`world`) wg.Done() &#125;() wg.Wait()&#125; 首先，Go 根据机器逻辑 CPU 的个数来创建不同的 P，并且把它们保存在一个空闲 P 的 list 里。 然后，为了更好地工作新创建的已经准备好的协程会唤醒一个 P。这个 P 通过与之相关联的 OS 线程来创建一个 M： 然而，像 P 那样，系统调用返回的甚至被 gc 强行停止的空闲的 M — 比如没有协程在等待运行 — 也会被加到一个空闲 list： 在程序启动阶段，Go 就已经创建了一些 OS 线程并与 M 想关联了。在我们的例子中，打印 hello 的第一个协程会使用主协程，第二个会从这个空闲 list 中获取一个 M 和 P： 现在我们已经掌握了协程和线程管理的基本要义，来一起看看什么情形下 Go 会用比 P 多的 M，在系统调用时怎么管理协程。 系统调用Go 会优化系统调用 — 无论阻塞与否 — 通过运行时封装他们。封装的那一层会把 P 和线程 M 分离，并且可以让另一个线程在它上面运行。我们拿文件读取举例： 123456789func main() &#123; buf := make([]byte, 0, 2) fd, _ := os.Open(\"number.txt\") fd.Read(buf) fd.Close() println(string(buf)) // 42&#125; 文件读取的流程如下： P0 现在在空闲 list 中，有可能被唤醒。当系统调用 exit 时，Go 会遵守下面的规则，直到有一个命中了。 尝试去捕获相同的 P，在我们的例子中就是 P0，然后 resume 执行过程 尝试从空闲 list 中捕获一个 P，然后 resume 执行过程 把协程放到全局队列里，把与之相关联的 M 放回空闲 list 去 然而，在像 http 请求等 non-blocking I/O 情形下，Go 在资源没有准备好时也会处理请求。在这种情形下，第一个系统调用 — 遵循上述流程图 — 由于资源还没有准备好所以不会成功，（这样就）迫使 Go 使用 network poller 并使协程停驻。请看示例： 123func main() &#123; http.Get(`https://httpstat.us/200`)&#125; 当第一个系统调用完成且显式地声明了资源还没有准备好，协程会在 network poller 通知它资源准备就绪之前一直处于停驻状态。在这种情形下，线程 M 不会阻塞： 在 Go 调度器在等待信息时协程会再次运行。调度器在获取到等待的信息后会询问 network poller 是否有协程在等待被运行。 如果多个协程都准备好了，只有一个会被运行，其他的会被加到全局的可运行队列中，以备后续的调度。 OS 线程方面的限制在系统调用中，Go 不会限制可阻塞的 OS 线程数，源码中有解释： The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. There is no limit to the number of threads that can be blocked in system calls on behalf of Go code; those do not count against the GOMAXPROCS limit. This package’s GOMAXPROCS function queries and changes the limit. 译注：GOMAXPROCS 变量表示可同时运行用户级 Go 代码的操作系统线程的最大数量。系统调用中可被阻塞的最大线程数并没有限制；可被阻塞的线程数对 GOMAXPROCS 没有影响。这个包的 GOMAXPROCS 函数查询和修改这个最大数限制。 对这种情形举例： 123456789101112131415func main() &#123; var wg sync.WaitGroup for i := 0;i &lt; 100 ;i++ &#123; wg.Add(1) go func() &#123; http.Get(`https://httpstat.us/200?sleep=10000`) wg.Done() &#125;() &#125; wg.Wait()&#125; 利用追踪工具得到的线程数如下： 由于 Go 优化了线程使用，所以当协程阻塞时，它仍可复用，这就解释了为什么图中的数跟示例代码循环中的数不一致。 via: https://medium.com/a-journey-with-go/go-goroutine-os-thread-and-cpu-management-2f5a5eaf518a 作者：Vincent Blanchon 译者：lxbwolf 校对：polaris1119 本文由 GCTT 原创编译，Go语言中文网 荣誉推出","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"Symbol Names of Keyboard","slug":"Symbol-Names-of-Keyboard","date":"2019-12-07T02:22:50.000Z","updated":"2019-12-07T02:25:27.232Z","comments":true,"path":"2d9f52fc/","link":"","permalink":"https://lxb.wiki/2d9f52fc/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536符号 英文名 中文名~ tilde or swung dash 波浪字符或代字号! exclamation mark 惊叹号@ at sign or commercial at 爱特或小老鼠# number sign 井号$ dollar sign 美元符% percent sign 百分号^ caret 脱字符&amp; ampersand 与和符* asterisk 星号() parentheses, round brackets, soft brackets, or circle brackets 小括号，圆括号[] brackets (US), square brackets, closed brackets or hard brackets 中括号，方括号&#123;&#125; braces (UK and US), French brackets, curly brackets 大括号，花括号&lt;&gt; angle brackets or chevrons 尖括号_ underscore 下划线+ plus sign 加号− minus sign 减号= equals sign 等号&lt; less-than sign 小于号&gt; greater-than sign 大于号. period, full stop or dot 句号，点, comma 逗号: colon 冒号; semicolon 分号? question mark 问号- hyphen 连字符... ellipsis 省略号– dash 破折号/ slash, forward slash 斜线\\ backslash 反斜线| vertical bar 竖线“ quotation mark 双引号‘ apostrophe 单引号，省略符号","categories":[{"name":"Tools","slug":"Tools","permalink":"https://lxb.wiki/categories/Tools/"}],"tags":[{"name":"symbol","slug":"symbol","permalink":"https://lxb.wiki/tags/symbol/"}]},{"title":"go匿名函数和闭包","slug":"go匿名函数和闭包","date":"2019-11-17T06:38:20.000Z","updated":"2020-05-05T11:39:48.212Z","comments":true,"path":"e2c91def/","link":"","permalink":"https://lxb.wiki/e2c91def/","excerpt":"","text":"函数变量(函数值)在 Go 语言中，函数被看作是第一类值，这意味着函数像变量一样，有类型、有值，其他普通变量能做的事它也可以。 123func square(x int) &#123; println(x * x)&#125; 直接调用：square(1) 把函数当成变量一样赋值：s := square；接着可以调用这个函数变量：s(1)。 注意：这里 square 后面没有圆括号，调用才有。 调用 nil 的函数变量会导致 panic。 函数变量的零值是 nil，这意味着它可以跟 nil 比较，但两个函数变量之间不能比较。 匿名函数作用: 在go语言中目前了解的作用就是用于构成闭包 闭包闭包通过引用的方式使用外部函数的变量函数与 与其(直接)相关的环境形成闭包 简单来说: 因为把返回的函数赋给了一个变量, 虽然函数在执行完一瞬间会销毁其执行环境, 但是如果有闭包的话, 闭包会保存外部函数的活动对象(变量), 所以如果不对闭包的引用消除掉, 闭包会一直存在内存中, 垃圾收集器不会销毁闭包占用的内存 实例112345678910//函数A是一个不带参数，返回值是一个匿名函数，且该函数//带有一个int类型参数，返回值为一个int类型func A() func(int) int &#123; sum := 0 return func(bb int) int &#123; sum += bb fmt.Println(&quot;bb=&quot;, bb, &quot;\\tsum=&quot;, sum) return sum &#125;&#125; 调用1: 12345678910func main() &#123; a := A()//定义变量a,并将函数A的返回值赋给a // 这个时候, 虽然有小括号, 但是func A()还未真正执行, 只是赋值给了变量a b := a(4) //真正执行func A() fmt.Println(b)&#125;/*** 输出： ** bb= 4 sum= 4** 4*/ 调用2 123456789101112func main() &#123; a := A() a(0) a(1) a(5)&#125;/*** 输出：** bb= 0 sum= 0** bb= 1 sum= 1** bb= 5 sum= 6*/ 以上调用通过闭包实现了sum的累加 调用3 123456789101112131415func main() &#123; a := A() c := A() a(0) a(5) c(10) c(20)&#125;/*** 输出：** bb= 0 sum= 0** bb= 5 sum= 5** bb= 10 sum= 10** bb= 20 sum= 30 */ 可以看出，上例中调用了两次函数A，构成了两个闭包，这两个闭包维护的变量sum不是同一个变量。 实例212345678910111213141516171819202122func B() []func() &#123; b := make([]func(), 3, 3) for i := 0; i &lt; 3; i++ &#123; b[i] = func() &#123; fmt.Println(i) &#125; &#125; return b&#125;func main() &#123; c := B() // 这个时候并未真正执行函数, 只是定义, 所以不会print c[0]() // 这个时候真正执行, 但是由于闭包, c[0] 中拿的i的引用 c[1]() c[2]()&#125;/*** 输出：** 3** 3** 3*/ 闭包通过引用的方式使用外部函数的变量。 上例中只调用了一次函数B,构成一个闭包(func() {fmt.Println(i)} 与它的环境func B() []func(){} 构成闭包)，i 在外部函数B中定义，所以闭包维护该变量 i ，c[0]、c[1]、c[2]中的 i 都是闭包中 i 的引用。 因此执行c:=B()后，i 的值已经变为3，故再调用c0时的输出是3而不是0。 可作如下修改： 123456789101112131415161718192021222324func B() []func() &#123; b := make([]func(), 3, 3) for i := 0; i &lt; 3; i++ &#123; b[i] = (func(j int) func() &#123; return func() &#123; fmt.Println(j) &#125; &#125;)(i) // 这个地方的小括号是真正执行了 &#125; return b&#125;func main() &#123; c := B() c[0]() c[1]() c[2]()&#125;/*** 输出：** 0** 1** 2*/ 函数func() {fmt.Println(j)} 与它的环境func(j int) func() {} 构成闭包, 变量i(实参) 并没有在它的环境范围内, 且 j是形参以上修改可能没有什么实际意义，此处仅为说明问题使用。 在使用defer的时候可能出现类似问题，需要注意： 12345678910for j := 0; j &lt; 2; j++ &#123; defer (func() &#123; fmt.Println(j) &#125;)()&#125;/*** 输出： ** 2 ** 2*/ 实例3:1234567func incr() func() int &#123; var x int return func() int &#123; x++ return x &#125;&#125; 调用这个函数会返回一个函数变量。i := incr() : 通过把这个函数变量赋值给i, i 就成为了一个闭包所以i 保存着对x 的引用, 可以想象i 中有着一个指针指向x 或者 i 中有x 的地址 由于i 有着指向x 的指针, 所以可以修改x , 且保持着状态: 123println(i()) // 1println(i()) // 2println(i()) // 3 也就是说, x 逃逸了, 它的声明周期没有随着它的作用域结束而结束但是这段代码却不会递增： 123println(incr()()) // 1println(incr()()) // 1println(incr()()) // 1 这是因为这里调用了三次 incr()，返回了三个闭包，这三个闭包引用着三个不同的 x，它们的状态是各自独立的。 实例4: 闭包引用产生的问题1234567x := 1f := func() &#123; println(x)&#125;x = 2x = 3f() // 3 因为闭包对外层词法域变量是引用的，所以这段代码会输出 3。可以想象 f 中保存着 x 的地址，它使用 x 时会直接解引用，所以 x 的值改变了会导致 f 解引用得到的值也会改变。但是，这段代码却会输出 1： 123456x := 1func() &#123; println(x) // 1&#125;()x = 2x = 3 这是因为 f 调用时就已经解引用取值了，这之后的修改就与它无关了。 不过如果再次调用 f 还是会输出 3，这也再一次证明了 f 中保存着 x 的地址。可以通过在闭包内外打印所引用变量的地址来证明： 12345x := 1func() &#123; println(&amp;x) // 0xc0000de790&#125;()println(&amp;x) // 0xc0000de790 可以看到引用的是同一个地址。 实例5.1: 循环闭包引用12345for i := 0; i &lt; 3; i++ &#123; func() &#123; println(i) // 0, 1, 2 &#125;()&#125; 这段代码相当于： 123456for i := 0; i &lt; 3; i++ &#123; f := func() &#123; println(i) // 0, 1, 2 &#125; f()&#125; 每次迭代后都对 i 进行了解引用并使用得到的值且不再使用，所以这段代码会正常输出。 实例5.2正常代码：输出 0, 1, 2： 1234var dummy [3]intfor i := 0; i &lt; len(dummy); i++ &#123; println(i) // 0, 1, 2&#125; 然而这段代码会输出 3： 12345678var dummy [3]intvar f func()for i := 0; i &lt; len(dummy); i++ &#123; f = func() &#123; println(i) &#125;&#125;f() // 3 这个地方i最后的值是3, 而不是2, 因为只有i的值是3时, 才会跳出循环 实例5.312345678910var funcSlice []func()for i := 0; i &lt; 3; i++ &#123; funcSlice = append(funcSlice, func() &#123; println(i) &#125;)&#125;for j := 0; j &lt; 3; j++ &#123; funcSlice[j]() // 3, 3, 3&#125; 为了解决上面这种情况, 可以声明新的匿名函数并传参: 123456789101112var funcSlice []func()for i := 0; i &lt; 3; i++ &#123; func(k int) &#123; funcSlice = append(funcSlice, func() &#123; println(k) &#125;) &#125;(i)&#125;for j := 0; j &lt; 3; j++ &#123; funcSlice[j]() // 0, 1, 2&#125; 现在 println(k) 使用的 k 是通过函数参数传递进来的，并且 Go 语言的函数参数是按值传递的。(把k换成i也没有问题, 即使它与for条件的中的i 和func的入参i 重名也能正常运行) 所以相当于在这个新的匿名函数内声明了三个变量，被三个闭包函数独立引用。原理跟第一种方法是一样的。 这里的解决方法可以用在大多数跟闭包引用有关的问题上，不局限于第三个例子。","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"合并区间56","slug":"合并区间56","date":"2019-11-06T15:31:03.000Z","updated":"2020-05-05T11:45:38.293Z","comments":true,"path":"a2b71d73/","link":"","permalink":"https://lxb.wiki/a2b71d73/","excerpt":"","text":"给出一个区间的集合，请合并所有重叠的区间。 示例 1: 123输入: [[1,3],[2,6],[8,10],[15,18]]输出: [[1,6],[8,10],[15,18]]解释: 区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6]. 示例 2: 123输入: [[1,4],[4,5]]输出: [[1,5]]解释: 区间 [1,4] 和 [4,5] 可被视为重叠区间。 思路: 怎么判断重叠: 两区间的最小的右边界 大于或等于 两区间最大的左边界. 如[1,5]和[2,8] 入参是切片的切片(intervals), 拿intervals[0]与它后面的所有区间对比, 从intervals[1]开始, 如果有与之重叠的区间, 就把合并后的新区间赋给intervals[0], 并删除参与合并的那个旧区间 intervals[0]完成后, 拿``intervals[1]与它后边的所有区间对比, 从intervals[2]开始, 如果有与之重叠的区间, 就把合并后的新区间赋给intervals[1]`, 并删除参与合并的那个就区间 拿intervals[i] 与它后边的所有区间对比, 从intervals[i+1] 开始, 如果有与之重叠的区间intervals[j] , 就把合并后的新区间赋给 intervals[i] , 并删除参与合并的intervals[j] 如果第3步出现了有重叠的区间intervals[j], 那么合并后i 的值变了, 就有可能由原来 在i 到j 之间没有重叠的区间 变成 有重叠的区间, 所以需要从头(i+1) 再遍历一次, 直到再也没有重叠的区间 重复, 一直到切片末尾 code 12345678910111213141516171819202122232425262728293031323334353637383940func min(x, y int) int &#123; if x &lt; y &#123; return x &#125; return y&#125;func max(x, y int) int &#123; if x &gt; y &#123; return x &#125; return y&#125;func merge(intervals [][]int) [][]int &#123; for i := 0; i &lt; len(intervals); &#123; merged := false for j := i + 1; j &lt; len(intervals); j++ &#123; x, y := intervals[i], intervals[j] if min(x[1], y[1]) &gt;= max(x[0], y[0]) &#123; merged = true //重新赋值 intervals[i][0], intervals[i][1] = min(x[0], y[0]), max(x[1], y[1]) //删除j intervals[j] = intervals[len(intervals) - 1] intervals = intervals[:len(intervals) - 1] &#125; &#125; if merged &#123; continue &#125; i++ &#125; return intervals&#125;","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://lxb.wiki/categories/Algorithm/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"},{"name":"算法","slug":"算法","permalink":"https://lxb.wiki/tags/算法/"}]},{"title":"Golang的反射","slug":"Golang的反射","date":"2019-11-03T13:21:59.000Z","updated":"2020-05-05T11:38:55.811Z","comments":true,"path":"1d3c1f0e/","link":"","permalink":"https://lxb.wiki/1d3c1f0e/","excerpt":"","text":"编程语言中反射的概念在计算机科学领域，反射是指一类应用，它们能够自描述和自控制。也就是说，这类应用通过采用某种机制来实现对自己行为的描述（self-representation）和监测（examination），并能根据自身行为的状态和结果，调整或修改应用所描述行为的状态和相关的语义。 每种语言的反射模型都不同，并且有些语言根本不支持反射。Golang语言实现了反射，反射机制就是在运行时动态的调用对象的方法和属性，官方自带的reflect包就是反射相关的，只要包含这个包就可以使用。 Golang的gRPC也是通过反射实现的。 interface 和反射先来看看Golang关于类型设计的一些原则 变量包括（value, type）两部分 理解这一点就知道为什么nil != nil了 type 包括 static type和concrete type. 简单来说 static type是你在编码是看见的类型(如int、string)，concrete type是runtime系统看见的类型 类型断言能否成功，取决于变量的concrete type，而不是static type. 因此，一个 reader变量如果它的concrete type也实现了write方法的话，它也可以被类型断言为writer. 反射，就是建立在类型之上的，Golang的指定类型的变量的类型是静态的（也就是指定int、string这些的变量，它的type是static type），在创建变量的时候就已经确定，反射主要与Golang的interface类型相关（它的type是concrete type），只有interface类型才有反射一说。 在Golang的实现中，每个interface变量都有一个对应pair，pair中记录了实际变量的值和类型: (value, type) value是实际变量值，type是实际变量的类型。一个interface{}类型的变量包含了2个指针，一个指针指向值的类型【对应concrete type】，另外一个指针指向实际的值【对应value】。 例如，创建类型为*os.File的变量，然后将其赋给一个接口变量r： 1234tty, err := os.OpenFile(&quot;/dev/tty&quot;, os.O_RDWR, 0)var r io.Readerr = tty 接口变量r的pair中将记录如下信息：(tty, *os.File)，这个pair在接口变量的连续赋值过程中是不变的，将接口变量r赋给另一个接口变量w: 12var w io.Writerw = r.(io.Writer) 接口变量w的pair与r的pair相同，都是:(tty, *os.File)，即使w是空接口类型，pair也是不变的。 interface及其pair的存在，是Golang中实现反射的前提，理解了pair，就更容易理解反射。反射就是用来检测存储在接口变量内部(值value；类型concrete type) pair对的一种机制。 reflect 基本功能TypeOf和ValueOf既然反射就是用来检测存储在接口变量内部(值value；类型concrete type) pair对的一种机制。那么在Golang的reflect反射包中有什么样的方式可以让我们直接获取到变量内部的信息呢？ 它提供了两种类型（或者说两个方法）让我们可以很容易的访问接口变量内容，分别是reflect.ValueOf() 和 reflect.TypeOf()，看看官方的解释 123456789101112// ValueOf returns a new Value initialized to the concrete value// stored in the interface i. ValueOf(nil) returns the zero func ValueOf(i interface&#123;&#125;) Value &#123;...&#125;翻译一下：ValueOf用来获取输入参数接口中的数据的值，如果接口为空则返回0// TypeOf returns the reflection Type that represents the dynamic type of i.// If i is a nil interface value, TypeOf returns nil.func TypeOf(i interface&#123;&#125;) Type &#123;...&#125;翻译一下：TypeOf用来动态获取输入参数接口中的值的类型，如果接口为空则返回nil reflect.TypeOf()是获取pair中的type，reflect.ValueOf()获取pair中的value，示例如下： 1234567891011121314151617package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() &#123; var num float64 = 1.2345 fmt.Println(&quot;type: &quot;, reflect.TypeOf(num)) fmt.Println(&quot;value: &quot;, reflect.ValueOf(num))&#125;运行结果:type: float64value: 1.2345 说明 reflect.TypeOf： 直接给到了我们想要的type类型，如float64、int、各种pointer、struct 等等真实的类型 reflect.ValueOf：直接给到了我们想要的具体的值，如1.2345这个具体数值，或者类似&amp;{1 “Allen.Wu” 25} 这样的结构体struct的值 也就是说明反射可以将“接口类型变量”转换为“反射类型对象”，反射类型指的是reflect.Type和reflect.Value这两种 从relfect.Value中获取接口interface的信息当执行reflect.ValueOf(interface)之后，就得到了一个类型为”relfect.Value”变量，可以通过它本身的Interface()方法获得接口变量的真实内容，然后可以通过类型判断进行转换，转换为原有真实类型。不过，我们可能是已知原有类型，也有可能是未知原有类型，因此，下面分两种情况进行说明。 已知原有类型【进行“强制转换”】 已知类型后转换为其对应的类型的做法如下，直接通过Interface方法然后强制转换，如下 1realValue := value.Interface().(已知的类型) 示例: 1234567891011121314151617181920212223242526package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() &#123; var num float64 = 1.2345 pointer := reflect.ValueOf(&amp;num) value := reflect.ValueOf(num) // 可以理解为“强制转换”，但是需要注意的时候，转换的时候，如果转换的类型不完全符合，则直接panic // Golang 对类型要求非常严格，类型一定要完全符合 // 如下两个，一个是*float64，一个是float64，如果弄混，则会panic convertPointer := pointer.Interface().(*float64) convertValue := value.Interface().(float64) fmt.Println(convertPointer) fmt.Println(convertValue)&#125;运行结果：0xc42000e2381.2345 说明 转换的时候，如果转换的类型不完全符合，则直接panic，类型要求非常严格！ 转换的时候，要区分是指针还是指 也就是说反射可以将“反射类型对象”再重新转换为“接口类型变量” 未知原有类型【遍历探测其Filed】 很多情况下，我们可能并不知道其具体类型，那么这个时候，该如何做呢？需要我们进行遍历探测其Filed来得知，示例如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)type User struct &#123; Id int Name string Age int&#125;func (u User) ReflectCallFunc() &#123; fmt.Println(&quot;Allen.Wu ReflectCallFunc&quot;)&#125;func main() &#123; user := User&#123;1, &quot;Allen.Wu&quot;, 25&#125; DoFiledAndMethod(user)&#125;// 通过接口来获取任意参数，然后一一揭晓func DoFiledAndMethod(input interface&#123;&#125;) &#123; getType := reflect.TypeOf(input) fmt.Println(&quot;get Type is :&quot;, getType.Name()) getValue := reflect.ValueOf(input) fmt.Println(&quot;get all Fields is:&quot;, getValue) // 获取方法字段 // 1. 先获取interface的reflect.Type，然后通过NumField进行遍历 // 2. 再通过reflect.Type的Field获取其Field // 3. 最后通过Field的Interface()得到对应的value for i := 0; i &lt; getType.NumField(); i++ &#123; field := getType.Field(i) value := getValue.Field(i).Interface() fmt.Printf(&quot;%s: %v = %v\\n&quot;, field.Name, field.Type, value) &#125; // 获取方法 // 1. 先获取interface的reflect.Type，然后通过.NumMethod进行遍历 for i := 0; i &lt; getType.NumMethod(); i++ &#123; m := getType.Method(i) fmt.Printf(&quot;%s: %v\\n&quot;, m.Name, m.Type) &#125;&#125;运行结果：get Type is : Userget all Fields is: &#123;1 Allen.Wu 25&#125;Id: int = 1Name: string = Allen.WuAge: int = 25ReflectCallFunc: func(main.User) 说明通过运行结果可以得知获取未知类型的interface的具体变量及其类型的步骤为： 先获取interface的reflect.Type，然后通过NumField进行遍历 再通过reflect.Type的Field获取其Field 最后通过Field的Interface()得到对应的value 通过运行结果可以得知获取未知类型的interface的所属方法（函数）的步骤为： 先获取interface的reflect.Type，然后通过NumMethod进行遍历 再分别通过reflect.Type的Method获取对应的真实的方法（函数） 最后对结果取其Name和Type得知具体的方法名 也就是说反射可以将“反射类型对象”再重新转换为“接口类型变量” struct 或者 struct 的嵌套都是一样的判断处理方式 通过reflect.Value设置实际变量的值 reflect.Value是通过reflect.ValueOf(X)获得的，只有当X是指针的时候，才可以通过reflec.Value修改实际变量X的值，即：要修改反射类型的对象就一定要保证其值是“addressable”的。 示例如下： 12345678910111213141516171819202122232425262728293031323334package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() &#123; var num float64 = 1.2345 fmt.Println(&quot;old value of pointer:&quot;, num) // 通过reflect.ValueOf获取num中的reflect.Value，注意，参数必须是指针才能修改其值 pointer := reflect.ValueOf(&amp;num) newValue := pointer.Elem() fmt.Println(&quot;type of pointer:&quot;, newValue.Type()) fmt.Println(&quot;settability of pointer:&quot;, newValue.CanSet()) // 重新赋值 newValue.SetFloat(77) fmt.Println(&quot;new value of pointer:&quot;, num) //////////////////// // 如果reflect.ValueOf的参数不是指针，会如何？ pointer = reflect.ValueOf(num) //newValue = pointer.Elem() // 如果非指针，这里直接panic，“panic: reflect: call of reflect.Value.Elem on float64 Value”&#125;运行结果：old value of pointer: 1.2345type of pointer: float64settability of pointer: truenew value of pointer: 77 说明 需要传入的参数是* float64这个指针，然后可以通过pointer.Elem()去获取所指向的Value，注意一定要是指针。 如果传入的参数不是指针，而是变量，那么 通过Elem获取原始值对应的对象则直接panic 通过CanSet方法查询是否可以设置返回false newValue.CantSet()表示是否可以重新设置其值，如果输出的是true则可修改，否则不能修改，修改完之后再进行打印发现真的已经修改了。 reflect.Value.Elem() 表示获取原始值对应的反射对象，只有原始对象才能修改，当前反射对象是不能修改的 也就是说如果要修改反射类型对象，其值必须是“addressable”【对应的要传入的是指针，同时要通过Elem方法获取原始值对应的反射对象】 struct 或者 struct 的嵌套都是一样的判断处理方式 通过reflect.ValueOf来进行方法的调用 这算是一个高级用法了，前面我们只说到对类型、变量的几种反射的用法，包括如何获取其值、其类型、如果重新设置新值。但是在工程应用中，另外一个常用并且属于高级的用法，就是通过reflect来进行方法【函数】的调用。比如我们要做框架工程的时候，需要可以随意扩展方法，或者说用户可以自定义方法，那么我们通过什么手段来扩展让用户能够自定义呢？关键点在于用户的自定义方法是未可知的，因此我们可以通过reflect来搞定 示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)type User struct &#123; Id int Name string Age int&#125;func (u User) ReflectCallFuncHasArgs(name string, age int) &#123; fmt.Println(&quot;ReflectCallFuncHasArgs name: &quot;, name, &quot;, age:&quot;, age, &quot;and origal User.Name:&quot;, u.Name)&#125;func (u User) ReflectCallFuncNoArgs() &#123; fmt.Println(&quot;ReflectCallFuncNoArgs&quot;)&#125;// 如何通过反射来进行方法的调用？// 本来可以用u.ReflectCallFuncXXX直接调用的，但是如果要通过反射，那么首先要将方法注册，也就是MethodByName，然后通过反射调动mv.Callfunc main() &#123; user := User&#123;1, &quot;Allen.Wu&quot;, 25&#125; // 1. 要通过反射来调用起对应的方法，必须要先通过reflect.ValueOf(interface)来获取到reflect.Value，得到“反射类型对象”后才能做下一步处理 getValue := reflect.ValueOf(user) // 一定要指定参数为正确的方法名 // 2. 先看看带有参数的调用方法 methodValue := getValue.MethodByName(&quot;ReflectCallFuncHasArgs&quot;) args := []reflect.Value&#123;reflect.ValueOf(&quot;wudebao&quot;), reflect.ValueOf(30)&#125; methodValue.Call(args) // 一定要指定参数为正确的方法名 // 3. 再看看无参数的调用方法 methodValue = getValue.MethodByName(&quot;ReflectCallFuncNoArgs&quot;) args = make([]reflect.Value, 0) methodValue.Call(args)&#125;运行结果：ReflectCallFuncHasArgs name: wudebao , age: 30 and origal User.Name: Allen.WuReflectCallFuncNoArgs 说明 要通过反射来调用起对应的方法，必须要先通过reflect.ValueOf(interface)来获取到reflect.Value，得到“反射类型对象”后才能做下一步处理 reflect.Value.MethodByName这.MethodByName，需要指定准确真实的方法名字，如果错误将直接panic，MethodByName返回一个函数值对应的reflect.Value方法的名字。 []reflect.Value，这个是最终需要调用的方法的参数，可以没有或者一个或者多个，根据实际参数来定。 reflect.Value的 Call 这个方法，这个方法将最终调用真实的方法，参数务必保持一致，如果reflect.Value’Kind不是一个方法，那么将直接panic。 本来可以用u.ReflectCallFuncXXX直接调用的，但是如果要通过反射，那么首先要将方法注册，也就是MethodByName，然后通过反射调用methodValue.Call golang的反射reflect性能 Golang的反射很慢，这个和它的API设计有关。在 java 里面，我们一般使用反射都是这样来弄的。 123Field field = clazz.getField(&quot;hello&quot;);field.get(obj1);field.get(obj2); 这个取得的反射对象类型是 java.lang.reflect.Field。它是可以复用的。只要传入不同的obj，就可以取得这个obj上对应的 field。 但是Golang的反射不是这样设计的: 12type_ := reflect.TypeOf(obj)field, _ := type_.FieldByName(&quot;hello&quot;) 这里取出来的 field 对象是 reflect.StructField 类型，但是它没有办法用来取得对应对象上的值。如果要取值，得用另外一套对object，而不是type的反射 12type_ := reflect.ValueOf(obj)fieldValue := type_.FieldByName(&quot;hello&quot;) 这里取出来的 fieldValue 类型是 reflect.Value，它是一个具体的值，而不是一个可复用的反射对象了，每次反射都需要malloc这个reflect.Value结构体，并且还涉及到GC。 Golang reflect慢主要有两个原因 涉及到内存分配以及后续的GC； reflect实现里面有大量的枚举，也就是for循环，比如类型之类的。 总结上述详细说明了Golang的反射reflect的各种功能和用法，都附带有相应的示例，相信能够在工程应用中进行相应实践，总结一下就是： 反射可以大大提高程序的灵活性，使得interface{}有更大的发挥余地 反射必须结合interface才玩得转 变量的type要是concrete type的（也就是interface变量）才有反射一说 反射可以将“接口类型变量”转换为“反射类型对象” 反射使用 TypeOf 和 ValueOf 函数从接口中获取目标对象信息 反射可以将“反射类型对象”转换为“接口类型变量 reflect.value.Interface().(已知的类型) 遍历reflect.Type的Field获取其Field 反射可以修改反射类型对象，但是其值必须是“addressable” 想要利用反射修改对象状态，前提是 interface.data 是 settable,即 pointer-interface 通过反射可以“动态”调用方法 因为Golang本身不支持模板，因此在以往需要使用模板的场景下往往就需要使用反射(reflect)来实现","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"}]},{"title":"环形链表142","slug":"环形链表142","date":"2019-10-30T16:09:51.000Z","updated":"2020-05-05T11:45:23.229Z","comments":true,"path":"fb68a62c/","link":"","permalink":"https://lxb.wiki/fb68a62c/","excerpt":"","text":"给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 说明：不允许修改给定的链表。 解题关键是理解 非环部分的长度与相遇点到环起点那部分环的长度 是相等的 这个数学关系 假设非环部分长度为x, 从环起点到相遇点的长度为y , 环的长度为c 慢指针(slow)走过的长度可以表示为``ds = x + n1 * c + y, 快指针(fast) 的速度是慢指针的两倍, 意味着 快指针走过的长度为df = 2(x + n1 * c + y)` 还有一个约束是, fast 走过的路程一定比slow走的路程多出环长度的整数倍(记为n2 * c) 所以 123df - ds = n2 * c2(x + n1 * c + y) - (x + n1 * c + y) = n2 * cx + n1 * c + y = n2 * c 解读下第三步的等式: 非环部分的长度 + 环起点到相遇点之间的长度 就是环的整数倍 意味着, 当以环的起点为原点时, 已经走过y(即前面从环起点到相遇点的长度)的前提下, 如果再走x , 就刚好走了很多圈(n2 * c). *”很多圈” 的意思, 就是从原点再到原点, 终点的位置和起点的位置重合.* 怎么才能再走x呢? 让一个指针从head 开始走, 另一个指针从相遇点开始走, 等这两个指针相遇, 就是走了x. 如果不能理解为何相遇恰好就在上面说的原点处, 应该反复琢磨斜体”很多圈”那句话 code 1234567891011121314151617func detectCycle(head *ListNode) *ListNode &#123; fast, slow := head, head for fast != nil &amp;&amp; fast.Next != nil &amp;&amp; fast.Next.Next != nil &#123; slow = slow.Next fast = fast.Next.Next if fast == slow &#123; fast = head for fast != slow &#123; fast = fast.Next slow = slow.Next &#125; return fast &#125; &#125; return nil&#125;","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://lxb.wiki/categories/Algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://lxb.wiki/tags/算法/"},{"name":"链表","slug":"链表","permalink":"https://lxb.wiki/tags/链表/"}]},{"title":"如何使用GitHub搜索","slug":"如果使用Github搜索","date":"2019-10-28T15:03:30.000Z","updated":"2020-05-05T11:38:16.692Z","comments":true,"path":"a813d59a/","link":"","permalink":"https://lxb.wiki/a813d59a/","excerpt":"","text":"1. 明确搜索仓库标题、仓库描述、README in:name 关键词 如果想查找描述的内容，可以使用这样的方式： in:descripton 关键词 这里就是搜索上面项目描述的内容。 一般项目，都会有个README文件，如果要查该文件包含特定关键词的话 in:readme 关键词 2. 明确搜索 star、fork 数大于多少的 一个项目 star 数的多少，一般代表该项目有受欢迎程度。虽然现在也有垃圾项目刷 star ，但毕竟是少数， star 依然是个不错的衡量标准。 stars:&gt; 数字 关键字 比如要找 star 数大于 3000 的Spring Cloud 仓库，就可以这样 stars:&gt;3000 spring cloud 如果不加 &gt;= 的话，是要精确找 star 数等于具体数字的，这个一般有点困难。 如果要找在指定数字区间的话，使用 stars: 10..20 关键词 fork 数同理，将上面的 stars 换成 fork，其它语法相同 3. 明确搜索仓库大小的 比如只想看个简单的 Demo，不想找特别复杂的且占用磁盘空间较多的，可以在搜索的时候直接限定仓库的 size 。 使用方式： size:&gt;=5000 关键词 这里注意下，这个数字代表K, 5000代表着5M。 4. 明确仓库是否还在更新维护 我们在确认是否要使用一些开源产品，框架的时候，是否继续维护是很重要的一点。如果已经过时没人维护的东西，踩了坑就不好办了。而在 GitHub 上找项目的时候，不再需要每个都点到项目里看看最近 push 的时间，直接在搜索框即可完成。 元旦刚过，比如咱们要找临近年底依然在勤快更新的项目，就可以直接指定更新时间在哪个时间前或后的 通过这样一条搜索 pushed:&gt;2019-01-03 spring cloud 就找到了1月3号之后，还在更新的项目 想找指定时间之前或之后创建的仓库也是可以的，把 pushed 改成 created 就行。 5. 明确搜索仓库的 LICENSE 经常使用开源软件，一定都知道，开源软件也是分不同的「门派」不同的LICENSE。开源不等于一切免费，不同的许可证要求也大不相同。 2018年就出现了 Facebook 修改 React 的许可协议导致各个公司纷纷修改自己的代码，寻找替换的框架。 例如要找协议是最为宽松的 Apache License 2 的代码，可以这样 license:apache-2.0 spring cloud 其它协议就把 apache-2.0 替换一下即可，比如换成 mit 之类的。 6. 明确搜索仓库的语言 比如咱们就找 Java 的库， 除了像上面在左侧点击选择之外，还可以在搜索中过滤。像这样： language:java 关键词 7.明确搜索某个人或组织的仓库 user:joshlong 组合使用一下，把 Java 项目过滤出来，多个查询之间「空格」分隔即可。 user:joshlong language:java 找某个组织的代码话，可以这样： org:spring-cloud 就可以列出具体org 的仓库。","categories":[{"name":"Tools","slug":"Tools","permalink":"https://lxb.wiki/categories/Tools/"}],"tags":[{"name":"GitHub","slug":"GitHub","permalink":"https://lxb.wiki/tags/GitHub/"}]},{"title":"删除排序数组中的重复项","slug":"删除排序数组中的重复项","date":"2019-10-23T14:50:21.000Z","updated":"2019-10-30T16:10:24.866Z","comments":true,"path":"6a0b412d/","link":"","permalink":"https://lxb.wiki/6a0b412d/","excerpt":"","text":"leetcode-26 给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 1234567891011给定数组 nums = [1,1,2], 函数应该返回新的长度 2, 并且原数组 nums 的前两个元素被修改为 1, 2。 你不需要考虑数组中超出新长度后面的元素。给定 nums = [0,0,1,1,1,2,2,3,3,4],函数应该返回新的长度 5, 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4。你不需要考虑数组中超出新长度后面的元素。 理解题意: 1. 当给定数组为空时, 返回0 2. 不能引入其他数组空间, 即不能再使用一个新的数组来存放结果 3. 最终结果不重复, 整体思路是把数组后面的几个元素挪到前面去, 用后面的元素覆盖掉前面重复了的元素, 保持数组的长度始终不变. 数组中超出新长度(去重后的长度) 后的元素无视用快慢指针的思路解答: 给定两个游标 left和right 当给定数组的下标为left和right的值相等时, 就不管 当不相等时, 做一个操作: 把当前right的值赋给left的下一个坐标 code: 12345678910111213141516171819func removeDuplicates(nums []int) int &#123; //如果是空切片，那就返回0 if len(nums) == 0 &#123; return 0 &#125; //用两个标记来比较相邻位置的值 //当一样的话，那就不管继续 //当不一样的时候，就把right指向的值赋值给left下一位 left, right := 0, 1 for ; right &lt; len(nums); right++ &#123; if nums[left] == nums[right] &#123; continue &#125; left++ nums[left] = nums[right] &#125; fmt.Println(nums[:left+1]) return left + 1&#125;","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://lxb.wiki/categories/Algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://lxb.wiki/tags/算法/"}]},{"title":"Golang编译器漏洞和标准库设计失误","slug":"Golang编译器漏洞和标准库设计失误","date":"2019-10-20T03:29:37.000Z","updated":"2020-05-05T11:39:10.277Z","comments":true,"path":"4fe063a2/","link":"","permalink":"https://lxb.wiki/4fe063a2/","excerpt":"","text":"字节切片（byte slice）相关的编译器漏洞和标准库设计失误 假如一个类型MyByte定义如下，如何将一个[]MyByte切片值和一个[]byte切片值互相转换为对方的类型？ 123456789101112package maintype MyByte bytevar ( x = []byte&#123;1, 2, 3&#125; y = []MyByte&#123;1, 2, 3&#125;)func main() &#123; x = []byte(y) // error: 非法的转换 y = []MyByte(x) // error: 非法的转换&#125; 如上例所示，在Go中，这两个类型的值之间的类型转换是非法的。因为Go规定两个切片只有在它们的类型的底层类型（underlying type）相同的情况下才能转换到对方的类型。而一个非定义类型（undefined type）的底层类型为此非定义类型本身。类型[]MyByte和[]byte均为非定义类型，所以它们的底层类型不同，从而它们的值也就不能转换到对方的类型。 难道真没有办法实现它们之间的转换了？有，而且有好几种。第一种方法是使用类型非安全指针来实现双向转换，另外两种方法只能实现单向转换。另外的这两种方法要么利用了编译器的漏洞，要么利用了reflect标准库包的设计失误。 使用类型非安全指针的实现。 12345678910111213141516package mainimport &quot;unsafe&quot;type MyByte bytevar ( x = []byte&#123;1, 2, 3&#125; y = []MyByte&#123;1, 2, 3&#125;)func main() &#123; p := unsafe.Pointer(&amp;y) x = *(*[]byte)(p) x[0] = 99 println(y[0]) // 99&#125; 在使用类型非安全指针的实现中，转换结果和原切片共享底层元素。 利用标准编译器的bug 我们可以将一个[]byte切片值转换为string, 再把string转换为类型[]MyByte。转换结果和原切片不共享底层元素。 1234567891011121314package maintype MyByte bytevar ( x = []byte&#123;1, 2, 3&#125; y = []MyByte&#123;1, 2, 3&#125;)func main() &#123; // 下一行利用了编译器漏洞 y = []MyByte(string(x)) y[0] = 99 println(x[0]) // 1&#125; Go白皮书提到一个字节切片可以转换为一个字符串，反之亦然。但是什么是字节切片类型呢？底层类型为[]byte的切片类型还是元素类型的底层类型为byte的切片类型？如果字节切片类型定义为元素类型的底层类型为byte的切片类型，则[]MyByte和[]byte都可称为字节切片类型。如果字节切片类型定义为底层类型为[]byte的切片类型，则只有[]byte可以被称为字节切片类型。我们认为标准编译器采纳了底层类型为[]byte的切片类型才称为字节切片这一定义，因为下面这个程序使用标准编译器是编译不过的。 12345678910package maintype MyByte bytevar ( x = []byte&#123;1, 2, 3&#125; y = []MyByte&#123;1, 2, 3&#125;)func main() &#123; _ = string(y) // error: 非法转换&#125; 但是，标准编译器（v1.12）却认为转换[]MyByte(“abc”)是合法的。这显然是一个漏洞。 对于码点切片（rune slice）和字符串之间的转换，同样的情况也存在。 对于gccgo编译器来说，此漏洞是对称的，因而更严重。此更严重的漏洞使得上述两种类型的值之间的转换是双向有效的。比如，下面这段代码使用gccgo（v8.0）编译是没问题的。 12345678910111213141516171819package maintype MyByte bytevar ( x = []byte&#123;1, 2, 3&#125; y = []MyByte&#123;1, 2, 3&#125;)func main() &#123; // 下一行利用了编译器漏洞 y = []MyByte(string(x)) y[0] = 99 println(x[0]) // 1 // 下一行利用了编译器漏洞 x = []byte(string(y)) x[0] = 127 println(y[0]) // 99&#125; 事实上，gccgo编译器在内置copy和append函数的实现中也存在着同样的漏洞。 1234567891011121314151617181920package maintype MyByte bytevar ( x = []byte&#123;1, 2, 3&#125; y = []MyByte&#123;1, 2, 3&#125;)func main() &#123; // 下一行利用了编译器漏洞 copy(y, string(x)) y[0] = 99 println(x[0]) // 1 // 下一行利用了编译器漏洞 y = append([]MyByte(nil), string(x)...) y[0] = 99 println(x[0]) // 1&#125; 第三种方法利用了reflect标准库包的设计失误。此失误导致将[]MyByte值单向转换为类型[]byte是可行的，虽然这违反了Go类型系统确定的规则。使用第三种方法得到的结果切片和原切片共享底层元素。 12345678910111213141516package mainimport &quot;reflect&quot;type MyByte bytevar ( x = []byte&#123;1, 2, 3&#125; y = []MyByte&#123;1, 2, 3&#125;)func main() &#123; v := reflect.ValueOf(y) x = v.Bytes() x[0] = 99 println(y[0]) // 99&#125;","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://lxb.wiki/tags/Go/"},{"name":"编译器","slug":"编译器","permalink":"https://lxb.wiki/tags/编译器/"}]},{"title":"分布式锁高并发优化","slug":"分布式锁高并发优化","date":"2019-10-18T14:38:39.000Z","updated":"2019-10-18T14:38:40.582Z","comments":true,"path":"7db296fb/","link":"","permalink":"https://lxb.wiki/7db296fb/","excerpt":"","text":"问题场景:假如下单时，用分布式锁来防止库存超卖，但是是每秒上千订单的高并发场景，如何对分布式锁进行高并发优化来应对这个场景？ 库存超卖现象是怎么产生的？ 假设订单系统部署两台机器上，不同的用户都要同时买10台iphone，分别发了一个请求给订单系统。接着每个订单系统实例都去数据库里查了一下，当前iphone库存是12台于是乎，每个订单系统实例都发送SQL到数据库里下单，然后扣减了10个库存，其中一个将库存从12台扣减为2台，另外一个将库存从2台扣减为-8台 用分布式锁如何解决库存超卖问题？分布式锁的实现原理:同一个锁key，同一时间只能有一个客户端拿到锁，其他客户端会陷入无限的等待来尝试获取那个锁，只有获取到锁的客户端才能执行下面的业务逻辑。 从上图可以看到，只有一个订单系统实例可以成功加分布式锁，然后只有他一个实例可以查库存、判断库存是否充足、下单扣减库存，接着释放锁。 释放锁之后，另外一个订单系统实例才能加锁，接着查库存，一下发现库存只有2台了，库存不足，无法购买，下单失败。不会将库存扣减为-8的 分布式锁的方案在高并发场景下分布式锁一旦加了之后，对同一个商品的下单请求，会导致所有客户端都必须对同一个商品的库存锁key进行加锁。 比如，对iphone这个商品的下单，都必对“iphone_stock”这个锁key来加锁。这样会导致对同一个商品的下单请求，就必须串行化，一个接一个的处理。 假设加锁之后，释放锁之前，查库存 -&gt; 创建订单 -&gt; 扣减库存，这个过程性能很高吧，算他全过程20毫秒，这应该不错了。 那么1秒是1000毫秒，只能容纳50个对这个商品的请求依次串行完成处理。 比如一秒钟来50个请求，都是对iphone下单的，那么每个请求处理20毫秒，一个一个来，最后1000毫秒正好处理完50个请求。 所以, 能看出来简单的使用分布式锁来处理库存超卖问题，存在的缺陷就是同一个商品多用户同时下单的时候，会基于分布式锁串行化处理，导致没法同时处理同一个商品的大量下单的请求。 这种方案，要是应对那种低并发、无秒杀场景的普通小电商系统，可能还可以接受。 因为如果并发量很低，每秒就不到10个请求，没有瞬时高并发秒杀单个商品的场景的话，其实也很少会对同一个商品在一秒内瞬间下1000个订单，因为小电商系统没那场景。 如何对分布式锁进行高并发优化？现在按照刚才的计算，你一秒钟只能处理针对iphone的50个订单。 其实说出来也很简单，相信很多人看过java里的ConcurrentHashMap的源码和底层原理，应该知道里面的核心思路，就是分段加锁！ 把数据分成很多个段，每个段是一个单独的锁，所以多个线程过来并发修改数据的时候，可以并发的修改不同段的数据。不至于说，同一时间只能有一个线程独占修改ConcurrentHashMap中的数据。 另外，Java 8中新增了一个LongAdder类，也是针对Java 7以前的AtomicLong进行的优化，解决的是CAS类操作在高并发场景下，使用乐观锁思路，会导致大量线程长时间重复循环。 LongAdder中也是采用了类似的分段CAS操作，失败则自动迁移到下一个分段进行CAS的思路。 其实分布式锁的优化思路也是类似的，之前我们是在另外一个业务场景下落地了这个方案到生产中，不是在库存超卖问题里用的。 但是库存超卖这个业务场景不错，很容易理解，所以我们就用这个场景来说一下。 其实这就是分段加锁。你想，假如你现在iphone有1000个库存，那么你完全可以给拆成20个库存段，要是你愿意，可以在数据库的表里建20个库存字段，比如stock_01，stock_02，类似这样的，也可以在redis之类的地方放20个库存key。 总之，就是把你的1000件库存给他拆开，每个库存段是50件库存，比如stock_01对应50件库存，stock_02对应50件库存。 接着，每秒1000个请求过来了，好！此时其实可以是自己写一个简单的随机算法，每个请求都是随机在20个分段库存里，选择一个进行加锁。 这样就好了，同时可以有最多20个下单请求一起执行，每个下单请求锁了一个库存分段，然后在业务逻辑里面，就对数据库或者是Redis中的那个分段库存进行操作即可，包括查库存 -&gt; 判断库存是否充足 -&gt; 扣减库存。 这相当于什么呢？相当于一个20毫秒，可以并发处理掉20个下单请求，那么1秒，也就可以依次处理掉20 * 50 = 1000个对iphone的下单请求了。 一旦对某个数据做了分段处理之后，有一个坑大家一定要注意：如果某个下单请求，咔嚓加锁，然后发现这个分段库存里的库存不足了，此时咋办？ 这时你得自动释放锁，然后立马换下一个分段库存，再次尝试加锁后尝试处理。这个过程一定要实现 分布式锁并发优化方案有没有什么不足？","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://lxb.wiki/tags/redis/"},{"name":"优化","slug":"优化","permalink":"https://lxb.wiki/tags/优化/"},{"name":"分布式","slug":"分布式","permalink":"https://lxb.wiki/tags/分布式/"}]},{"title":"CentOS释放被占用端口","slug":"CentOS释放被占用端口","date":"2019-10-09T16:36:59.000Z","updated":"2019-10-09T16:36:59.584Z","comments":true,"path":"2d1cc7f9/","link":"","permalink":"https://lxb.wiki/2d1cc7f9/","excerpt":"","text":"场景:在前面的某些操作中, 启动某进程时, 监听8080 和 443 端口, 后进程关闭, 这两个端口却一直处于占用状态, 导致后面再起进程想监听这两个端口时, 启动报错 1.输入netstat -tln,查看系统当前所有被占用端口,主要是为了查看你的端口是否真正的被占用着,搭建可以看到我的9001,和9002端口都已经被占用了,所以我需要释放这两个端口 2.根据端口查询进程,输入lsof -i :9001,切记不要忘了添加冒号,如下图,就可以看到当前被占用的端口的进程的进程编号 kill 掉PID 再netstat -tln 确认下, 然后就可以起进程了","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[{"name":"进程","slug":"进程","permalink":"https://lxb.wiki/tags/进程/"},{"name":"端口","slug":"端口","permalink":"https://lxb.wiki/tags/端口/"}]},{"title":"网易云音乐破版权","slug":"网易云音乐破版权","date":"2019-10-08T10:07:13.000Z","updated":"2019-10-09T17:30:16.320Z","comments":true,"path":"c6996379/","link":"","permalink":"https://lxb.wiki/c6996379/","excerpt":"","text":"源码地址: https://github.com/lxbwolf/UnblockNeteaseMusic 原理:使用其它音乐平台的歌曲替换网易云音乐无版权歌曲。目前备用的平台有：网易云旧链 、QQ 、 虾米 、 百度 、酷狗 、酷我 、咕咪 、JOOX 音源替换变灰歌曲链接 (默认仅启用前四)。 1、打开网易云音乐客户端的时候，客户端不再直接访问网易云服务器而是访问UnblockNeteaseMusic服务。 2、UnblockNeteaseMusic收到客户端的请求后，透传给网易云音乐的服务器，并再拿到相关的数据后进行检查，如果发现其中的歌曲没有版权，那么去其它平台查询此歌曲的相关信息。 3、将查到的数据返回给网易云客户端。 4、至此完成网易云音乐的解锁。 整个流程要解决两个重要的问题。 核心工作:1、将UnblockNeteaseMusic部署到服务器。可以是本地服务器也可以是云服务器。 2、为网易云客户端设置代理，以达到访问UnblockNeteaseMusic项目的目的。 部署服务部分 安装node.js git clone https://github.com/lxbwolf/UnblockNeteaseMusic.git 在UnblockNeteaseMusic 目录下, 执行npx @nondanee/unblockneteasemusic(官方) 或者 用docker 启动docker run nondanee/unblockneteasemusic &amp;&amp; docker-compose up, 还有另一种方式:在UnblockNeteaseMusic 目录下, 执行node app.js -p 8080:443 -f 59.111.160.195其中59.111.160.195 这个地址是通过ping music.163.com 测出来的 正常情况下, 服务端启动进程, 客户端配置好IP Port, 就可以用了, 此时服务端接收到请求会有log, 如果服务端log一直卡在 12HTTP Server running @ http://0.0.0.0:8080HTTPS Server running @ https://0.0.0.0:443 说明客户端的请求并没有打到服务器上, 可能原因是8080和443端口还没有开启 配置参数 1234567891011121314151617$ unblockneteasemusic -husage: unblockneteasemusic [-v] [-p port] [-a address] [-u url] [-f host] [-o source [source ...]] [-t token] [-e url] [-s] [-h]optional arguments: -v, --version output the version number -p port, --port port specify server port -a address, --address address specify server host -u url, --proxy-url url request through upstream proxy -f host, --force-host host force the netease server ip -o source [source ...], --match-order source [source ...] set priority of sources -t token, --token token set up proxy authentication -e url, --endpoint url replace virtual endpoint with public host -s, --strict enable proxy limitation -h, --help output usage information 客户端配置 源码中的README 有详细说明 平台 基础设置 Windows 设置 &gt; 工具 &gt; 自定义代理 (客户端内) UWP Windows 设置 &gt; 网络和 Internet &gt; 代理 Linux 系统设置 &gt; 网络 &gt; 网络代理 macOS 系统偏好设置 &gt; 网络 &gt; 高级 &gt; 代理 Android WLAN &gt; 修改网络 &gt; 高级选项 &gt; 代理 iOS 无线局域网 &gt; HTTP 代理 &gt; 配置代理 Android 手机详细配置: 设置 &gt; WLAN &gt; 修改网络 &gt; 高级选项 &gt; 代理 12IP: 106.13.86.198Port: 8080 破解前效果 破解后效果","categories":[{"name":"Tools","slug":"Tools","permalink":"https://lxb.wiki/categories/Tools/"}],"tags":[{"name":"破解","slug":"破解","permalink":"https://lxb.wiki/tags/破解/"}]},{"title":"Go闭包技术","slug":"Go闭包技术","date":"2019-10-04T11:25:07.000Z","updated":"2019-10-04T11:25:07.539Z","comments":true,"path":"eb01d7dc/","link":"","permalink":"https://lxb.wiki/eb01d7dc/","excerpt":"","text":"斐波那契数列(Fibonacci sequence),又称黄金分割数列 .因数学家列昂纳多·斐波那契(Leonardoda Fibonacci)以兔子繁殖为例子而引入,故又称为“兔子数列”,指的是这样一个数列: 1、1、2、3、5、8、13、21、34、……在数学上,斐波那契数列以如下被以递推的方法定义: F(1)=1，F(2)=1, F(n)=F(n-1)+F(n-2)（n&gt;=3，n∈N*） 斐波那契数列就是形如 1 1 2 3 5 8 13 21 34 55 的递增数列,从第三项开始起,当前项是前两项之和.为了计算方便,定义两个变量 a,b 表示前两项,初始值分别设置成 0,1 ,示例: 1234// 0 1 1 2 3 5 8 13 21 34 55// a b// a ba, b := 0, 1 初始化后下一轮移动,a, b = b, a+b 结果是 a , b = 1 , 1,刚好能够表示斐波那契数列的开头. 123456func fibonacciByNormal() &#123; a, b := 0, 1 a, b = b, a+b fmt.Print(a, &quot; &quot;) fmt.Println()&#125; 但是上述示例只能生成斐波那契数列中的第一个数字,假如我们需要前十个数列,又该如何? 12345678func fibonacciByNormal() &#123; a, b := 0, 1 for i := 0; i &lt; 10; i++ &#123; a, b = b, a+b fmt.Print(a, &quot; &quot;) &#125; fmt.Println()&#125; 通过指定循环次数再稍加修改上述单数列代码,现在就可以生成前十位数列: 1234// 1 1 2 3 5 8 13 21 34 55func TestFibonacciByNormal(t *testing.T) &#123; fibonacciByNormal()&#125; 这种做法是接触闭包概念前我们一直在采用的解决方案,相信稍微有一定编程经验的开发者都能实现,但是闭包却提供了另一种思路! 12345678// 1 1 2 3 5 8 13 21 34 55func fibonacci() func() int &#123; a, b := 0, 1 return func() int &#123; a, b = b, a+b return a &#125;&#125; 不论是普通函数还是闭包函数,实现斐波那契数列生成器函数的逻辑不变,只是实现不同,闭包返回的是内部函数,留给使用者继续调用而普通函数是直接生成斐波那契数列. 12345678// 1 1 2 3 5 8 13 21 34 55func TestFibonacci(t *testing.T) &#123; f := fibonacci() for i := 0; i &lt; 10; i++ &#123; fmt.Print(f(), &quot; &quot;) &#125; fmt.Println()&#125; 对于这种函数内部嵌套另一个函数并且内部函数引用了外部变量的这种实现方式,称之为”闭包”! 闭包自带独立的运行环境,每一次运行闭包的环境都是相互独立的,正如面向对象中类和对象实例化的关系那样,闭包是类,闭包的引用是实例化对象. 1234567func autoIncrease() func() int &#123; i := 0 return func() int &#123; i = i + 1 return i &#125;&#125; 上述示例是闭包实现的计算器自增,每一次引用 autoIncrease 函数获得的闭包环境都是彼此独立的,直接上单元测试用例. 12345678func TestAutoIncrease(t *testing.T) &#123; a := autoIncrease() // 1 2 3 t.Log(a(), a(), a()) b := autoIncrease() // 1 2 3 t.Log(b(), b(), b())&#125; 函数引用 a 和 b 的环境是独立的,相当于另一个一模一样计数器重新开始计数,并不会影响原来的计数器的运行结果. 普通函数内部定义的变量寿命有限,函数运行结束后也就被系统销毁了,结束了自己短暂而又光荣的一生. 但是,闭包所引用的变量却不一样,只要一直处于使用中状态,那么变量就会”长生不老”,并不会因为出身于函数内就和普通变量拥有一样的短暂人生. 1234567891011121314func fightWithHorse() func() int &#123; horseShowTime := 0 return func() int &#123; horseShowTime++ fmt.Printf(&quot;(%d)祖国需要我,我就提枪上马立即战斗!\\n&quot;,horseShowTime) return horseShowTime &#125;&#125;func TestFightWithHorse(t *testing.T) &#123; f := fightWithHorse() // 1 2 3 t.Log(f(), f(), f())&#125; 凡事有利必有弊,闭包不死则引用变量不灭,如果不理解变量长生不老的特性,编写闭包函数时可能一不小心就掉进作用域陷阱了,千万要小心!下面以绑定循环变量为例讲解闭包作用域的陷阱,示例如下: 123456789func countByClosureButWrong() []func() int &#123; var arr []func() int for i := 1; i &lt;= 3; i++ &#123; arr = append(arr, func() int &#123; return i &#125;) &#125; return arr&#125; countByClosureButWrong 闭包函数引用的自由变量不仅有 arr 数组还有循环变量 i ,函数的整体逻辑是: 闭包函数内部维护一个函数数组,保存的函数主要返回了循环变量. 123456func TestCountByClosure(t *testing.T) &#123; // 4 4 4 for _, c := range countByClosureButWrong() &#123; t.Log(c()) &#125;&#125; 当我们运行 countByClosureButWrong 函数获得闭包返回的函数数组 arr,然后通过 range 关键字进行遍历数组,得到正在遍历的函数项 c. 当我们运行 c() 时,期望输出的 1,2,3 循环变量的值,但是实际结果却是 4,4,4. 原因仍然是变量长生不老的特性:遍历循环时绑定的变量值肯定是 1,2,3,但是循环变量 i 却没有像普通函数那样消亡而是一直长生不老,所以变量的引用发生变化了! 长生不老的循环变量的值刚好是当初循环的终止条件 i=4,只要运行闭包函数,不论是数组中的哪一项函数引用的都是相同的变量 i,所以全部都是 4,4,4. 既然是变量引用出现问题,那么解决起来就很简单了,不用变量引用就好了嘛! 最简单的做法就是使用短暂的临时变量 n 暂存起来正在遍历的值,闭包内引用的变量不再是 i 而是临时变量 n. 123456789101112func countByClosureButWrong() []func() int &#123; var arr []func() int for i := 1; i &lt;= 3; i++ &#123; n := i fmt.Printf(&quot;for i=%d n=%d \\n&quot;, i,n) arr = append(arr, func() int &#123; fmt.Printf(&quot;append i=%d n=%d\\n&quot;, i, n) return n &#125;) &#125; return arr&#125; 上述解决办法很简单就是采用临时变量绑定循环变量的值,而不是原来的长生不老的变量引用,但是这种做法不够优雅,还可以继续简化进行版本升级. 既然是采用变量赋值的做法,是不是和参数传递中的值传递很相像?那我们就可以用值传递的方式重新复制一份变量的值传递给闭包函数. 12345678910111213func countByClosureWithOk() []func() int &#123; var arr []func() int for i := 1; i &lt;= 3; i++ &#123; fmt.Printf(&quot;for i=%d \\n&quot;, i) func(n int) &#123; arr = append(arr, func() int &#123; fmt.Printf(&quot;append n=%d \\n&quot;, n) return n &#125;) &#125;(i) &#125; return arr&#125; 采用匿名函数进行值传递进行改造后,我们再次运行测试用例验证一下改造结果: 123456func TestCountByClosureWithOk(t *testing.T) &#123; // 1 2 3 for _, c := range countByClosureWithOk() &#123; t.Log(c()) &#125;&#125; 模拟类和对象的关系,也可以实现封装,具备一定面向对象能力 每次调用闭包函数所处的环境都是相互独立的,这种特性类似于面向对象中类和实例化对象的关系. 缓存复杂逻辑,常驻内存,避免滥用全局变量徒增维护成本. 长生不老的特性使得闭包引用变量可以常驻内存,用于缓存一些复杂逻辑代码非常合适,避免了原来的全局变量的滥用. 实现闭包成本较高,同时也增加了理解难度. 普通函数转变成闭包函数不仅实现起来有一定难度,而且理解起来也不容易,不仅要求多测试几遍还要理解闭包的特性. 滥用容易占用过多内存,可能造成内存泄漏. 过多使用闭包势必造成引用变量一直常驻内存,如果出现循环引用或者垃圾回收不及时有可能造成内存泄漏问题.","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[]},{"title":"github 博客绑定域名","slug":"github-博客绑定域名","date":"2019-09-26T16:54:15.000Z","updated":"2019-10-03T16:52:39.427Z","comments":true,"path":"e9fbcad9/","link":"","permalink":"https://lxb.wiki/e9fbcad9/","excerpt":"","text":"某篇文章说, CNAME 解析只支持 www 不支持@所以@ 只能 解析到一个一个的 IP 1. source 添加 CNAME 文件在源码的source 目录下, 添加一个CNAME文件文件内容为 1lxb.wiki 2. DNS 设置 记录类型 主机记录 解析路线(isp) 记录值 MX优先级 TTL 状态 操作 CNAME www 默认 lxbwolf.github.io – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.108.153 – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.111.153 – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.110.153 – 10 分钟 正常 修改暂停删除备注 A @ 默认 185.199.109.153 – 10 分钟 正常 修改暂停删除备注 3. hexo 部署1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"博客","slug":"博客","permalink":"https://lxb.wiki/tags/博客/"}]},{"title":"Hello Hexo","slug":"hello-hexo","date":"2019-09-26T16:14:22.211Z","updated":"2019-10-03T07:57:15.349Z","comments":true,"path":"a1751c09/","link":"","permalink":"https://lxb.wiki/a1751c09/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"xargs","slug":"xargs","date":"2019-08-19T17:14:58.000Z","updated":"2019-10-03T07:57:15.416Z","comments":true,"path":"38dfadad/","link":"","permalink":"https://lxb.wiki/38dfadad/","excerpt":"","text":"xargs 是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。 xargs 可以将管道或标准输入（stdin）数据转换成命令行参数，也能够从文件的输出中读取数据。 xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。 xargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。 xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。 之所以能用到这个命令，关键是由于很多命令不支持|管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令 例如: 12find /sbin -perm 700 |ls -l #这个命令是错误的find /sbin -perm 700 |xargs ls -l #这样才是正确的 命令格式somecommand |xargs -item command 重要参数: -i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给 {}，可以用 {} 代替。 其他参数: -a file 从文件中读入作为sdtin -e flag ，注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。 -p 当每次执行一个argument的时候询问一次用户。 -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。 -t 表示先打印命令，然后再执行。 -r no-run-if-empty 当xargs的输入为空的时候则停止xargs，不用再去执行了。 -s num 命令行的最大字符数，指的是 xargs 后面那个命令的最大命令行字符数。 -L num 从标准输入一次读取 num 行送给 command 命令。 -l 同 -L。 -d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。 -x exit的意思，主要是配合-s使用。。 -P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。 实例:1. 多行变成单行1234567# cat test.txta b c d e f gh i j k l m no p qr s tu v w x y z 12# cat test.txt | xargsa b c d e f g h i j k l m n o p q r s t u v w x y z 2. 一次使用n个参数1234567891011# cat test.txt | xargs -n3a b cd e fg h ij k lm n op q rs t uv w xy z 3. d选项指定分隔符123# echo &quot;nameXnameXnameXname&quot; | xargs -dXname name name name 结合-n 选项使用 1234# echo &quot;nameXnameXnameXname&quot; | xargs -dX -n2name namename name 4. I选项的使用4.1 获取参数并替换{}假设一个命令为 sk.sh 和一个保存参数的文件 arg.txt： 1234#!/bin/bash#sk.sh命令内容，打印出所有参数。echo $* arg.txt.文件内容 12345# cat arg.txtaaabbbccc xargs 的一个选项 -I，使用 -I 指定一个替换字符串 {}，这个字符串在 xargs 扩展时会被替换掉，当 -I 与 xargs 结合使用，每一个参数命令都会被执行一次： 12345# cat arg.txt | xargs -I &#123;&#125; ./sk.sh sombefore &#123;&#125; someaftersombefore aaa someaftersombefore bbb someaftersombefore ccc someafter 4.2 复制文件实例复制所有图片文件到 /data/images 目录下： 1ls *.jpg | xargs -n1 -I &#123;&#125; cp &#123;&#125; /data/images/ 4.3 xargs 结合find 使用用 rm 删除太多的文件时候，可能得到一个错误信息：/bin/rm Argument list too long. 用 xargs 去避免这个问题： find . -type f -name &quot;*.log&quot; -print0 | xargs -0 rm -f xargs -0 将 \\0 作为定界符。 统计一个源代码目录中所有 php 文件的行数： find . -type f -name &quot;*.php&quot; -print0 | xargs -0 wc -l 查找所有的 jpg 文件，并且压缩它们： find . -type f -name &quot;*.jpg&quot; -print | xargs tar -czvf images.tar.gz 4.4 下载多个文件假如你有一个文件包含了很多你希望下载的 URL，你能够使用 xargs下载所有链接： # cat url-list.txt | xargs wget -c","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"升级https","slug":"升级https","date":"2019-08-07T18:53:34.000Z","updated":"2019-10-03T16:44:30.044Z","comments":true,"path":"ddf7de45/","link":"","permalink":"https://lxb.wiki/ddf7de45/","excerpt":"","text":"环境 CentOS nginx获取证书HTTPS 证书分三类：1. DV 域名验证证书 2. OV 组织机构验证证书 3. EV 增强的组织机构验证证书。每类证书的审核要求不同，在浏览器地址栏也会有区分，对于个人网站而言，使用免费的 DV 证书就足够了。 我使用了大名鼎鼎的 Let’s Encrypt 来生成证书。 1. 安装 certbotcertbot 是 Let’s Encrypt 提供的一套自动化工具。 yum install epel-release yum install certbot2. 生成证书这里采用 webroot 作为 Let’s Encrypt 的认证方式。 certbot certonly -a webroot --webroot-path=/your/project/path -d example.com -d www.example.comwebroot-path就是项目根路径，使用 -d 可以添加多个域名。这时证书就已经生成成功了，默认保存在 /etc/letsencrypt/live/example.com/ 下。证书文件包括： cert.pem: 服务端证书 chain.pem: 浏览器需要的所有证书但不包括服务端证书，比如根证书和中间证书 fullchain.pem: 包括了cert.pem和chain.pem的内容 privkey.pem: 证书私钥 3. 生成迪菲-赫尔曼密钥交换组（ Strong Diffie-Hellman Group）为了进一步提高安全性，也可以生成一个 Strong Diffie-Hellman Group。 openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048配置nginx编辑 Nginx 配置文件，如果你不知道配置文件在哪，可以用 locate /nginx.conf 命令查找。添加以下内容，具体参数以你的实际情况为准。 server { listen 443 ssl; # 启用http2 # 需要安装 Nginx Http2 Module # listen 443 http2 ssl; server_name my_server_name; #证书文件 ssl_certificate /etc/letsencrypt/live/my_server_name/fullchain.pem; #私钥文件 ssl_certificate_key /etc/letsencrypt/live/my_server_name/privkey.pem; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 优先采取服务器算法 ssl_prefer_server_ciphers on; # 定义算法 ssl_ciphers &amp;quot;EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH&amp;quot;; ssl_ecdh_curve secp384r1; ssl_session_cache shared:SSL:10m; ssl_session_tickets off; ssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4 valid=300s; resolver_timeout 5s; add_header Strict-Transport-Security &amp;quot;max-age=63072000; includeSubdomains&amp;quot;; add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; # 使用DH文件 ssl_dhparam /etc/ssl/certs/dhparam.pem; location ~ /.well-known { allow all; } location ~ \\.php$ { root my_root; fastcgi_pass my_host:my_port; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } root my_root; index index.html index.php; location / { root my_root; autoindex on; index index.html index.php; client_max_body_size 1024m; } }其中的几项配置: ssl_stapling on; 开启 OCSP Stapling，使服务端主动获取 OCSP 查询结果并随着证书一起发送给客户端，从而让客户端跳过自己去验证的过程，提高 TLS 握手效率。 add_header Strict-Transport-Security &amp;quot;max-age=63072000; includeSubdomains&amp;quot;; 启用 HSTS 策略，强制浏览器使用 HTTPS 连接，max-age设置单位时间内強制使用 HTTPS 连接；includeSubDomains 可选，设置所有子域同时生效。浏览器在获取该响应头后，在 max-age 的时间内，如果遇到 HTTP 连接，就会通过 307 跳转強制使用 HTTPS 进行连接 add_header X-Frame-Options DENY; 添加 X-Frame-Options 响应头，可以禁止网站被嵌入到 iframe 中，减少点击劫持 (clickjacking)攻击。 add_header X-Content-Type-Options nosniff; 添加 X-Content-Type-Options 响应头，防止 MIME 类型嗅探攻击 测试nginx.conf 是否有语法错误 nginx -t 重启nginx nginx -s reload","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"https","slug":"https","permalink":"https://lxb.wiki/tags/https/"}]},{"title":"docker挂载目录失败/权限拒绝","slug":"docker挂载目录失败:权限拒绝","date":"2019-07-23T09:32:38.000Z","updated":"2019-10-03T07:57:15.287Z","comments":true,"path":"498654c2/","link":"","permalink":"https://lxb.wiki/498654c2/","excerpt":"","text":"把宿主机的一个目录挂载到容器中的一个目录，当访问容器中的这个目录时，出现如下问题： ls: cannot open directory .: Permission denied无法访问目录，权限拒绝。该问题通常在centos7下出现。或者一个容器启动成功后，里面的服务无法成功访问，这是因为centos7中的安全模块selinux把权限禁掉了，一般的解决方案有以下两种：（1）临时关闭selinux直接在centos服务器上执行以下命令即可。执行完成以后建议重新docker run。 setenforce 0（2）给容器加权限在docker run时给该容器加权限，加上以下参数即可： --privileged=true 一般都推荐使用这种方式。 按上述方法修改后, 如果执行下面命令失败 docker run --name rookie-nginx-test -d -p 8082:80 -v ~/nginx/www:/usr/share/nginx/html -v ~/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v ~/nginx/logs:/var/log/nginx -v ~/nginx/conf/conf.d:/etc/nginx/conf.d --link php7-fpm:php nginx则是因为~/nginx/www/ 目录下没有index 文件导致. 手动创建index.php 文件解决","categories":[{"name":"Docker","slug":"Docker","permalink":"https://lxb.wiki/categories/Docker/"}],"tags":[]},{"title":"Mac iTerm2登陆CentOS提示warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory","slug":"mac-iterm2-e7-99-bb-e9-99-86centos-e6-8f-90-e7-a4-bawarning-setlocale-lc-ctype-cannot-change-locale-utf-8-no-such-file-or-directory","date":"2019-07-02T03:48:53.000Z","updated":"2020-05-05T11:47:33.550Z","comments":true,"path":"784beb8f/","link":"","permalink":"https://lxb.wiki/784beb8f/","excerpt":"","text":"【报错原因】：没有utf-8这个语系（没添加语言_国名前缀），LC_ALL又没设定值。 服务端解决方法： 在远程系统上， /etc/environment加入以下两行，重新登陆即可。 LANG=en_US.utf-8 LC_ALL=en_US.utf-8Mac终端解决方法： 编辑~/.bashrc或者~/.zshrc文件，添加 export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8","categories":[{"name":"Tools","slug":"Tools","permalink":"https://lxb.wiki/categories/Tools/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://lxb.wiki/tags/工具/"}]},{"title":"升级到php7.1之后wordpress 网站出现Error establishing a database connection的解决方法","slug":"升级到php7.1之后wordpress 网站出现Error establishing a database connection的解决方法","date":"2019-06-12T15:37:00.000Z","updated":"2019-10-03T16:44:16.932Z","comments":true,"path":"b6b408b2/","link":"","permalink":"https://lxb.wiki/b6b408b2/","excerpt":"","text":"现在很多WordPress的插件都推荐将php版本升级到7.0或者7.1以上，于是就折腾了一下把几个blog升级到了7.1.5，升级的过程不难，无非就是额外安装一个php，然后启动自带的配套php-fpm7，然后nginx里location转发到新的php socket文件，这里就不表了。 升级完了，phpinfo()发现一切都正常，但是访问WordPress，却意外提示Error establishing a database connection，但是db的连接信息明明没有问题，经过反复搜索尝试，发现只要将 /usr/share/nginx/html/wp-config.php 文件里的 define(&#39;DB_HOST&#39;, &#39;localhost&#39;); 修改为 define(&#39;DB_HOST&#39;, &#39;127.0.0.1&#39;); 即可解决，猜测原因可能是php7.1中对域的resolve问题 另外, 为了 Debug, 可以把 /usr/share/nginx/html/wp-config.php 的 debug 改为 true define(&#39;WP_DEBUG&#39;, true); 改好了, 再改成 false.","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"wordpress","slug":"wordpress","permalink":"https://lxb.wiki/tags/wordpress/"}]},{"title":"date 命令转换时间戳","slug":"date命令转换时间戳","date":"2019-06-10T08:59:14.000Z","updated":"2019-10-03T07:57:15.285Z","comments":true,"path":"7b4019ad/","link":"","permalink":"https://lxb.wiki/7b4019ad/","excerpt":"","text":"给定时间戳, 转换成日期网上所有的命令都是date -d @$stamp &quot;+%Y-%m-%d&quot; 但是一直提示 date: invalid date@stamp’带上&quot;@&quot; 符号, 就参数错误 正确使用方法:date -d “1970-01-01 UTC 1287331200 seconds” +%F或者使用awkawk ‘{print strftime(“%Y%m”, 1287331200)}’调用外部命令耗时比较长, 更高效的方法:printf “%(%Y%m)T\\n” “$str” &gt;&gt; file如果bash 版本低于4, printf 不支持打印日期格式, 因此使用 下面这个bash/opt/compiler/gcc-4.8.2/bin/bash`","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"大小端","slug":"大小端","date":"2019-05-24T10:37:53.000Z","updated":"2019-10-03T16:46:23.023Z","comments":true,"path":"7ee0edaa/","link":"","permalink":"https://lxb.wiki/7ee0edaa/","excerpt":"","text":"计算机系统中内存是以字节为单位进行编址的，每个地址单元都唯一的对应着1个字节（8 bit）。这可以应对char类型数据的存储要求，因为char类型长度刚好是1个字节，但是有些类型的长度是超过1个字节的（字符串虽然是多字节的，但它本质是由一个个char类型组成的类似数组的结构而已），比如C/C++中，short类型一般是2个字节，int类型一般4个字节等。因此这里就存在着一个如何安排多个字节数据中各字节存放顺序的问题。正是因为不同的安排顺序导致了大端存储模式和小端存储模式的存在。 1. 解释假如有一个4字节的数据为 0x12 34 56 78（十进制：305419896，0x12为高字节，0x78为低字节），若将其存放于地址 0x4000 8000中，则有： 内存地址 0x4000 8000（低地址） 0x4000 8001 0x4000 8002 0x4000 8003（高地址） 大端模式 0x12（高字节） 0x34 0x56 0x78（低字节） 小端模式 0x78（低字节） 0x56 0x34 0x12（高字节） 大端模式：是指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中 小端模式，是指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中 为什么截然相反的大小端存储模式能够并存至今？在标准化备受推崇的今天，为什么大小端谁都没有被另外一个所同化？我想这除了历史的惯性使然，还与它们各自的优缺点有关。 大端模式优点：符号位在所表示的数据的内存的第一个字节中，便于快速判断数据的正负和大小 小端模式优点：1. 内存的低地址处存放低字节，所以在强制转换数据时不需要调整字节的内容（注解：比如把int的4字节强制转换成short的2字节时，就直接把int数据存储的前两个字节给short就行，因为其前两个字节刚好就是最低的两个字节，符合转换逻辑）； 2. CPU做数值运算时从内存中依顺序依次从低位到高位取数据进行运算，直到最后刷新最高位的符号位，这样的运算方式会更高效 其各自的优点就是对方的缺点，正因为两者彼此不分伯仲，再加上一些硬件厂商的坚持（见1.3节），因此在多字节存储顺序上始终没有一个统一的标准 Intel的80×86系列芯片使用小端存储模式 ARM芯片默认采用小端，但可以切换为大端 MIPS芯片采用大端，但可以在大小端之间切换 在网络上传输的数据普遍采用的都是大端 2. 判断方法一：通过将多字节数据强制类型转换成单字节数据，再通过判断起始存储位置是数据高字节还是低字节进行检测 // @Ret: 大端，返回true; 小端，返回false bool IsBigEndian_1() { int nNum = 0x12345678; char cLowAddressValue = *(char*)&amp;nNum; // 低地址处是高字节，则为大端 if ( cLowAddressValue == 0x12 ) return true; return false; }方法二：利用联合体union的存放顺序是所有成员都从低地址开始存放这一特性进行检测 // @Ret: 大端，返回true; 小端，返回false bool isBigEndian_2() { union uendian { int nNum; char cLowAddressValue; }; uendian u; u.nNum = 0x12345678; if ( u.cLowAddressValue == 0x12 ) return true; return false; }3. 转换大小端转换 // 实现16bit的数据之间的大小端转换 #define BLSWITCH16(A) ( ( ( (uint16)(A) &amp; 0xff00 ) &gt;&gt; 8 ) | \\ ( ( (uint16)(A) &amp; 0x00ff ) &lt;&lt; 8 ) ) // 实现32bit的数据之间的大小端转换 #define BLSWITCH32(A) ( ( ( (uint32)(A) &amp; 0xff000000) &gt;&gt; 24) |\\ (((uint32)(A) &amp; 0x00ff0000) &gt;&gt; 8) | \\ (((unit32)(A) &amp; 0x0000ff00) &lt;&lt; 8) | \\ (((uint32)(A) &amp; 0x000000ff) &lt;&lt; 32) )由于网络字节序一律为大端，而目前个人PC大部分都是X86的小端模式，因此网络编程中不可避免得要进行网络字节序和主机字节序之间的相互转换","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[]},{"title":"分布式发号器架构设计","slug":"分布式发号器架构设计","date":"2019-05-10T06:04:04.000Z","updated":"2020-05-05T11:48:14.773Z","comments":true,"path":"3d5a1f1d/","link":"","permalink":"https://lxb.wiki/3d5a1f1d/","excerpt":"","text":"一 需求设计 分布式环境下，保证每个序列号（sequence）是全系统唯一的； 序列号可排序，满足单调递增的规律； 特定场景下，能生成无规则（或者看不出规则）的序列号； 生成的序列号尽量短； 序列号可进行二次混淆，提供可扩展的interface，业务方自定义实现。 二 方案设计为了满足上述需求，发号器必须能够支持不同的生成策略，最好是还能支持自定义的生成策略，这就对系统本身的可扩展性提出了要求。 目前，发号器设计了两种比较通用的基础策略，各有优缺点，但结合起来，能达到优势互补的目的。 1. segment第一种策略称之为『分段』（segment），下文将对其进行详细阐述： 整个segment发号器有两个重要的角色：Redis和MongoDB，理论上MongoDB是可以被MySQL或其他DB产品所替代的。 segment发号器所产生的号码满足单调递增的规律，短时间内产生的号码不会有过长的问题（可根据实际需要，设置初始值，比如 100）。 Redis数据结构（Hash类型）key: &lt;string&gt;，表示业务主键/名称 value: { cur: &lt;long&gt;，表示当前序列号 max: &lt;long&gt;，表示这个号段最大的可用序列号 }取号的大部分操作都集中在Redis，为了保证序列号递增的原子性，取号的功能可以用Lua脚本实现。 --[[ 由于RedisTemplate设置的HashValueSerializer是GenericToStringSerializer，故此处的HASH结构中的 VALUE都是string类型，需要使用tonumber函数转换成数字类型。 ]] local max = redis.pcall(&quot;HGET&quot;, KEYS[1], &quot;max&quot;) --获取一段序列号的max local cur = redis.pcall(&quot;HGET&quot;, KEYS[1], &quot;cur&quot;) --获取当前发号位置 if tonumber(cur) &gt;= tonumber(max) then --没有超过这段序列号的上限 local step = ARGV[1] if (step == nil) then --没有传入step参数 step = redis.pcall(&quot;HGET&quot;, KEYS[1], &quot;step&quot;) --获取这段序列号的step配置参数值 end redis.pcall(&quot;HSET&quot;, KEYS[1], &quot;max&quot;, tonumber(max) + tonumber(step)) --调整max参数值，扩展上限 end return redis.pcall(&quot;HINCRBY&quot;, KEYS[1], &quot;cur&quot;, 1) --触发HINCRBY操作，对cur自增，并返回自增后的值注意：在redis执行lua script期间，redis处于BUSY状态，这个时候对redis的任何形式的访问都会抛出JedisBusyException异常，所以lua script中的处理逻辑不得太复杂。 值得一提的是，即使切换到一个新的database，或者开启新线程执行lua script，都将会遇到同样的问题，毕竟redis是单进程单线程的。 如果不幸遇到上述问题，需要使用redis-cli客户端连上redis-server，向其发送SCRIPT KILL命令，即可终止脚本执行， 如果想避免上述问题，也可以直接使用Springboot提供的RedisTemplate，能支持绝大部分redis command。 MongoDB 数据结构{ bizTag: &lt;string&gt;, 表示业务主键/名称 max: &lt;long&gt;, 表示这个号段最大的可用序列号 step: &lt;int&gt;, 每次分段的步长 timestamp: &lt;long&gt;, 更新数据的时间戳（毫秒） }MongoDB部分主要是对号段的分配进行管理，一个号段不能多发，也可以根据发号情况，适当放缩号段步长（step）。 到此为止，segment发号器的雏形已经形成了。 一个比较突出的问题是在两个号段衔接的时间点，当一个segment派发完了后，会对MongoDB和Redis中的数据中的max扩容，I/O消耗比正常发号要稍多，会遇到“尖刺” 为了消除“尖刺”，可以使用双Buffer模型 这个模型的核心思想就是“预分配”。可以设置一个阈值（threshold），比如20%，当Buffer-1里面的号段已经消耗了20%，那么立刻根据Buffer-1的max和step，开辟Buffer-2。 当Buffer-1完全消耗了，可以无缝衔接Buffer-2,。如果Buffer-2的消耗也达到阈值了，又可以开辟Buffer-1，如此往复。 接下来，我们来讨论一下异常/故障情况。 ① Redis宕机。因为大部分发号工作都是依靠Redis完成的，所以发生了这种情况是非常糟糕的。如果想有效降低此风险，最行之有效的办法是对Redis进行集群化，通常是1主2从，这样可以挺住非常高的QPS了。 当然也有退而求其次的办法，就是利用上述提到的双Buffer模型。不依赖Redis取号，直接通过程序控制，利用机器内存。所以当需要重启发号服务之前，要确保依赖的组件是运行良好的，不然号段就丢失了。 ② 要不要持久化的问题。这个问题主要是针对Redis，如果没有记录下当前的取号进度，那么随着Redis的宕机，取号现场就变得难以恢复了；如果每次都记录取号进度，那么这种I/O高密度型的作业会对服务性能 造成一定影响，并且随着取号的时间延长，恢复取号现场就变得越来越慢了，甚至到最后是无法忍受的。除了对Redis做高可用之外，引入MongoDB也是出于对Redis持久化功能辅助的考虑。 个人建议：如果Redis已经集群化了，而且还开启了双Buffer的策略，以及MongoDB的加持，可以不用再开启Redis的持久化了。 如果考虑到极端情况下，Redis还是宕机了，我们可以使用MongoDB里面存下来的max，就max+1赋值给cur（避免上个号段取完，正好宕机了）。 ③ MongoDB宕机。这个问题不是很严重，只要将step适当拉长一些（至少取号能支撑20分钟），利用Redis还在正常取号的时间来抢救MongoDB。不过，考虑到实际可能没这么快恢复mongo服务，可以在程序中采取 一些容错措施，比如号段用完了，mongo服务无法到达，直接关闭取号通道，直到MongoDB能正常使用；或者程序给一个默认的step，让MongoDB中的max延长到max+step*n（可能取了N个号段MongoDB才恢复过来）， 这样取号服务也可以继续。依靠程序本身继续服务，那么需要有相关的log，这样才有利于恢复MongoDB中的数据。 ④ 取号服务宕机。这个没什么好说的，只能尽快恢复服务运行了。 ⑤ Redis，MongoDB都宕机了。这种情况已经很极端了，只能利用双Buffer策略，以及程序默认的设置进行工作了，同样要有相关的log，以便恢复Redis和MongoDB。 ⑥ 都宕机了。我有一句mmp不知当讲不当讲…… 2、snowflake第二种策略是Twitter出品，算法思想比较巧妙，实现的难度也不大。 以上示意图描述了一个序列号的二进制组成结构。 第一位不用，恒为0，即表示正整数； 接下来的41位表示时间戳，精确到毫秒。为了节约空间，可以将此时间戳定义为距离某个时间点所经历的毫秒数（Java默认是1970-01-01 00:00:00）； 再后来的10位用来标识工作机器，如果出现了跨IDC的情况，可以将这10位一分为二，一部分用于标识IDC，一部分用于标识服务器； 最后12位是序列号，自增长。 snowflake的核心思想是64bit的合理分配，但不必要严格按照上图所示的分法。 如果在机器较少的情况下，可以适当缩短机器id的长度，留出来给序列号。 当然，snowflake的算法将会面临两个挑战： ① 机器id的指定。这个问题在分布式的环境下会比较突出，通常的解决方案是利用Redis或者Zookeeper进行机器注册，确保注册上去的机器id是唯一的。为了解决 强依赖Redis或者Zookeeper的问题，可以将机器id写入本地文件系统。 ② 机器id的生成规则。这个问题会有一些纠结，因为机器id的生成大致要满足三个条件：a. int类型(10bit)纯数字，b. 相对稳定，c. 与其他机器要有所区别。至于优雅美观，都是其次了。对于机器id的存储，可以使用HASH结构，KEY的规则是“application-name.port.ip”，其中ip是通过算法转换成了一段长整型的纯数字，VALUE则是机器id， 服务id，机房id，其中，可以通过服务id和机房id反推出机器id。 假设服务id(workerId)占8bit，机房id(rackId)占2bit，从1开始，workerId=00000001，rackId=01，machineId=00000000101 如果用Redis存储，其表现形式如下： 如果存储在文件中（建议properties文件），则文件名是sequence-client:8112:3232235742.properties，文件内容如下： 如果发号服务上线，直接按照“application-name.port.ip”的规则取其内容。 ③ 时钟回拨。因为snowflake对系统时间是很依赖的，所以对于时钟的波动是很敏感的，尤其是时钟回拨，很有可能就会出现重复发号的情况。时钟回拨问题解决策略通常是直接拒绝发号，直到时钟正常，必要时进行告警。 三 程序设计整个发号过程可以分成三个层次： 1、策略层(strategy layer)：这个层面决定的是发号方法/算法，涵盖了上述所讲的segment和snowflake两种方式，当然，用户也可以自己扩展实现其他发号策略。 最顶上定义Sequence实际上就是发号的结果。bizType是对发号业务场景的定义，比如订单号，用户ID，邀请好友的分享码。 发号策略的init接口是发号前的初始化工作，而generate接口就是调用发号器的主入口了。 当然，考虑到各种异常情况，加入了拒绝发号的处理器（SequenceRejectedHandler），默认实现只是记录日志，用户可根据需求去实现该处理器，然后用set方法设置发号策略的拒绝处理器。 2、插件层(plugin layer)：此处的插件可以理解是一种拦截器，贯穿SequenceStrategy的发号全周期。引入插件后，无疑是丰富了整个发号的操作过程，用户可以从中干预到发号的整个流程，以便达到其他的目的，比如：记录发号历史，统计发号速率，发号二次混淆等。 可以看出，插件被设计成『注册式』的，发号策略只有注册了相关插件之后，插件才能生效， 当然，一个插件能被多个发号策略所注册，一个发号策略也能同时注册多个插件，所以两者是多对多的关系，PluginManager的出现就是解决插件的注册管理问题。 从SequencePlugin的定义中可以发现，插件是有优先级（Order）的，通过getOrder()可以获得，在这套发号系统里，Order值越小，表示该插件越优先执行。此外，插件有三个重要的操作： before，表示发号之前的处理。若返回了false，那么该插件后面的操作都失效了，否则继续执行发号流程。 after，表示发号之后的处理。 doException，表示插件发生异常的处理方法。 3、持久层(persistence layer)：这个层面指代的是上述所提的MongoDB部分，如果不需要持久化的支持，可以不实现此接口，那么整个发号器就变成纯内存管理的了。 PersistRepository定义了基本的CRUD方法，其中persistId可以理解成上述提到的BizType。 一切的持久化对象都是从PersistModel开始的，上图中的Segment、PersistDocument都是为了实现分段发号器而定义的。 四 总结这篇文章详细阐述了分布式发号器系统的设计，旨在能做出一个可扩展，易维护的发号系统。业界比较知名的发号算法似乎也不多，整个发号系统不一定就按照笔者所做的设计，还是要立足于具体的业务需求。","categories":[{"name":"DB","slug":"DB","permalink":"https://lxb.wiki/categories/DB/"}],"tags":[{"name":"架构","slug":"架构","permalink":"https://lxb.wiki/tags/架构/"}]},{"title":"mysqldump: Got error: 1044: Access denied for user","slug":"mysqldump-got-error-1044-access-denied-for-user","date":"2019-04-27T03:24:27.000Z","updated":"2019-10-03T16:57:23.397Z","comments":true,"path":"a333bc04/","link":"","permalink":"https://lxb.wiki/a333bc04/","excerpt":"","text":"mysqldump -u username -p dbname &gt; dbname.sql mysqldump: Got error: 1044: Access denied for user XXX to database XXX when using LOCK TABLES 解决方法: mysqldump -u dbuser -ppass db --skip-lock-tables &gt; db.sql","categories":[{"name":"DB","slug":"DB","permalink":"https://lxb.wiki/categories/DB/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://lxb.wiki/tags/mysql/"}]},{"title":"DDBS","slug":"ddbs","date":"2019-04-10T06:00:41.000Z","updated":"2019-10-03T16:41:23.262Z","comments":true,"path":"728c18a3/","link":"","permalink":"https://lxb.wiki/728c18a3/","excerpt":"","text":"业务规模较小时，使用单机mysql作存储。但伴随业务发展，存储容量和并发能力会有瓶颈。 首先，假设单机的硬盘为1.8T，也可以挂更大容量硬盘，但仍有限。 其次，单机的读写并发能力有限，假设峰值写入qps1000，峰值读取qps3000，网卡对读取时流量也有要求，单次访问的读取量不应过大。 单机的链接数也有限。 那么，当使用单机mysql的业务发展，受到以上瓶颈时，一般的思路会是什么呢？一台机器不行，用两台呢，再不行，扩展更多台。 一台扩展为两台，磁盘容量扩大了，通过分表，将表打散在不同机器上，共同承担写入任务，并发也提高了，感觉这个思路是对的。 那么在这个过程中，我们需要做什么？ 业务发展到单机无法承受，即使在单机上，很多表应该也做过分表了。一般会根据业务选择分表键。单个表的大小mysql也有一定要求，一般存储量不大于1G，单条记录小一些，一般不超过1k，条数一般不超过1000万条，最多不超过5000万条，否则表的使用和维护效率都很低。假设业务已经做了足够多的分表，满足三年的数据增长需要，第一年过后，每个分表的条数达到200万条，整机存储容量使用了一半，此时我们想拆分为两台机器。 此时我们可以将原机器上部分表数据同步到新机器上，并在model层抽象一个路由层，将对数据库的操作发到不同的机器上，上层业务仍可以认为在使用单机。此时可以将原机器上不归属自己管理范围的表删除，腾出空间。 一台变成了两台，向分布式走了一步。此时存储容量和并发都提高了，由路由层管理两台机器。如果两台或今后的多台机器，并发数高于路由层处理能力怎么办？那还要把路由层机器也扩一下，把路由规则都写进去，大家按一个格则办事。 经过上面的一番折腾，数据库机器水平扩展，解决了单机存在的一些问题。在这个扩展的过程中，是否会对业务产生中断影响呢？ 会有一点影响。至少在路由层改路由表时，会中断数据库的写入，读取此时可以不中断。 ddbs中，使用到的多台机器，都叫做分片。分片提高了系统存储容量和并发能力，引入分片，也是系统的复杂度提高了，需要引入路由层机器，路由机器也可能需要扩展，复杂操作，还需要添加更多逻辑功能。但至少可以可业务逻辑区分开，业务可以把ddbs当做单机在使用。 那么ddbs有哪些不足呢？ ddbs还是要基于分表、分片实现的。那么对数据库的任何操作，首要条件是需要指明操作的分表键。没有这个维度的准确值，就不能对数据库操作，当然除非是备用库，那你随便扫表，因为备用库可以转为冗余安全，不走线上流量，可以做统计任务。 单指明分表键还不行，还要注意操作的数据可能会分布在不同分表、不同分片中，这样的操作会引发ddbs产生大量并发操作，业务的一个请求就会占用多个机器多个链接，使ddbs得并发能力大打折扣。比如‘where 分表键 in （）’操作，这种操作要慎重，in中个数不可太多。 分表键最好选用整形，字符串型，可能hash后分配不均，表大小不均衡。 事物操作在ddbs中的实现，非常耗费系统性能。事务类操作需要路由控制层控制整个操作过程，期中可能涉及多个分片，多个不同的表的操作，对系统整体可用性要求高","categories":[{"name":"DB","slug":"DB","permalink":"https://lxb.wiki/categories/DB/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://lxb.wiki/tags/mysql/"}]},{"title":"跑道问题","slug":"跑道问题","date":"2019-03-12T15:20:56.000Z","updated":"2019-10-03T07:57:15.303Z","comments":true,"path":"81be14b5/","link":"","permalink":"https://lxb.wiki/81be14b5/","excerpt":"","text":"25个人，每5个人一个跑道，最少经过几次比赛，得到前三名 初步思路: 第一步, 每5人一组, 全跑完后, 每组的后两名一定不在最终要的”前三名” 结果内, 所以每组可以排除2人, 剩下25-2_5=15人. 共经过5次比赛 第二步, 剩下的15人, 每5人一组, 跑完后, 每组淘汰2人, 剩下 15-2_3=9人. 经过3次比赛 第三步, 剩下的9个人分两组, A组5人B组4人, 跑完后, A组淘汰2人, B组淘汰1人, 剩下 9-2-1=6人. 经过2次比赛 第四步, 剩下的6人分两组, C组5人D组1人, A组跑完后, 淘汰2人, B组1人不需要跑, 剩下 6-2=4人. 经过1次比赛 第五步, 剩下的4个人, 跑一次, 得出前三名. 经过1次比赛 共经过 5+3+2+1+1=12次 在第一步中, 5组全跑完后, 每组的第一名再跑一次, 按速度快慢分别标为A1 B1 C1 D1 E1. 则A1 为25人中的第一名. 经过5+1=6次比赛 在第6次比赛中, 落后的两名D1 和E1, 可以被排除, 进而整个D组和E组都可以排除. C1不可能是第二名. 第二名可能的人员有A2 B1, 第三名可能的人员有 B1 A3 B2 A2 C1. 第二名的集合是第三名集合的子集. 第三名所有可能的5个人跑一次, 得出第二名和第三名.经过1次比赛 共经过7次比赛","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://lxb.wiki/categories/Algorithm/"}],"tags":[]},{"title":"架构设计原则","slug":"架构设计原则","date":"2019-03-01T14:52:22.000Z","updated":"2019-10-03T07:57:15.440Z","comments":true,"path":"88049151/","link":"","permalink":"https://lxb.wiki/88049151/","excerpt":"","text":"GRASP 通用职责分配软件模式来自 Craig Larman 的软件设计书《UML 和模式应用》[附录 1]，Larman 在书中提出软件设计的关键任务是职责分配，并提炼总结出 9 种 (5 种核心 +4 种扩展) 软件职责分配模式，这些模式是比 GoF 设计模式更抽象的元模式。 1. 信息专家 (Information Expert) 为对象分配职责的通用原则 – 把职责分配给拥有足够信息可以履行职责的专家 2. 创建者 (Creator) 将创建 A 的职责赋给 B，如果至少下面一种情况为真： B“包含”或者聚合 A B 记录 A 的实例 B 密切地使用 A B 拥有 A 的初始化数据 3. 低耦合 (Low Coupling) 赋予职责使得对象间的耦合度尽可能低，最小化对象间的依赖和变更影响，最大化重用。 4. 高内聚 (High Cohesion) 赋予职责使得每个对象的职责尽可能保持聚焦和单一，易于管理和理解。 5. 控制器 (Controller) 把职责赋予系统、设备或者子系统的表示类 (门面控制器)，或者某个用例的表示类 (用例控制器)，让控制器接收事件并协调整个系统的运作。 6. 多态 (Polymorphism) 将职责分配给多个具有同名方法的多态子类，运行时根据需要动态切换子类，让系统行为变得可插拔。 7. 纯虚构 (Pure Fabrication) 针对真实问题域中不存在，但是设计建模中有用的概念，设计虚构类并赋予职责。 8. 间接 (Indirection) 在两个或者多个对象间有交互的情况下，为避免直接耦合，提高重用性，创建中间类并赋予职责，对象的交互交由中间类协调。 9. 受保护的变化 (Protected Variation) 简单讲就是封装变化。识别系统中可能的不稳定或者变化，在不稳定组件上创建稳定的抽象接口，将可能的变化封装在接口之后，使得系统内部的不稳定或者变化不会对系统的其它部分产生不良影响。 SOLID 面向对象设计原则S.O.L.I.D 是面向对象设计和编程 (OOD&amp;OOP) 中几个重要原则的首字母缩写，受 Robert Martin 推崇。 1. 单一职责原则 (The Single Responsibility Principle) 修改某个类的理由应该只有一个，如果超过一个，说明类承担不止一个职责，要视情况拆分。 2. 开放封闭原则 (The Open Closed Principle) 软件实体应该对扩展开放，对修改封闭。一般不要直接修改类库源码（即使你有源代码），通过继承等方式扩展。 3. 里氏替代原则 (The Liskov Substitution Principle) 当一个子类的实例能够被替换成任何超类的实例时，它们之间才是真正的 is-a 关系。 4. 依赖倒置原则 (The Dependency Inversion Principle) 高层模块不应该依赖于底层模块，二者都应该依赖于抽象。换句话说，依赖于抽象，不要依赖于具体实现。比方说，你不会把电器电源线焊死在室内电源接口处，而是用标准的插头插在标准的插座 (抽象) 上。 5. 接口分离原则 (The Interface Segregation Principle) 不要强迫用户去依赖它们不使用的接口。换句话说，使用多个专门的接口比使用单一的大而全接口要好。 备注 高内聚 + 低耦合，就像道中的一阴一阳，是所有其它 OO 设计原则的原则 (元原则)，其它设计原则都是在这两个基础上泛化衍生出来的。 上述原则虽然是针对 OO 设计和编程提出，但是对于大规模系统架构仍然适用。比如，微服务架构就体现了： 单一职责：一个微服务尽可能要职责单一，提供的接口也尽可能单一 (接口分离原则)，安全 / 路由 / 限流等跨横切面的关注点 (Cross-Cutting Concerns) 由独立网关负责，体现关注分离 (Separation of Concerns)。 信息专家：当不确定哪个团队应该负责某个微服务时，一般原则也是谁拥有数据谁负责，基于有界上下文 Bounded Context（一般是边界比较清晰的领域数据源）构建微服务。 松散耦合：服务之间通过 HTTP/JSON 等轻量机制通信，服务之间不强耦合。 受保护的变化和依赖倒置：服务之间只依赖抽象接口，实现可能随时变化。 间接：网关在外面的客户端和内部的服务之间增加了一层间接，使两者不强耦合，可以相互独立演化。 作为架构师或者设计师，有两个设计能力是需要重点培养的，也是最难和最能体现架构设计水平的： 合理的职责分配能力，也就是每个类 / 组件 / 子系统应该承担什么职责，如何保证职责单一，它们之间如何协作； 系统抽象和核心领域建模能力，需要深入一线业务域。 分布式系统架构设计原则和理论AKF 架构原则这 15 个架构原则来自《架构即未来 (The Art of Scalability)》[附录 2] 一书，作者马丁 L. 阿伯特和迈克尔 T. 费舍尔分别是 eBay 和 PayPal 的前 CTO，他们经历过 eBay 和 PayPal 大规模分布式电商平台的架构演进，在一线实战经验的基础上总结并提炼出 15 条架构原则： 1.N + 1 设计 永远不要少于两个，通常为三个。比方说无状态的 Web/API 一般部署至少&gt;=2 个。 2. 回滚设计 确保系统可以回滚到以前发布过的任何版本。可以通过发布系统保留历史版本，或者代码中引入动态开关切换机制 (Feature Switch)。 3. 禁用设计 能够关闭任何发布的功能。新功能隐藏在动态开关机制 (Feature Switch) 后面，可以按需一键打开，如发现问题随时关闭禁用。 4. 监控设计 在设计阶段就必须考虑监控，而不是在实施完毕之后补充。例如在需求阶段就要考虑关键指标监控项，这就是度量驱动开发 (Metrics Driven Development) 的理念。 5. 设计多活数据中心 不要被一个数据中心的解决方案把自己限制住。当然也要考虑成本和公司规模发展阶段。 6. 使用成熟的技术 只用确实好用的技术。商业组织毕竟不是研究机构，技术要落地实用，成熟的技术一般坑都被踩平了，新技术在完全成熟前一般需要踩坑躺坑。 7. 异步设计 能异步尽量用异步，只有当绝对必要或者无法异步时，才使用同步调用。 8. 无状态系统 尽可能无状态，只有当业务确实需要，才使用状态。无状态系统易于扩展，有状态系统不易扩展且状态复杂时更易出错。 9. 水平扩展而非垂直升级 永远不要依赖更大、更快的系统。一般公司成长到一定阶段普遍经历过买更大、更快系统的阶段，即使淘宝当年也买小型机扛流量，后来扛不住才体会这样做不 scalable，所以才有后来的去 IOE 行动。 10. 设计时至少要有两步前瞻性 在扩展性问题发生前考虑好下一步的行动计划。架构师的价值就体现在这里，架构设计对于流量的增长要有提前量。 11. 非核心则购买 如果不是你最擅长，也提供不了差异化的竞争优势则直接购买。避免 Not Invented Here 症状，避免凡事都要重造轮子，毕竟达成业务目标才是重点。 12. 使用商品化硬件 在大多数情况下，便宜的就是最好的。这点和第 9 点是一致的，通过商品化硬件水平扩展，而不是买更大、更快的系统。 13. 小构建、小发布和快试错 全部研发要小构建，不断迭代，让系统不断成长。这个和微服务理念一致。 14. 隔离故障 实现故障隔离设计，通过断路保护避免故障传播和交叉影响。通过舱壁泳道等机制隔离失败单元 (Failure Unit)，一个单元的失败不至影响其它单元的正常工作。 15. 自动化 设计和构建自动化的过程。如果机器可以做，就不要依赖于人。自动化是 DevOps 的基础。 备注 这 15 条架构原则基本上是 eBay 在发展，经历过流量数量级增长冲击过程中，通过不断踩坑踩出来的，是干货中的干货。消化吸收这 15 条原则，基本可保系统架构不会有原则性问题。 这 15 条原则同样适用于现在的微服务架构。eBay 发展较早，它内部其实很早 (差不多 2010 年前) 就已形成完善的微服务生态，只是没有提出微服务这个概念。 这 15 条原则可根据 TTM(Time To Market)，可用性 / 可扩展性 / 质量，成本 / 效率分布在三个环内，如下图所示。 12 要素应用基于上百万应用的托管和运营经验，创始人 Adam Wiggins 提出了 12 要素应用宣言 。简单讲，满足这 12 个要素的应用是比较容易云化并居住在 Heroku 平台上的。 1. 基准代码 一份基准代码，多份部署。如果用镜像部署方式，则一个镜像可以部署到多个环境 (测试，预发，生产)，而不是给每个环境制作一个不同镜像。 2. 依赖 显式声明依赖。如果用镜像部署，则一般依赖被直接打在镜像中，或者声明在 docker file 中。 3. 配置 在环境中存储配置。在 Heroku 或者类似的 PaaS 平台上，配置一般是推荐注入到环境变量中的。现在采用集中式配置中心也是一种流行方式。 4. 后端服务 把后端服务 (例如缓存，数据库，MQ 等) 当作附加资源，相关配置和连接字符串通过环境变量注入，或者采用配置中心。 5. 构建、发布和运行 严格分离构建和运行。如果使用镜像部署，则构建、发布 / 运行是通过镜像这种中间格式严格分离的。 6. 进程 一个或者多个无状态的进程运行应用。容器运行时相当于进程，适用于无状态 Web/API。 7. 端口绑定 通过端口绑定提供服务。容器也是通过端口绑定对外提供服务。 8. 并发 通过进程模型进行扩展。容器运行时相当于进程，通过起多个容器可以任意扩展并发数量。 9. 易处理 快速启动和优雅终止可最大化健壮性。docker 容器支持秒级启动和关闭。 10. 开发环境和线上环境等价 尽可能保持开发、测试、预发和线上环境相同。容器可以保证容器内运行时环境的一致性，还需要保证不同环境的一致性，例如不同环境内的操作系统，负载均衡，服务发现，后台服务，监控告警等要尽可能一致。 11. 日志 把日志当作数据流。Heroku 不支持本地文件，所以必须以流方式把日志输送到后台日志服务。除了日志以外还要补充考虑 metrics 流的采集和输送。 12. 管理进程 后台管理任务当作一次性的进程。其实相当于在 Heroku 上以独立进程方式运行任务 Job。 备注 12 要素应用也是当前云原生应用 (Cloud Native App) 的参考标准，也称为云应用迁移原则。满足这 12 个要素的应用，可以比较顺利迁移到各种云平台 (Kubernetes, Marathon, Cloud Foundry 等) 上。 对于面临企业遗留应用改造和云化迁移的架构师，可以重点参考这 12 条迁移原则。 Docker 容器技术可以认为是为云迁移量身定制的技术。容器化是后续云迁移的捷径，所以遗留应用改造可以先想办法做到容器化。 CAP 定理2000 年 7 月，加州大学伯克利分校的 Eric Brewer 教授在 ACM PODC 会议上提出 CAP 猜想。2 年后，麻省理工学院的 Seth Gilbert 和 Nancy Lynch 从理论上证明了 CAP。之后，CAP 理论正式成为分布式计算领域的公认定理。 CAP 认为：一个分布式系统最多同时满足一致性 (Consistency)，可用性 (Availability) 和分区容忍性 (Partition Tolerance) 这三项中的两项。 1.一致性 (Consistency) 一致性指“all nodes see the same data at the same time”，即更新操作成功，所有节点在同一时间的数据完全一致。 2.可用性 (Availability) 可用性指“Reads and writes always succeed”，即服务一直可用，而且响应时间正常。 3.分区容忍性 (Partition tolerance) 分区容忍性指“the system continue to operate despite arbitrary message loss or failure of part of the system.”，即分布式系统在遇到某节点或网络分区故障时，仍然能够对外提供满足一致性和可用性的服务。 BASE 理论eBay 架构师 Dan Pritchett 基于对大规模分布式系统的实践总结，在 ACM 上发表文章提出了 BASE 理论，BASE 理论是对于 CAP 理论的延伸，核心思想是即使无法做到强一致性 (Strong Consistency，CAP 中的一致性指强一致性)，但是可以采用适当的方式达到最终一致性 (Eventual Consistency)。 BASE 指基本可用 (Basically Available)、软状态 (Soft State) 和最终一致性 (Eventual Consistency)。 1.基本可用 (Basically Available) 基本可用是指分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。比如服务降级。 2.软状态 (Soft State) 软状态是指允许系统存在中间状态，而该中间状态不会影响系统的整体可用性。分布式存储中一般一份数据至少存三个副本，允许不同节点间副本同步的延迟就是软状态的体现。 3.最终一致性 (Eventual Consistency) 最终一致性是指系统中的所有数据副本经过一段时间后，最终能够达成一致状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。 备注 CAP 和 BASE 理论可以抠得很深，背后甚至有很复杂的数学证明。我理解得相对简单浅显：性能、高可用、不丢数据和数据一致性对分布式系统来说一般是强需求，随着流量的增长，复制和分区在所难免： 复制 (replication)：数据在多个节点上存多份保证不丢和高可用； 分区 (partition)：数据按某个纬度切分分布在不同节点上分摊流量压力保证高性能，同时也是为了降低每个节点的复杂性。例如数据库的分库分表，系统拆分微服务化也是一种分区。这两者都会带来一致性问题，一致性在时间上有一点妥协的余地 - 即是最终一致性；时间上要求强一致的话，只有可用性可以适当折中。系统架构的游戏很大部分是和状态一致性作斗争的游戏。 选择使用分布式产品时，比如 NoSQL 数据库，你需要了解它在 CAP 环中所在的位置，确保它满足你的场景需要。 组织和系统改进原则康威法则Melvin Conway 在 1967 年提出所谓康威法则 ，指出组织架构和系统架构之间有一种隐含的映射关系： Organization which design system […] are constrained to produce designs which are copies of the communication structures of these organization. 设计系统的组织其产生的设计等价于组织间的沟通结构。 康威法则也可以倒过来阐述： Conway’s law reversed：You won’t be able to successfully establish an efficient organization structure that is not supported by your system design(architecture)。 如果系统架构不支持，你无法建立一个高效的组织；同样，如果你的组织架构不支持，你也无法建立一个高效的系统架构。 系统改进三原则IT 运维管理畅销书《凤凰项目》[附录 8] 的作者 Gene Kim 在调研了众多高效能 IT 组织后总结出支撑 DevOps 运作的三个原理 (The Three Ways: The Principles Underpinning DevOps)[附录 9]，我认为也是系统改进提升的一般性原理 [附录 7]，见下图： 原理一：系统思考 (System Thinking) 开发驱动的组织，其能力不是制作软件，而是持续的交付客户价值。价值从业务需求开始，经过研发测试，到部署运维，依次流动，并最终以服务形式交付到客户手中。整个价值链流速并不依赖单个部分 (团队或个人) 的杰出工作，而是受整个价值链最薄弱环节 (瓶颈) 的限制。所以局部优化通常无效，反而招致全局受损。 Gene Kim 特别指出：Any improvements made anywhere besides the bottleneck are an illusion. 在瓶颈之外的任何优化提升都只是幻象。 原理二：强化反馈环 (Amplify Feedback Loops) 过程改进常常通过加强反馈环来达成。原理二强调企业和客户之间、组织团队间、流程上和系统内的反馈环。没有测量就没有提升，反馈要以测量数据为准，通过反馈数据优化改进系统。 原理三：持续试验和学习的文化 (Culture of Continual Experimentation And Learning) 在企业管理文化层面强调勇于试错和持续试验、学习和改进的文化。 备注 康威法则给我们的启示：系统架构和组织架构之间有隐含的映射关系，你不能单方面改变一方的结构，调整时必须两边联动。系统架构如果是耦合的，就很难组织分散式的团队结构，两边映射不起来，团队之间容易摩擦导致生产率下降。所以一般先按业务边界对单块应用进行解耦拆分，同时做相应的团队拆分，使两边可以映射，每个团队可以独立开发、测试和部署各自的微服务，进而提升生产率。这就是近年流行的微服务架构背后的组织原则。详见我之前发表的文章《企业的组织架构是如何影响技术架构的》[附录 6]。 系统思考要求我们加强团队合作，培养流式思维和瓶颈约束思维，找出瓶颈并针对性地优化。在研发型组织中，常见的系统瓶颈如运维机器资源提供 (Provisioning) 缓慢，发布流程繁琐容易出错，开发 / 测试／UAT 环境缺失或不完善，遗留系统耦合历史负担重，基础研发平台薄弱等等。这些瓶颈点特别需要关注优化。 反馈原理要求我们关注基于数据的反馈，技术上的手段包括大数据分析和系统各个层次的测量监控。没有测量就没有反馈，没有反馈就没有提升。 在管理文化层面： 管理层要承认企业内部近 50% 的创新或流程改进项目是有可能失败的，即使失败，员工不会受到责罚，鼓励持续的试验和从中学习； 管理层要有技术偿债意识，勿追求 100% 员工利用率，要预留 20%~30% 的时间给员工做创新和系统改进提升项目","categories":[{"name":"Design","slug":"Design","permalink":"https://lxb.wiki/categories/Design/"}],"tags":[]},{"title":"秒杀系统优化思路","slug":"秒杀系统优化思路","date":"2019-02-26T14:13:12.000Z","updated":"2019-10-03T07:57:15.299Z","comments":true,"path":"bcca3074/","link":"","permalink":"https://lxb.wiki/bcca3074/","excerpt":"","text":"一、秒杀业务为什么难做1）im系统，例如qq或者微博，每个人都读自己的数据（好友列表、群列表、个人信息）； 2）微博系统，每个人读你关注的人的数据，一个人读多个人的数据； 3）秒杀系统，库存只有一份，所有人会在集中的时间读和写这些数据，多个人读一个数据。 例如：小米手机每周二的秒杀，可能手机只有1万部，但瞬时进入的流量可能是几百几千万。 又例如：12306抢票，票是有限的，库存一份，瞬时流量非常多，都读相同的库存。读写冲突，锁非常严重，这是秒杀业务难的地方。那我们怎么优化秒杀业务的架构呢？ 二、优化方向优化方向有两个（今天就讲这两个点）： （1）将请求尽量拦截在系统上游（不要让锁冲突落到数据库上去）。传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的有效流量甚小。以12306为例，一趟火车其实只有2000张票，200w个人来买，基本没有人能买成功，请求有效率为0。 （2）充分利用缓存，秒杀买票，这是一个典型的读多写少的应用场景，大部分请求是车次查询，票查询，下单和支付才是写请求。一趟火车其实只有2000张票，200w个人来买，最多2000个人下单成功，其他人都是查询库存，写比例只有0.1%，读比例占99.9%，非常适合使用缓存来优化。好，后续讲讲怎么个“将请求尽量拦截在系统上游”法，以及怎么个“缓存”法，讲讲细节。 三、常见秒杀架构（1）浏览器端，最上层，会执行到一些JS代码 （2）站点层，这一层会访问后端数据，拼html页面返回给浏览器 （3）服务层，向上游屏蔽底层数据细节，提供数据访问 （4）数据层，最终的库存是存在这里的，mysql是一个典型（当然还有会缓存） 四、各层次优化细节第一层，客户端怎么优化（浏览器层，APP层）微信的摇一摇抢红包，每次摇一摇，就会往后端发送请求么？下单抢票的场景，点击了“查询”按钮之后，系统那个卡呀，进度条涨的慢呀，作为用户，我会不自觉的再去点击“查询”，对么？继续点，继续点，点点点。。。有用么？平白无故的增加了系统负载，一个用户点5次，80%的请求是这么多出来的，怎么整？ （a）产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求； （b）JS层面，限制用户在x秒之内只能提交一次请求； APP层面，可以做类似的事情，虽然你疯狂的在摇微信，其实x秒才向后端发起一次请求。这就是所谓的“将请求尽量拦截在系统上游”，越上游越好，浏览器层，APP层就给拦住，这样就能挡住80%+的请求，这种办法只能拦住普通用户（但99%的用户是普通用户）对于群内的高端程序员是拦不住的。firebug一抓包，http长啥样都知道，js是万万拦不住程序员写for循环，调用http接口的，这部分请求怎么处理？ 第二层，站点层面的请求拦截怎么拦截？怎么防止程序员写for循环调用，有去重依据么？ip？cookie-id？…想复杂了，这类业务都需要登录，用uid即可。在站点层面，对uid进行请求计数和去重，甚至不需要统一存储计数，直接站点层内存存储（这样计数会不准，但最简单）。一个uid，5秒只准透过1个请求，这样又能拦住99%的for循环请求。 5s只透过一个请求，其余的请求怎么办？缓存，页面缓存，同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面。同一个item的查询，例如车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面。如此限流，既能保证用户有良好的用户体验（没有返回404）又能保证系统的健壮性（利用页面缓存，把请求拦截在站点层了）。 页面缓存不一定要保证所有站点返回一致的页面，直接放在每个站点的内存也是可以的。优点是简单，坏处是http请求落到不同的站点，返回的车票数据可能不一样，这是站点层的请求拦截与缓存优化。 好，这个方式拦住了写for循环发http请求的程序员，有些高端程序员（黑客）控制了10w个肉鸡，手里有10w个uid，同时发请求（先不考虑实名制的问题，小米抢手机不需要实名制），这下怎么办，站点层按照uid限流拦不住了。 第三层 服务层来拦截（反正就是不要让请求落到数据库上去）服务层怎么拦截？大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？没错，请求队列！ 对于写请求，做请求队列，每次只透有限的写请求去数据层（下订单，支付这样的写业务） 1w部手机，只透1w个下单请求去db 3k张火车票，只透3k个下单请求去db 如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”。 对于读请求，怎么优化？cache抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的。如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了。 当然，还有业务规则上的一些优化。回想12306所做的，分时分段售票，原来统一10点卖票，现在8点，8点半，9点，…每隔半个小时放出一批：将流量摊匀。 其次，数据粒度的优化：你去购票，对于余票查询这个业务，票剩了58张，还是26张，你真的关注么，其实我们只关心有票和无票？流量大的时候，做一个粗粒度的“有票”“无票”缓存即可。 第三，一些业务逻辑的异步：例如下单业务与 支付业务的分离。这些优化都是结合 业务 来的，我之前分享过一个观点“一切脱离业务的架构设计都是耍流氓”架构的优化也要针对业务。 第四层 最后是数据库层浏览器拦截了80%，站点层拦截了99.9%并做了页面缓存，服务层又做了写请求队列与数据缓存，每次透到数据库层的请求都是可控的。db基本就没什么压力了，闲庭信步，单机也能扛得住，还是那句话，库存是有限的，小米的产能有限，透这么多请求来数据库没有意义。 全部透到数据库，100w个下单，0个成功，请求有效率0%。透3k个到数据，全部成功，请求有效率100%。 五、总结上文应该描述的非常清楚了，没什么总结了，对于秒杀系统，再次重复下我个人经验的两个架构优化思路： （1）尽量将请求拦截在系统上游（越上游越好）； （2）读多写少的常用多使用缓存（缓存抗读压力）； 浏览器和APP：做限速 站点层：按照uid做限速，做页面缓存 服务层：按照业务做写请求队列控制流量，做数据缓存 数据层：闲庭信步 并且：结合业务做优化","categories":[{"name":"Design","slug":"Design","permalink":"https://lxb.wiki/categories/Design/"}],"tags":[{"name":"架构","slug":"架构","permalink":"https://lxb.wiki/tags/架构/"}]},{"title":"制作种子","slug":"制作种子","date":"2018-11-01T13:20:18.000Z","updated":"2020-05-10T03:21:42.401Z","comments":true,"path":"7592d71b/","link":"","permalink":"https://lxb.wiki/7592d71b/","excerpt":"","text":"1.下载mktorrentgit clone https://github.com/lxbwolf/mktorrent.git2.下载完成后进入到文件夹里面例如：cd mktorrent（如果是根目录的话）3. make4. make install5. 默认安装目录位于/usr/local/bin，使用cd命令，从默认的/root路径切换到要制作成种子的文件上一级。 例如cd /Downloads6. 制作种子命令为： mktorrent -v -p -l 22 -a tracker_address -o name.torrent file_name参数说明： tracker_address为你要发布的网站的tracker。 name.torrent为对生成torrent种子文件的命名，规则为：xxx.torrent。 file_name为你要做种的文件或文件夹。避免含有空格。7. 等待一会儿会提示做种完成，在当前目录下即可找到。","categories":[{"name":"RPI","slug":"RPI","permalink":"https://lxb.wiki/categories/RPI/"}],"tags":[{"name":"torrent","slug":"torrent","permalink":"https://lxb.wiki/tags/torrent/"}]},{"title":"树莓派搭建迅雷远程下载服务器","slug":"树莓派搭建迅雷远程下载服务器","date":"2018-11-01T12:09:55.000Z","updated":"2020-05-10T03:22:15.859Z","comments":true,"path":"1846a864/","link":"","permalink":"https://lxb.wiki/1846a864/","excerpt":"","text":"1. 下载路由器固件从 官网 或者 百度网盘 解压到指定目录如 /root/xunlei 进入目录 执行./portal 稍等片刻，会在最后输出一个激活码 2. 在迅雷远程下载页面绑定树莓派登录迅雷远程下载主页,登录之后，左侧会有一个添加按钮，点击添加按钮 不需要选择绑定设备类型, 直接将树莓派上获得的激活码填入框中，点击绑定后左侧就会出现树莓派对应的设备列表了，但是，如果我们此时就在右侧点击新建之后会发现,弹出的新建页面中会提示找不到挂载磁盘 3. 自定义迅雷的下载目录进入/mnt目录，创建目录TDDOWNLOAD(名字随意) 执行mount --bind /data/TDDOWNLOAD /mnt/TDDOWNLOAD 其中/data/TDDOWNLOAD就是自定义的下载目录，你可以指定为其他任何目录。 然后再刚刚迅雷固件的解压目录下创建目录etc,同时在etc下创建文件thunder_mounts.cfg,编辑此文件, 写入内容 avaliable_mount_path_pattern { /mnt/TDDOWNLOAD }重启路由器固件 ./root/xunlei/portal 再进入远程下载界面新建下载就没有了没挂载磁盘的提示了 4. 迅雷路由器固件开机启动在/etc/init.d/下新建xunlei脚本，写入: #!/bin/sh # # Xunlei initscript # ### BEGIN INIT INFO # Provides: xunlei # Required-Start: $network $local_fs $remote_fs # Required-Stop:: $network $local_fs $remote_fs # Should-Start: $all # Should-Stop: $all # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start xunlei at boot time # Description: A downloader ### END INIT INFO do_start() { ./root/xunlei/portal } do_stop() { ./root/xunlei/portal -s } case &quot;$1&quot; in start) do_start ;; stop) do_stop ;; esac然后将该脚本加入默认自启动中 update-rc.d xunlei defaults","categories":[{"name":"RPI","slug":"RPI","permalink":"https://lxb.wiki/categories/RPI/"}],"tags":[{"name":"thunder","slug":"thunder","permalink":"https://lxb.wiki/tags/thunder/"}]},{"title":"树莓派基础环境","slug":"树莓派基础环境","date":"2018-10-18T14:24:19.000Z","updated":"2020-05-10T03:22:26.951Z","comments":true,"path":"5976aace/","link":"","permalink":"https://lxb.wiki/5976aace/","excerpt":"","text":"修改软件源sudo -s echo -e &quot;deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi \\n deb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi&quot; &amp;gt; /etc/apt/sources.list echo -e &quot;deb http://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui&quot; &amp;gt; /etc/apt/sources.list.d/raspi.list exit sudo apt update &amp;amp;&amp;amp; sudo apt -y upgrade中文输入法sudo apt-get install -y ttf-wqy-zenhei sudo apt-get install -y scim-pinyin看门狗(防止树莓派死机的监控)当利用树莓派来做一些需要长期待机的应用时，如下载机、云储存、家庭影院等应用，我们往往会遇到的一个问题就是树莓派会因为过热而死机，需要我们重新启动树莓派，然后再次开启树莓派上的应用。这会给我们的日常操作带来许多麻烦。 Watchdog（看门狗）就能让树莓派永不死机。 //树莓派自带看门狗模块，我们需要添加进去就好。 sudo modprobe bcm2708_wdog echo -e &quot;\\nbcm2708_wdog&quot; &amp;gt; sudo tee -a /etc/modules // 安装看门狗软件 sudo apt-get install -y chkconfig watchdog // 配置 sudo vim /etc/watchdog.conf // 去掉&quot;watchdog-device=/dev/watchdog&quot;这一行的#注释 // 其它配置参考如下: # 用于设定CPU温度重启条件 temperature-device = /sys/class/thermal/thermal_zone0/temp # 最大温度为100度，超过立即重启 max-temperature = 100000 # 1分钟最多进程为24个，超过即重启 max-load-15=12 # 5分钟最多进程为18个，超过即重启 max-load-15=12 # 15分钟最多进程为12个，超过即重启 max-load-15=12 // 完成配置后，启动看门狗 sudo /etc/init.d/watchdog start // 设置为开机自启 chkconfig watchdog onScreen(让树莓派永不失联)利用SSH（Serare Shell，安全外壳协议）来远程控制树莓派应该是我们最常用的 操作树莓派的方式，但在用SSH连接时，我们常常会遇到连接突然断开的问题。连 接一旦断开，原米我们进行的操作也就中断了，若再使用，就得从头再来了。相信你肯定因为电脑待机而中断树莓派的任务而苦恼过。 Screen来让树莓派永不失联的方法。此方法下，就算连接断开了，当我们重新连接后依旧进行原来的操作，而不需要从头再来。 // 直接安装Screen sudo apt-get install -y screen // 开启一个后台view（后台的终端，不会因为断开连接而终止） screen -S 终端名 // 然后就可以继续你的操作了","categories":[{"name":"RPI","slug":"RPI","permalink":"https://lxb.wiki/categories/RPI/"}],"tags":[]},{"title":"树莓派3B+ 安装系统","slug":"树莓派3B+安装系统","date":"2018-10-17T14:11:38.000Z","updated":"2020-05-10T03:25:16.308Z","comments":true,"path":"cfbe6b0a/","link":"","permalink":"https://lxb.wiki/cfbe6b0a/","excerpt":"","text":"安装步骤: 官网下载系统 -- 刷入TF卡 -- 设置开启显示器和SSH -- 通电 -- 进入系统 0. 很重要装完系统，写完 wpa_supplicant.conf 配置文件后，无论如何不要 reboot，不要 reboot， 不要 reboot ！！！3B+ 有极大的概率，reboot 后无法正常连接 WIFI，只能重新烧录系统。 1. 进入官方网站下载系统镜像官方系统 raspbian地址 https://www.raspberrypi.org/downloads/ 2. Windows系统下的安装2.1 下载SD格式化工具SDFormatter 地址 https://www.sdcard.org/downloads/formatter\\_4/eula\\_windows/ 安装后直接用默认选项 格式化SD卡 2.2 下载写镜像工具Win32 DiskImager 地址 http://sourceforge.net/projects/win32diskimager/ 3. MAC系统下的安装3.1 查看当前已挂载的卷[liuxb@liuxb-mac]$ df -h Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk1 112Gi 81Gi 30Gi 73% 1014786 4293952493 0% / devfs 188Ki 188Ki 0Bi 100% 654 0 100% /dev map -hosts 0Bi 0Bi 0Bi 100% 0 0 100% /net map auto_home 0Bi 0Bi 0Bi 100% 0 0 100% /home /dev/disk2s3 92Gi 51Gi 41Gi 56% 336662 42525054 1% /Volumes/系统 /dev/disk2s4 20Gi 15Gi 4.4Gi 78% 92859 4579733 2% /Volumes/数据 /dev/disk3s1 29Gi 2.3Mi 29Gi 1% 107876 8373436 2% /Volumes/未命名对比Size和Name可以找到SD卡的分区在系统里对应的设备文件（这里是/dev/disk3s1），如果你有多个分区，可能还会有disk3s2之类的 3.2 使用diskutil unmount将分区卸载[liuxb@liuxb-mac]$ diskutil unmount /dev/disk3s1 Volume 未命名 on disk3s1 unmounted3.3 先对下载的zip压缩包进行解压，然后使用dd命令将系统镜像写入，需要特别特别注意disk后的数字，不能搞错说明：/dev/disk3s1是分区，/dev/disk3是块设备，/dev/rdisk3是原始字符设备 [liuxb@liuxb-mac]$ unzip 2017-09-07-raspbian-stretch.zip [liuxb@liuxb-mac]$ sudo dd bs=16m if=2017-09-07-raspbian-stretch.img of=/dev/rdisk3 _ 输入用户密码经过几分钟的等待，出现下面的提示，说明TF卡刷好了： 1172+1 records in 1172+1 records out 4916019200 bytes transferred in 127.253638 secs (9691442 bytes/sec)4. 开启SSH在TF卡分区里创建一个名为”ssh”的不带后缀的空文件 5. 开启强制HDMI输出在TF卡分区，打开config.txt文件(开机后位置： /boot/config.txt)，修改如下： hdmi_safe=1 config_hdmi_boost=4 hdmi_ignore_edid=0xa5000080 hdmi_group=2 hdmi_mode=82参数介绍: 项 解释 hdmi_safe=1 安全启动HDMI config_hdmi_boost=4 开启热插拔 hdmi_group=1 CEA电视显示器 hdmi_group=2 DMT电脑显示器 hdmi_ignore_edid=0xa5000080 忽略自动探测的分辨率 输出分辨率： hdmi_mode=4 640x480 60Hz hdmi_mode=9 800x600 60Hz hdmi_mode=16 1024x768 60Hz hdmi_mode=82 1080p 60Hz 6.设置无线WI-FI连接：（假设没有网线，而且没能连接显示器）在TF卡的boot分区，创建wpa_supplicant.conf文件，加入如下内容： country=CN ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=&quot;lxb-wifi&quot; psk=&quot;123456789&quot; priority=1 }在树莓派通电后会自动添加到/etc/wpa_supplicant/wpa_supplicant.conf文件里面，进行自动连接。 // 详细介绍： #ssid:网络的ssid #psk:密码 #priority:连接优先级，数字越大优先级越高（不可以是负数） #scan_ssid:连接隐藏WiFi时需要指定该值为1 // 如果WiFi 没有密码 network={ ssid=&quot;无线网络名称（ssid）&quot; key_mgmt=NONE } // 如果WiFi 使用WEP加密 network={ ssid=&quot;无线网络名称（ssid）&quot; key_mgmt=NONE wep_key0=&quot;wifi密码&quot; } // 如果你的 WiFi 使用WPA/WPA2加密 network={ ssid=&quot;无线网络名称（ssid）&quot; key_mgmt=WPA-PSK psk=&quot;wifi密码&quot; }以上设置完成后, TF卡可以插入树莓派了, 通电. 默认登录账号:pi 密码: raspberry","categories":[{"name":"RPI","slug":"RPI","permalink":"https://lxb.wiki/categories/RPI/"}],"tags":[]},{"title":"Go 发送邮件","slug":"Go 发送邮件","date":"2018-09-01T14:26:49.000Z","updated":"2019-10-03T16:53:11.989Z","comments":true,"path":"c296dcc8/","link":"","permalink":"https://lxb.wiki/c296dcc8/","excerpt":"","text":"需要引入 smtp包 mail.go package main import ( &quot;bytes&quot; &quot;encoding/base64&quot; &quot;fmt&quot; &quot;io/ioutil&quot; &quot;net/smtp&quot; &quot;strings&quot; ) const const_smtp_server = &quot;server-ip:port&quot; //const const_email_content_type = &quot;Content-Type: text/plain; charset=UTF-8&quot; const const_email_content_type = &quot;Content-Type: text/html; charset=UTF-8&quot; const const_boundary = &quot;THIS_IS_THE_BOUNDARY_FOR_EMAIL_BY_LXB&quot; func SendEmail(sender string, receivers []string, subject string, content string, attach_files []string) error { var buf bytes.Buffer buf.WriteString(&quot;To: &quot;) buf.WriteString(strings.Join(receivers, &quot;,&quot;)) buf.WriteString(&quot;\\r\\nFrom: &quot;) //nickname := strings.Split(sender,&quot;@&quot;)[0] //buf.WriteString(nickname) buf.WriteString(&quot;&lt;&quot;) buf.WriteString(sender) buf.WriteString(&quot;&gt;&quot;) buf.WriteString(&quot;\\r\\nSubject: &quot;) buf.WriteString(subject) buf.WriteString(&quot;\\r\\nContent-Type: multipart/mixed; boundary=&quot;) buf.WriteString(const_boundary) buf.WriteString(&quot;\\r\\n--&quot;) buf.WriteString(const_boundary) buf.WriteString(&quot;\\r\\n&quot;) buf.WriteString(const_email_content_type) buf.WriteString(&quot;\\r\\n\\r\\n&quot;) buf.WriteString(content) buf.WriteString(&quot;\\r\\n\\r\\n--&quot;) buf.WriteString(const_boundary) buf.WriteString(&quot;\\r\\n&quot;) for _, filepath := range attach_files { // 第一个附件 filedepts := strings.Split(filepath, &quot;/&quot;) filename := filedepts[len(filedepts)-1] buf.WriteString(&quot;Content-Type: application/octet-stream\\r\\n&quot;) buf.WriteString(&quot;Content-Description: 附件\\r\\n&quot;) buf.WriteString(&quot;Content-Transfer-Encoding: base64\\r\\n&quot;) buf.WriteString(&quot;Content-Disposition: attachment; filename=\\&quot;&quot; + filename + &quot;\\&quot;\\r\\n\\r\\n&quot;) //读取并编码文件内容 attaData, err := ioutil.ReadFile(filepath) if err != nil { print(err) return err } b := make([]byte, base64.StdEncoding.EncodedLen(len(attaData))) base64.StdEncoding.Encode(b, attaData) buf.Write(b) buf.WriteString(fmt.Sprintf(&quot;\\r\\n--%s\\r\\n&quot;, const_boundary)) } fmt.Println(buf.String()) err := smtp.SendMail(const_smtp_server, nil, sender, receivers, buf.Bytes()) fmt.Println(&quot;send mail err:&quot;, err) return err }main.go package main import ( //&quot;flag&quot; //&quot;fmt&quot; &quot;os&quot; ) func main() { //var task string //flag.StringVar(&amp;task, &quot;t&quot;, &quot;&quot;, &quot;task id&quot;) //flag.Parse() //if task == &quot;&quot; { // fmt.Println(&quot;task is required.&quot;) // flag.Usage() // os.Exit(2) //} testStr := os.Args[1] cont := &quot;&lt;html&gt;&lt;body&gt;&lt;p align=\\&quot;center\\&quot;&gt;表: 1&lt;/p&gt;&lt;table align=\\&quot;center\\&quot; border=\\&quot;1\\&quot; cellpadding=\\&quot;10\\&quot;&gt;&lt;tr&gt;&lt;td&gt;任务ID&lt;/td&gt;&lt;td&gt;列1&lt;/td&gt;&lt;td&gt;列2&lt;/td&gt;&lt;td&gt;列3&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;td&gt;&quot; + testStr + &quot;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;&quot; sender := &quot;&quot; rcvs := []string{} sbj := &quot;test email&quot; // cont := &quot;This is content&quot; file := []string{} SendEmail(sender, rcvs, sbj, cont, file) }","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[{"name":"邮件","slug":"邮件","permalink":"https://lxb.wiki/tags/邮件/"}]},{"title":"Shell数组笔记","slug":"Shell数组笔记","date":"2018-08-30T14:33:15.000Z","updated":"2019-10-03T07:57:15.387Z","comments":true,"path":"21c4d609/","link":"","permalink":"https://lxb.wiki/21c4d609/","excerpt":"","text":"Bash shell 只支持一维数组. 初始化时不需要定义数组大小(与 PHP 类似). 数组元素的下标由0开始 shell 数组用括号来表示, 元素用”空格”符号分隔开, 语法: array_name=(value1 value2 ...valuen) 实例#!/bin/bash my_array=(A B &quot;C&quot; D)也可以用下标来定义数组 array_name[0]=value0 array_name[1]=value1 array_name[2]=value2读取数组${array_name[index]} 实例#!/bin/bash my_array=(A B &quot;C&quot; D) echo &quot;第一个元素为: ${my_array[0]}&quot; echo &quot;第二个元素为: ${my_array[1]}&quot; echo &quot;第三个元素为: ${my_array[2]}&quot; echo &quot;第四个元素为: ${my_array[3]}&quot;获取数组中的所有元素使用@ 或 * 可以后去数组中的所有元素 #!/bin/bash my_array[0]=A my_array[1]=B my_array[2]=C my_array[3]=D echo &quot;数组的元素为: ${my_array[*]}&quot; echo &quot;数组的元素为: ${my_array[@]}&quot;获取数组的长度获取数组长度的方法与获取字符串长度的方法相同 #!/bin/bash my_array[0]=A my_array[1]=B my_array[2]=C my_array[3]=D echo &quot;数组元素个数为: ${#my_array[*]}&quot; echo &quot;数组元素个数为: ${#my_array[@]}&quot;","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"split命令","slug":"split命令","date":"2018-08-30T14:23:34.000Z","updated":"2019-10-03T07:57:15.389Z","comments":true,"path":"aa4c47b6/","link":"","permalink":"https://lxb.wiki/aa4c47b6/","excerpt":"","text":"选项-b 值为每一个输出档案的大小, 单位为byte -C 每一个输出档中, 单行的最大byte 数 -d 使用数字作为后缀 -l 值为每一个输出档的行数大小实例生成一个大小为100KB 的测试文件 dd if=/dev/zero bs=100k count=1 of=date.file 1+0 records in 1+0 records out 102400 bytes (102 kB) copied, 0.00043 seconds, 238 MB/s使用split 命令将上面创建的date.file文件分割成大小为10KB 的小文件 $ split -b 10k date.file $ ls date.file xaa xab xac xad xae xaf xag xah xai xaj文件被分割成带有字母的后缀文件, 如果想用数字后缀可使用-d参数, 同时可以使用-a length指定后缀的长度 [root@localhost split]# split -b 10k date.file -d -a 3 [root@localhost split]# ls date.file x000 x001 x002 x003 x004 x005 x006 x007 x008 x009为分割后的文件指定文件名的前缀 [root@localhost split]# split -b 10k date.file -d -a 3 split_file [root@localhost split]# ls date.file split_file000 split_file001 split_file002 split_file003 split_file004 split_file005 split_file006 split_file007 split_file008 split_file009使用-l选项根据文件的行数来分割文件,如把文件分割成每个包含10行的小文件 split -l 10 date.file","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"mysql启动时 &quot;No space left on device&quot;","slug":"mysql启动时 No space left on device","date":"2018-08-28T11:04:37.000Z","updated":"2019-10-03T16:56:31.273Z","comments":true,"path":"77f38978/","link":"","permalink":"https://lxb.wiki/77f38978/","excerpt":"","text":"先用free 命令查看剩余空间 [root@tokyo mysqld]# free total used free shared buff/cache available Mem: 1016108 632132 205776 66344 178200 180496 Swap: 0 0 0发现swap 为零了 执行 dd if=/dev/zero of=/swapfile bs=1M count=1024 mkswap /swapfile wapon /swapfile swapon /swapfile再用free查看 [root@tokyo ~]# free total used free shared buff/cache available Mem: 1016108 732148 63984 51400 219976 71132 Swap: 1048572 209240 839332启动mysql, 解决","categories":[{"name":"DB","slug":"DB","permalink":"https://lxb.wiki/categories/DB/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://lxb.wiki/tags/mysql/"}]},{"title":"Linux 后台执行命令","slug":"linux-e5-90-8e-e5-8f-b0-e6-89-a7-e8-a1-8c-e5-91-bd-e4-bb-a4","date":"2018-08-27T17:01:30.000Z","updated":"2019-10-03T07:57:15.352Z","comments":true,"path":"be78f922/","link":"","permalink":"https://lxb.wiki/be78f922/","excerpt":"","text":"当我们在终端或控制台工作时，可能不希望由于运行一个作业而占住了屏幕，因为可能还有更重要的事情要做，比如阅读电子邮件。对于密集访问磁盘的进程，我们更希望它能够在每天的非负荷高峰时间段运行(例如凌晨)。为了使这些进程能够在后台运行，也就是说不在终端屏幕上运行，有几种选择方法可供使用 &amp; nohup ctrl + z ctrl + c jobs bg fg &amp;当在前台运行某个作业时，终端被该作业占据；可以在命令后面加上&amp; 实现后台运行。例如：sh test.sh &amp; 适合在后台运行的命令有f i n d、费时的排序及一些s h e l l脚本。在后台运行作业时要当心：需要用户交互的命令不要放在后台执行，因为这样你的机器就会在那里傻等。不过，作业在后台运行一样会将结果输出到屏幕上，干扰你的工作。如果放在后台运行的作业会产生大量的输出，最好使用下面的方法把它的输出重定向到某个文件中： command &gt; out.file 2&gt;&amp;1 &amp; 当你成功地提交进程以后，就会显示出一个进程号，可以用它来监控该进程，或杀死它。(ps -ef | grep 进程号 或者 kill -9 进程号） nohup使用&amp;命令后，作业被提交到后台运行，当前控制台没有被占用，但是一但把当前控制台关掉(退出帐户时)，作业就会停止运行。nohup命令可以在你退出帐户之后继续运行相应的进程。nohup就是不挂起的意思( no hang up)。该命令的一般形式为： nohup command &amp;如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件： nohup command &gt; myout.file 2&gt;&amp;1 &amp;使用了nohup之后，很多人就这样不管了，其实这样有可能在当前账户非正常退出或者结束的时候，命令还是自己结束了。所以在使用nohup命令后台运行命令之后，需要使用exit正常退出当前账户，这样才能保证命令一直在后台运行 ctrl + z可以将一个正在前台执行的命令放到后台，并且处于暂停状态 ctrl + c终止前台命令 jobs查看当前有多少在后台运行的命令。 jobs -l选项可显示所有任务的PID，jobs的状态可以是running, stopped, Terminated。但是如果任务被终止了（kill），shell 从当前的shell环境已知的列表中删除任务的进程标识 bg将一个在后台暂停的命令，变成继续执行 （在后台执行） 如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid) 将任务转移到后台运行： 先ctrl + z；再bg，这样进程就被移到后台运行，终端还能继续接受命令。 fg将后台中的命令调至前台继续运行 如果后台中有多个命令，可以用 fg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid)","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[]},{"title":"Golang select 的用法","slug":"golang-select-e7-9a-84-e7-94-a8-e6-b3-95","date":"2018-08-26T16:27:40.000Z","updated":"2019-10-03T07:57:15.314Z","comments":true,"path":"e353ee8e/","link":"","permalink":"https://lxb.wiki/e353ee8e/","excerpt":"","text":"基本使用select 是 Go 中的一个控制结构, 类似于switch 语句, 用于处理异步 IO 操作. select 语句会监听 case语句中channel 的读写操作, 当case 中 channel 读写操作为非阻塞状态(即能读写)时, 将会触发相应的动作. select 中的 case 语句必须是一个 channel 操作 select 中的 default 子句总是可运行的 如果有多个 case 都可以运行, select 会随机公平地选出一个执行, 其他不会执行 如果没有可运行的 case 语句, 且有 default 语句, 则会执行 default 的动作 如果没有可运行的 case 语句, 且没有 default 语句, select 将阻塞, 知道某个 case 通信可以运行 例 package main import &quot;fmt&quot; func main() { var c1, c2, c3 chan int var i1, i2 int select { case i1 = &lt;-c1: fmt.Printf(&quot;received &quot;, i1, &quot; from c1\\n&quot;) case c2 &lt;- i2: fmt.Printf(&quot;sent &quot;, i2, &quot; to c2\\n&quot;) case i3, ok := (&lt;-c3): // same as: i3, ok := &lt;-c3 if ok { fmt.Printf(&quot;received &quot;, i3, &quot; from c3\\n&quot;) } else { fmt.Printf(&quot;c3 is closed\\n&quot;) } default: fmt.Printf(&quot;no communication\\n&quot;) } } //输出：no communication典型用法1. 超时判断//比如在下面的场景中，使用全局resChan来接受response，如果时间超过3S,resChan中还没有数据返回，则第二条case将执行 var resChan = make(chan int) // do request func test() { select { case data := &lt;-resChan: doData(data) case &lt;-time.After(time.Second * 3): fmt.Println(&quot;request time out&quot;) } } func doData(data int) { //... }2. 退出//主线程（协程）中如下： var shouldQuit=make(chan struct{}) fun main(){ { //loop } //...out of the loop select { case &lt;-c.shouldQuit: cleanUp() return default: } //... } //再另外一个协程中，如果运行遇到非法操作或不可处理的错误，就向shouldQuit发送数据通知程序停止运行 close(shouldQuit)3. 判断 channel 是否阻塞//在某些情况下是存在不希望channel缓存满了的需求的，可以用如下方法判断 ch := make (chan int, 5) //... data：=0 select { case ch &lt;- data: default: //做相应操作，比如丢弃data。视需求而定 }","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[]},{"title":"Redis 笔记","slug":"Redis 笔记","date":"2018-07-19T10:51:37.000Z","updated":"2019-10-03T17:01:06.013Z","comments":true,"path":"9f188831/","link":"","permalink":"https://lxb.wiki/9f188831/","excerpt":"","text":"数据类型Redis 支持5中数据类型 字符串(string)Redis 中字符串是一个字节序列. Redis 中的字符串是二进制安全的, 这意味着它们的长度不由任何特殊的终止字符决定。因此，可以在一个字符串中存储高达512兆字节的任何内容 例 redis 127.0.0.1:6379&gt; SET name &quot;value&quot; OK redis 127.0.0.1:6379&gt; GET name &quot;value&quot; Redis命令不区分大小写.字符串的最大长度为512M散列/哈希(Hash)Redis散列/哈希(Hashes)是键值对的集合。Redis散列/哈希是字符串字段和字符串值之间的映射。因此，它们用于表示对象。 例 redis 127.0.0.1:6379&gt; HMSET ukey username &quot;yiibai&quot; password &quot;passswd123&quot; points 200散列/哈希数据类型用于存储包含用户的基本信息的用户对象。这里HMSET，HGETALL是Redis的命令，而ukey是键的名称。 每个散列/哈希可以存储多达2^32 - 1个健-值对(超过40亿个)。 列表(List)Redis列表只是字符串列表，按插入顺序排序。可以向Redis列表的头部或尾部添加元素。 例 redis 127.0.0.1:6379&gt; lpush alist redis (integer) 1 redis 127.0.0.1:6379&gt; lpush alist mongodb (integer) 2 redis 127.0.0.1:6379&gt; lpush alist sqlite (integer) 3 redis 127.0.0.1:6379&gt; lrange alist 0 10 1) &quot;sqlite&quot; 2) &quot;mongodb&quot; 3) &quot;redis&quot;集合(Set)Redis集合是字符串的无序集合。在Redis中，可以添加，删除和测试成员存在的时间O(1)复杂性 例 redis 127.0.0.1:6379&gt; sadd yiibailist redis (integer) 1 redis 127.0.0.1:6379&gt; sadd yiibailist mongodb (integer) 1 redis 127.0.0.1:6379&gt; sadd yiibailist sqlite (integer) 1 redis 127.0.0.1:6379&gt; sadd yiibailist sqlite (integer) 0 redis 127.0.0.1:6379&gt; smembers yiibailist 1) &quot;sqlite&quot; 2) &quot;mongodb&quot; 3) &quot;redis&quot; 注意 - 在上面的示例中，sqlite被添加了两次，但是由于集合的唯一属性，所以它只算添加一次可排序集合(ZSET)Redis可排序集合类似于Redis集合，是不重复的字符集合。 不同之处在于，排序集合的每个成员都与分数相关联，这个分数用于按最小分数到最大分数来排序的排序集合。虽然成员是唯一的，但分数值可以重复 例 redis 127.0.0.1:6379&gt; zadd yiibaiset 0 redis (integer) 1 redis 127.0.0.1:6379&gt; zadd yiibaiset 0 mongodb (integer) 1 redis 127.0.0.1:6379&gt; zadd yiibaiset 1 sqlite (integer) 1 redis 127.0.0.1:6379&gt; zadd yiibaiset 1 sqlite (integer) 0 redis 127.0.0.1:6379&gt; ZRANGEBYSCORE yiibaiset 0 1000 1) &quot;mongodb&quot; 2) &quot;redis&quot; 3) &quot;sqlite&quot; 因为 ‘sqlite‘ 的排序值是 1 ，其它两个元素的排序值是 0 ，所以 ‘sqlite‘ 排在最后一个位置上","categories":[{"name":"DB","slug":"DB","permalink":"https://lxb.wiki/categories/DB/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://lxb.wiki/tags/redis/"}]},{"title":"Go 环境变量","slug":"Go 环境变量","date":"2018-07-17T16:57:36.000Z","updated":"2019-10-03T07:57:15.313Z","comments":true,"path":"2ddd6919/","link":"","permalink":"https://lxb.wiki/2ddd6919/","excerpt":"","text":"GOROOT，在Linux系统中一般安装在/usr/go或者/usr/local/go，这样Linux系统中的PATH变量一般都包含了这两个目录，所以就可以直接运行go命令，而Windows系统中一般默认安装在C:\\go中 自定义 GO安装路径, 可修改环境变量配置文件 export GOROOT=$HOME/go GOPATHgo的工作目录，这个目录指定了需要从哪个地方寻找GO的包、可执行程序等，这个目录可以是多个目录表示，go编译或者运行时会从这个环境变量中去对应查找，工作目录或者如官方文档中说的workspace 在这个目录进行编译、链接最后生成所需要的库、可执行文件，我们对比C程序的目录，也许更能方便理解，一般在C的工程项目中包含三个文件，一个include目录、src目录、Makefile文件。 include目录存放了所有的头文件可供其他地方包含 src目录则存放所有的.c后缀的源文件 Makefile则是该项目的编译，在编译整个工程时需要执行make命令，这里就发现GO就不需要去写什么Makefile了，执行go build xxx.go命令就可以编译 GOPATH 下的目录下, 一般有三个 目录 bin pkg src bin目录包含了可执行程序，注意是可执行的，不需要解释执行。 pkg目录包含了使用的包或者说库。 src里面包含了go的代码源文件，其中仍按包的不同进行组织 包名一般和目录名相同, 编译时, 可以在某个包下, 执行go build , 也可以在包上层直接编译包名go build pkg_name go install &lt;pkg_name/exe_name/all&gt; 先编译后把编译生成的可执行文件复制到bin 下","categories":[{"name":"Golang","slug":"Golang","permalink":"https://lxb.wiki/categories/Golang/"}],"tags":[]},{"title":"Linux 安装bashmarks","slug":"linux-e5-ae-89-e8-a3-85bashmarks","date":"2018-07-12T16:34:43.000Z","updated":"2019-10-03T07:57:15.353Z","comments":true,"path":"472d58f/","link":"","permalink":"https://lxb.wiki/472d58f/","excerpt":"","text":"下载源码git clone https://github.com/lxbwolf/bashmarks.git 把bashmarks.sh复制到~/bin/ 添加环境变量在环境变量文件里, 添加 . ~/bin/bashmarks.sh 相关命令s &lt;bookmark_name&gt; - Saves the current directory as &quot;bookmark_name&quot; g &lt;bookmark_name&gt; - Goes (cd) to the directory associated with &quot;bookmark_name&quot; p &lt;bookmark_name&gt; - Prints the directory associated with &quot;bookmark_name&quot; d &lt;bookmark_name&gt; - Deletes the bookmark l - Lists all available bookmarks","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"Linux 安装thefuck","slug":"linux-e5-ae-89-e8-a3-85thefuck","date":"2018-07-12T16:30:33.000Z","updated":"2019-10-03T07:57:15.354Z","comments":true,"path":"42c1114f/","link":"","permalink":"https://lxb.wiki/42c1114f/","excerpt":"","text":"下载源码git clone https://github.com/lxbwolf/thefuck.git 配置环境变量 把thefuck/**/libexec/bin 添加进环境变量 eval $(thefuck --alias fuck)","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"Linux 安装samba","slug":"Linux 安装samba","date":"2018-07-12T16:22:04.000Z","updated":"2019-10-03T16:57:11.741Z","comments":true,"path":"8f5b70d0/","link":"","permalink":"https://lxb.wiki/8f5b70d0/","excerpt":"","text":"开发机安装 sambayum install samba samba-client samba-swat 添加账号sampasswd -a 用户名 用户名只能为已经存在的账号 配置共享文件夹编辑etc/samba/smb.conf, 追加内容: [samba_share_dir] comment = samba_share path = /home/lxb/samba_share create mask = 0664 directory mask = 0775 writable = yes valid users = lxb browseable = yes配置环境变量环境变量文件添加: export LD_LIBRARY_PATH=/usr/local/samba/lib:$LD_LIBRARY_PATH samba 重启sudo /etc/init.d/smb restart MAC客户端连接Finder -&gt; 前往 -&gt; 连接服务器 -&gt; 输入smb地址 smb://user_name@IP/samba_share_dir","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[{"name":"samba","slug":"samba","permalink":"https://lxb.wiki/tags/samba/"}]},{"title":"nginx 配置中的rewrite","slug":"nginx 配置中的rewrite","date":"2018-06-02T14:35:47.000Z","updated":"2019-10-03T16:58:09.919Z","comments":true,"path":"389c639/","link":"","permalink":"https://lxb.wiki/389c639/","excerpt":"","text":"语法 rewrite regex replacement flag flag有如下: - last - break 中止 rewrite, 不再继续匹配 - redirect 返回临时重定向的 HTTP 状态302 - permanet 返回永久重定向的 HTTP 状态301 last 和 break 的不同:break 是终止当前location 的 rewrite 检测, 且不再进行 location 匹配;last是终止当前location的rewrite检测,但会继续重试location匹配并处理区块中的rewrite规则 下面是可以用来判断的表达式: -f 和!-f 判断是否存在文件 -d 和!-d 判断是否存在目录 -e 和!-e 判断是否存在文件或目录 -x 和!-x 判断文件是否可执行 下面是可以用作判断的全局变量 $args 等于请求行中的参数 $content_length 请求头中的Content-length 字段 $content_type 请求头中的Content-Type 字段 $document_root 当前请求在root 指令中指定的值 $host 请求主机头字段, 否则为服务器名称 $http_user_agent 客户端agent 信息 $http_cookie 客户端cookie 信息 $limit_rate 这个变量可以限制连接速率 $request_body_file 客户端请求主题信息的临时文件名 $request_method #客户端请求的动作，通常为GET或POST。 $remote_addr #客户端的IP地址。 $remote_port #客户端的端口。 $remote_user #已经经过Auth Basic Module验证的用户名。 $request_filename #当前请求的文件路径，由root或alias指令与URI请求生成。 $query_string #与$args相同。 $scheme #HTTP方法（如http，https）。 $server_protocol #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 $server_addr #服务器地址，在完成一次系统调用后可以确定这个值。 $server_name #服务器名称。 $server_port #请求到达服务器的端口号。 $request_uri #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。 $uri 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。 $document_uri #与$uri相同 例: http://localhost:88/test1/test2/test.php $host: localhost $server_post: 88 $request_uri: http://localhost:88/test1/test2/test.php $document_uri: /test1/test2/test.php $document_root: /usr/share/nginx/html (在nginx.conf里配置的) $request_filename: /usr/share/nginx/html/test1/test2/test.php (在nginx.conf里配置的)详例:多目录转成参数abc.domain.com/sort/2 =&gt; abc.domain.com/index.php?act=sort&amp;name=abc&amp;id=2 if ($host ~* (.*)\\.domain\\.com) { set $sub_name $1; rewrite ^/sort\\/(\\d+)\\/?$ /index.php?act=sort&amp;cid=$sub_name&amp;id=$1 last; }目录对换/123456/xxxx =&gt; /xxxx?id=123456 rewrite ^/(\\d+)\\/(.+)/ /$2?id=$1 last; // rewrite ^/\\/(\\d+)\\/(\\w+)\\/? /$2?id=$1 last;如果使用IE浏览器, 则重定向到/nginx-ie 目录下 if ($http_user_agent ~ MSIE) { rewrite ^(.*)$ /nginx-ie/$1 break; }*目录自动加 / * 1. if (-d $request_filename){ 2. rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent; 3. }禁止htaccess 1. location ~/\\.ht { 2. deny all; 3. }禁止多个目录 1. location ~ ^/(cron|templates)/ { 2. deny all; 3. break; 4. }禁止以/data开头的文件 可以禁止/data/下多级目录下.log.txt等请求; 1. location ~ ^/data { 2. deny all; 3. }禁止单个目录 不能禁止.log.txt能请求 1. location /searchword/cron/ { 2. deny all; 3. }禁止单个文件 1. location ~ /data/sql/data.sql { 2. deny all; 3. }给favicon.ico和robots.txt设置过期时间;这里为favicon.ico为99 天,robots.txt为7天并不记录404错误日志 1. location ~(favicon.ico) { 2. log_not_found off; 3. expires 99d; 4. break; 5. } 6. 7. location ~(robots.txt) { 8. log_not_found off; 9. expires 7d; 10. break; 11. }设定某个文件的过期时间;这里为600秒，并不记录访问日志 1. location ^~ /html/scripts/loadhead_1.js { 2. access_log off; 3. root /opt/lampp/htdocs/web; 4. expires 600; 5. break; 6. }文件反盗链并设置过期时间这里的return 412 为自定义的http状态码，默认为403，方便找出正确的盗链的请求“rewrite ^/ http://leech.c1gstudio.com/leech.gif;”显示一张防盗链图片“access_log off;”不记录访问日志，减轻压力“expires 3d”所有文件3天的浏览器缓存 1. location ~* ^.+\\.(jpg|jpeg|gif|png|swf|rar|zip|css|js)$ { 2. valid_referers none blocked *.c1gstudio.com *.c1gstudio.net localhost 208.97.167.194; 3. if ($invalid_referer) { 4. rewrite ^/ http://leech.c1gstudio.com/leech.gif; 5. return 412; 6. break; 7. } 8. access_log off; 9. root /opt/lampp/htdocs/web; 10. expires 3d; 11. break; 12. }只充许固定ip访问网站，并加上密码 1. root /opt/htdocs/www; 2. allow 208.97.167.194; 3. allow 222.33.1.2; 4. allow 231.152.49.4; 5. deny all; 6. auth_basic &quot;C1G_ADMIN&quot;; 7. auth_basic_user_file htpasswd;将多级目录下的文件转成一个文件，增强seo效果/job-123-456-789.html 指向 /job/123/456/789.html 1. rewrite ^/job-([0-9]+)-([0-9]+)-([0-9]+)\\.html$ /job/$1/$2/jobshow_$3.html last;将根目录下某个文件夹指向2级目录如/shanghaijob/ 指向 /area/shanghai/ 如果你将last改成permanent，那么浏览器地址栏显是 /location/shanghai/ 1. rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;上面例子有个问题是访问/shanghai 时将不会匹配 1. rewrite ^/([0-9a-z]+)job$ /area/$1/ last; 2. rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;这样/shanghai 也可以访问了，但页面中的相对链接无法使用， 如./list_1.html真实地址是/area /shanghia/list_1.html会变成/list_1.html,导至无法访问。那我加上自动跳转也是不行咯 (-d $request_filename)它有个条件是必需为真实目录，而我的rewrite不是的，所以没有效果 1. if (-d $request_filename){ 2. rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent; 3. }知道原因后就好办了，让我手动跳转吧 1. rewrite ^/([0-9a-z]+)job$ /$1job/ permanent; 2. rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;文件和目录不存在的时候重定向： 1. if (!-e $request_filename) { 2. proxy_pass http://127.0.0.1; 3. }域名跳转 1. server 2. { 3. listen 80; 4. server_name jump.c1gstudio.com; 5. index index.html index.htm index.php; 6. root /opt/lampp/htdocs/www; 7. rewrite ^/ http://www.c1gstudio.com/; 8. access_log off; 9. }多域名转向 1. server_name www.c1gstudio.com www.c1gstudio.net; 2. index index.html index.htm index.php; 3. root /opt/lampp/htdocs; 4. if ($host ~ &quot;c1gstudio\\.net&quot;) { 5. rewrite ^(.*) http://www.c1gstudio.com$1 permanent; 6. }三级域名跳转 1. if ($http_host ~* &quot;^(.*)\\.i\\.c1gstudio\\.com$&quot;) { 2. rewrite ^(.*) http://top.yingjiesheng.com$1; 3. break; 4. }域名镜向 1. server 2. { 3. listen 80; 4. server_name mirror.c1gstudio.com; 5. index index.html index.htm index.php; 6. root /opt/lampp/htdocs/www; 7. rewrite ^/(.*) http://www.c1gstudio.com/$1 last; 8. access_log off; 9. }","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://lxb.wiki/tags/nginx/"}]},{"title":"nginx配置中的location","slug":"nginx配置中的location","date":"2018-06-02T14:30:30.000Z","updated":"2019-10-03T16:57:58.088Z","comments":true,"path":"a209000e/","link":"","permalink":"https://lxb.wiki/a209000e/","excerpt":"","text":"语法location [=|~|~*|^~] /uri/ {...} 上下文: server 此命令随URL 不同而接受不同的结构. 可以配置使用常规字符串和正则表达式. 若使用正则表达式, 则必须使用~*前缀(选择不区分大小写的匹配) 或~前缀(区分大小写的匹配) = 表示uri 以某个常规字符串开头, 理解为匹配url 路径即可. nginx 不对url 做编码, 因此请求为/static/%20%/aa 可以被规则^~ /static/ /aa (有空格) 匹配到. ~ 表示区分大小写的正则匹配 ~* 表示不区分大小写的正则匹配 !~ 和 !~* 分别为区分大小写不匹配 和 不区分大小写不匹配 的正则 / 通用匹配, 任何请求都会匹配到 多个location 配置的情况下, 匹配顺序为: 先匹配=, 其次匹配^~, 再匹配按文件中顺序的正则匹配, 最后匹配/. 当有匹配成功的时候, 停止匹配, 按当前匹配规则处理请求. 例1: location = / { # 规则A } location = /login { # 规则B } location ^~ /static { # 规则C } location ~ \\.(gif|jpg|png|js|css)$ { # 规则D } location ~* \\.png$ { # 规则E } location !~ \\.xhtml$ { # 规则F } location !~* \\.xhtml$ { # 规则G } location / { # 规则H }产生效果如下: 1. 访问/ 根目录, 如http://localhost/ 将匹配规则A 2. 访问http://localhost/login 将匹配规则B; http://localhost/register 将匹配规则H 3. 访问http://localhost/static/a.html 将匹配规则C 4. 访问http://localhost/a.png 讲匹配规则D 和规则E, 但规则D 顺序优先, 规则E 不起作用 5. 访问http://localhost/static/c.png 优先匹配到规则C 6. 访问http://localhost/a.PNG 将匹配规则E 7. 访问http://localhost/a.xhtml 不会匹配到规则F 和规则G, http://localhost/a.XHTML 不会匹配到规则G 8. 访问http://localhost/category/id/1111 匹配到规则H, 因为以上规则都不匹配, 这个时候应该是nginx 转发给后端应用服务器, 如FastCGI(php), tomcat(jsp), nginx 作为反向代理服务器存在. 所以实际使用中, 通常有至少三个匹配规则定义, 如下: # 第一个必选规则 直接匹配网站根, 通过域名访问网站首页比较频繁, 使用这个会加速处理; 这里直接转发给后端应用服务器了, 也可以是一个静态首页 location = / { proxy_pass http://tomcat:8080/index } # 第二个必选规则 处理静态文件请求, 这是nginx 作为http 服务器的强项. 有如下两种配置模式, 目录匹配或后缀匹配, 任选其一或搭配使用 location ^~ /static/ { root /webroot/static/; } location ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ { root /webroot/res/; } # 第三个必选规则 通用规则, 用来转发动态请求到后端应用服务器. 非静态文件请求就默认是动态请求. location / { proxy_pass http://tomcat:8080/ }","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://lxb.wiki/tags/nginx/"}]},{"title":"nginx 寻找index 原理","slug":"nginx 寻找index 原理","date":"2018-06-01T14:32:04.000Z","updated":"2019-10-03T16:57:44.766Z","comments":true,"path":"ab14d5fa/","link":"","permalink":"https://lxb.wiki/ab14d5fa/","excerpt":"","text":"1. nginx 是怎么找index.php 文件的当nginx发现需要/web/echo/index.php 文件时, 就会向内核发起 IO 系统调用(因为要跟硬件打交道, 这里的硬件是指硬盘, 通常需要靠内核来操作, 而内核提供的这些功能是通过系统调用来实现的), 告诉内核, 我需要这个文件, 内核从/ 开始找到web 目录, 再在web 目录下找到echo 目录, 最后在echo 目录下找到index.php 文件, 于是把这个index.php 从硬盘上读取到内核自身的内存空间, 然后再把这个文件复制到nginx进程所在的内存空间, 于是 nginx就得到了自己想要的文件了 2. 寻找文件在文件系统层面是怎么操作的如, nginx 需要得到/web/echo/index.php 这个文件 每个分区(像ext3 等文件系统, block块是文件存储的最小单元, 默认是4096字节) 都是包含元数据区和数据区, 每个文件在元数据区都有元数据条目(一般是128字节大小), 每个条目都有一个编号, 称之为 inode(index node), 这个inode 里包含 文件类型, 权限, 连接次数, 属主和数组的 ID&amp;时间戳, 这个文件占据了哪些磁盘块也就是块的编号(block, 每个文件可以占用多个 block, 且 block 不一定是连续的, 每个 block 都有编号), 如下图: 目录其实也是普通文件, 也需要占用磁盘块, 目录不是一个容器. 默认创建的目录大小为4096字节, 即只需要占用一个磁盘块, 但这是不确定的. 所以要找到目录也是需要到元数据区里找到对应的条目, 只要找到对应的inode就可以找到目录所占用的磁盘块. 目录里存着一张表(映射表), 里面放着 目录或文件的名称和对应的inode号, 如下: - - 文件名称(只是字符串) inode 号 test.txt 100 假如 / 在数据区占据1, 2号 block, `/` 其实也是一个目录, 里面有两个目录, web 和 111 web 占据5号 block, 是目录, 里面有2个目录 echo 和 data echo 占据11号 block, 是目录, 里面有一个文件 index.php index.php 占据15, 16号 block, 是文件其在文件系统中分布如下图: 那么内核究竟是怎么找到index.php 这个文件的呢? 内核拿到 nginx 的 IO 系统调用要获取/web/echo/index.php 这个文件请求之后, 1. 内核读取元数据区 / 的inode, 从 inode 里读取 / 所对应的数据块的编号, 然后在数据区找到其对应的块(1, 2号块), 读取1号块上的映射表找到 web 这个名称在元数据区对应的 inode 号 2. 内核读取 web 对应的 inode(3号), 从中得到 web 在数据区对应的块是5号块, 于是到数据区找到5号块, 从中读取映射表, 知道 echo 对应的 inode 是5号, 于是到元数据区找到5号 inode 3. 内核读取5号 inode, 得到 echo 在数据区对应的事11号块, 于是到数据区读取11号块得到映射表, 得到index.php 对应的 inode 事9号 4. 内核到元数据区读取9号 inode, 得到 index.php 对应的事15号和16号数据块, 于是就到数据区域找到15 16号块, 读取其中的内容, 得到 index.php 的完整内容","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://lxb.wiki/tags/nginx/"}]},{"title":"HTTP协议笔记","slug":"http-e5-8d-8f-e8-ae-ae-e7-ac-94-e8-ae-b0","date":"2018-05-30T15:11:26.000Z","updated":"2019-10-03T16:54:06.517Z","comments":true,"path":"4e7c79/","link":"","permalink":"https://lxb.wiki/4e7c79/","excerpt":"","text":"HTTP协议的特点: 1. 支持客户端/服务器模式 2. 简单快速: 客户端向服务器请求服务时, 只需传送请求方法和路径. 请求方法常用的有GET, HEAD, POST. 每种方法规定了客户端与服务器联系的类型. 由于HTTP协议简单, 使得HTTP服务器的程序规模小, 因而通信速度很快. 3. 灵活: HTTP 允许传输任意类型的数据对象. 正在传输的类型由Content-Type加以标记. 4. 无连接: 限制每次连接只处理一个请求. 服务器处理完客户端的请求, 并收到客户端的应答后, 即断开连接. 采用着用方式可以节省传输时间. 5. 无状态: 无状态是指协议对于事务处理没有记忆能力. 缺少状态意味着如果后续处理需要前面的信息, 则它必须重传, 这样可能导致每次传送的数据量增大. 另一方面, 在服务器不需要先前信息时, 它的应答就较快. URLHTTP是一种基于请求与响应模式的, 无状态的, 应用层的协议, 常基于TCP的连接方式, HTTP1.1版本中给出一种持续连接的机制, 绝大多数的web开发, 都是构建在HTTP协议之上的web应用. HTTP URL(URL是一种特殊类型的URI, 包含了用于查找某个资源的足够的信息)的格式如下: http://host[:port][abs_path] http 表示要通过HTTP协议来定位网络资源 host 表示合法的Internet主机域名或者IP地址 port 指定端口号, 缺省端口为80 abs_path 指定请求资源的URI 若URI 中没有给出abs_path, 那当它作为请求URI时, 必须以”/“ 的形式给出, 通常这个工作浏览器会自动完成 请求HTTP请求由三部分组成: 请求行, 消息报头, 请求正文 请求行以一个方法符号开头, 以空格分开, 后面跟请求的URI和协议的版本, 格式如下: Method Request-URI HTTP-Version CRLF 请求方法有以下几种 - GET 请求获取Request-URI 所标识的资源 - POST 在Request-URI 表标识的资源后附加新的数据 - HEAD 请求获取由Request-URI 所标识的资源的响应消息报头 - PUT 请求服务器存储一个资源, 并用Request-URI 作为其标识 - DELETE 请求服务器删除Request-URI 所标识的资源 - TRACE 请求服务器会送收到的请求信息, 主要用于测试或诊断 - CONNECT 保留将来使用 - OPTIONS 请求查询服务器的性能, 或查询与资源相关的选项和需求 响应HTTP响应由三部分组成: 状态行, 消息报头, 响应正文 状态行格式: HTTP-Version Status-Code Reason-Phrase CRLF 状态码由三位数字组成, 第一个数字定义了响应的类别: - 1xx: 指示信息 – 表示请求已接受, 继续处理 - 2xx: 成功 – 表示请求已被成功接收, 理解, 接受 - 3xx: 重定向 – 要完成请求必须进行更进一步的操作 - 4xx: 客户端错误 – 请求有语法错误或请求无法实现 - 5xx: 服务器端错误 – 服务器未能实现合法的请求 报头HTTP 消息由客户端到服务器的请求和服务器到客户端的响应组成. 请求消息和相应消息都是由开始行(对于请求消息, 开始行就是请求行, 对于响应消息, 开始行就是状态行), 消息报头(可选), 空行(只有CRLF的行), 消息正文(可选) 组成 HTTP 消息报头包括 普通报头, 请求报头, 响应报头, 实体报头 每一个报头域都是由名字 + &quot;:&quot; + 空格 + 值 组成, 消息报头域的名字是大小写无关的. 普通报头在普通报头中, 有少数报头域用于所有的请求和响应消息, 但并用于被传输的实体, 只用于传输的消息 例: Cache-Control 用于指定缓存指令, 缓存指令是单向的(响应中出现的缓存指令在请求中未必会出现), 且是独立的(一个消息的缓存指令不会影响另一个消息处理的缓存机制), HTTP1.0使用类似的报头域为Pragma. 请求时的缓存指令包括: no-cache(用于指示请求或相应消息不能缓存), no-store, max-age, max-stale, min-fresh, only-if-cached; 响应时的缓存指令包括: public, private, no-cache, no-store, no-transform, must-revalidate, proxy-revalidate, max-age, s-maxage eg: 为了指示IE浏览器(客户端)不要缓存页面, 服务器端的JSP程序可以编写如下: response.setHeader(&quot;Cache-Control&quot;, &quot;no-cache&quot;); 或 response.setHeader(&quot;Pragma&quot;, &quot;no-cache&quot;); 两者作用相同, 在发送的响应消息中设置普通报头域: Cache-Control: no-cache. 请求报头请求报头允许客户端向服务器端传递请求的附加信息以及客户端自身的信息. 常用的请求报头域: Accept Accept 请求报头域用于指定客户端接收哪些类型的信息. 如: Accept:image/gif, 表明客户端希望接收GIF图像格式的资源; Accept:text/html, 表明客户端希望接受html 文本 Accept-Charset 指定客户端接受的字符集. 如: Accept-Charset:iso-8859-1,GB2312. 如果请求消息中没有设置这个域, 缺省是任何字符集都可以接受. Accept-Encoding 指定可接受的内容编码. 如Accept-Encoding:gzip.deflate 缺省是任何内容编码都可以接受 Accept-Language 指定一种自然语言. 如 Accept-Language:zh-cn 缺省是任何语言都可以接受 Authorization 用于证明客户端有权查看某个资源. 当浏览器访问一个页面时, 如果收到服务器的响应代码为401, 可以发送一个包含Authorization请求报头域的请求, 要求服务器对其进行验证 Host(发送请求时, 该报头域是必需的) 指定被请求资源的Internet主机和端口号, 通常从HTTP URL中提取出来 User-Agent 允许客户端将它的操作系统, 浏览器和其他属性告诉服务器. 不过这个报头域不是必需的, 如果我们自己写一个浏览器, 不使用User-Agent 请求报头域, 那么服务器端就无法得知我们的信息了. 请求报头示例 GET /form.html HTTP/1.1 (CRLF) Accept:image/gif,image/x-xbigmap,image/jpeg,application/x-shockwave-flash,application/vnd.ms-excel,application/vnd.ms-powerpoint,application/msword,*/* (CRLF) Accept-Language:zh-cn (CRLF) Acdept-Encoding:gzip,deflate (CRLF) If-Modified-Since:Wed,05 Jan 2007 11:21:25 GMT (CRLF) If-None-Match:Mozilla/4.0(compatible;MSIE6.0,Windows NT 5.0) (CRLF) Host:www.guet.edu.cn (CRLF) Connection:Keep-Alive (CRLF) (CRLF)响应报头响应报头允许服务器传递不能放在状态行中的附加响应信息, 以及关于服务器的信息和对Request-URI所标识的资源进行下一步访问的信息 Location 重定向接受者到一个新的位置. Location响应报头域常用在更换域名的时候 Server 包含了服务器用来处理请求的软件信息. 与User-Agent 请求报头域是相对应的 例: Server:Apache-Coyote/1.1 WWW-Authenticate WWW-Authenticate 响应报头域必须包含在401响应消息中, 客户端收到401响应消息的时候, 并发送Authorization报头域请求服务器对其进行验证时, 服务端响应报头就包含该报头域 例: WWW-Authenticate:Basic realm=&quot;Basic Auth Test! 实体报头请求和响应消息都可以传送一个实体. 一个实体由实体报头域和实体正文组成, 但并不是实体报头域和实体正文要在一起发送, 可以只发送实体报头域. 实体报头定义了关于实体正文(如: 有无实体正文) 和请求所标识的资源的元信息. Content-Encoding 被用作媒体类型的修饰符, 它的值指示了已经被应用到实体正文的附加内容的编码, 因而要获得Content-Type 报头域中所引用的媒体类型, 必须采用相应的解码机制. Content-Language 描述了资源所用的自然语言. 没有设置该域则认为实体内容将提供给所有的语言阅读 Content-Length 指明实体正文的长度, 以字节方式存储的十进制数字来表示 Content-Type 指明发送给接受者的实体正文的媒体类型 例: Content-Type:text/html;charset=ISO-8859-1 Content-Type:text/html;charset=GB2312 Last-Modified 指示资源的最后修改日期和时间 Expires 给出响应国企的日期和时间. 为了让代理服务器或浏览器在一段时间以后更新缓存中(再次访问曾访问过的页面时, 直接从缓存中加载, 缩短响应时间和降低服务器负载) 的页面, 我们可以使用Expires 实体报头域指定页面过期的时间. 例: Expires:Thu, 15 Sep 2006 16:23:12 GMT HTTP1.1 的客户端和缓存必须将其他非法的日期格式(包括0) 看作已经过期 为了让浏览器不要缓存页面, 我们也可以利用Expires 实体报头域设置为0, jsp中程序如下: response.setDateHeader(&quot;Expires&quot;, &quot;0&quot;);","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"http","slug":"http","permalink":"https://lxb.wiki/tags/http/"}]},{"title":"qt 同步方式发送post 请求","slug":"qt 同步方式发送post 请求","date":"2018-05-22T03:12:38.000Z","updated":"2019-10-03T17:00:11.819Z","comments":true,"path":"c57f4e3b/","link":"","permalink":"https://lxb.wiki/c57f4e3b/","excerpt":"","text":"不成功的方式: 1. QNetworkReply的isFinished()函数, 通过while循环判断reply是否已经结束, 结束后再调用readAll()读取响应信息, 结果与判断isRunning() 方式结果一样, 都会进入死循环, 没有响应. 2. QNetworkReply继承自QIODevice, 尝试调用QIODevice的waitForReadyRead()方法, 结果不阻塞, 直接返回 成功的方式: 使用QEventLoop来阻塞运行, 知道信号发出 QNetworkReply *reply = _manager-&gt;post(QNetworkRequest(QUrl(SERVER_URL)), data); QByteArray responseData; QEventLoop eventLoop; connect(_manager, SIGNAL(finished(QNetworkReply*)), &amp;eventLoop, SLOT(quit())); eventLoop.exec(); //block until finish responseData = reply-&gt;readAll();","categories":[{"name":"Qt","slug":"Qt","permalink":"https://lxb.wiki/categories/Qt/"}],"tags":[]},{"title":"qt 程序打包","slug":"qt 程序打包","date":"2018-05-03T15:03:09.000Z","updated":"2019-10-03T07:57:15.382Z","comments":true,"path":"7e8574d1/","link":"","permalink":"https://lxb.wiki/7e8574d1/","excerpt":"","text":"设置程序图标 把ico文件放到源文件目录下, 命名为”test.ico” 创建一个myico.rc 文件, 输入如下内容 IDI_ICON1 ICON DISCARDABLE &quot;test.ico&quot; 在pro文件写入 RC_FILE = myico.rc 执行qmake, 编译 编译, 打包 选择release编译运行 将生成的exe文件放到某个路径下, 如 Desktop/Test 在cmd里, 进入到exe存放路径, 使用wendeployqt工具拷贝exe运行需要的dll 使用Inno Setup Compiler生成安装文件 Inno Setup 工具使用注意事项 添加主执行文件外的其他应用程序文件夹下的文件时, 需要编辑一次, 重新指定目标子文件夹 编译脚本为*.iss 文件, 编译后默认在源exe的Base 目录下生成Output文件夹, 指定的setup.exe文件生成在Output 文件夹下 Inno Setup 工具基础版不支持中文. 如需显示中文, 需要找汉化版","categories":[{"name":"Qt","slug":"Qt","permalink":"https://lxb.wiki/categories/Qt/"}],"tags":[]},{"title":"Gerrit + apache 安装","slug":"Gerrit + apache 安装","date":"2018-05-02T15:56:55.000Z","updated":"2020-05-05T11:32:47.183Z","comments":true,"path":"2eb2e06d/","link":"","permalink":"https://lxb.wiki/2eb2e06d/","excerpt":"","text":"使用gerrit自带的数据库h2, 验证方式为HTTP, SMTP 服务器未配置 git 安装可直接从yum 源安装 gerrit 安装先添加gerrit 用户. gerrit 从2.10开始, 换成了新版界面. 几乎国内所有的镜像都会下载失败, 需要翻墙下载. 下载完成后, 初始化命令为: java -jar gerrrit-war init -d /home/gerrit/repository 初始化启动时, “Authentication method” 设为”http” ,其他默认 “Listen on port [8080]“ 可用默认, 如端口被占用, 初始化后也可在配置文件修改 apache 安装直接从yum源安装 apache名字为httpd, 服务名也是httpd. 服务启动后, 默认以apache用户运行. 如需访问其他用户的文件, 如/home/gerrit/repository/htpasswd, 需要确保apache 用户有足够的权限 配置 apache修改 apache 的conf 文件, 一般路径为/etc/httpd/conf/httpd.conf windows 下的配置文件路径为 INSTALL_DIR/conf/httpd.conf 去掉下面几行的注释 LoadModule proxy_module modules/mod_proxy.so LoadModule proxy_connect_module modules/mod_proxy_connect.so LoadModule proxy_http_module modules/mod_proxy_http.so LoadModule proxy_ftp_module modules/mod_proxy_ftp.so LoadModule negotiation_module modules/mod_negotiation.so在最后追加下面配置 &lt;VirtualHost *:8080&gt; ServerName v3server ProxyRequests Off ProxyVia Off ProxyPreserveHost On &lt;Proxy *:8080&gt; Order deny,allow Allow from all &lt;/Proxy&gt; &lt;Location /login/&gt; AuthType Basic AuthName &quot;Gerrit Code Review&quot; Require valid-user AuthUserFile D:/git/htpasswd &lt;/Location&gt; ProxyPass / http://10.14.132.179:9080/ ProxyPassReverse / http://10.14.132.179:9080/ &lt;/VirtualHost&gt;如端口被占用, 修改conf文件的”Listen 8080” 字段, 换成其他的端口 查看某个端口是否被占用 : netstat -lnp | grep 8080 ProxyPass 和 proxyPassReverse 的端口需与gerrit的conf文件里端口一致 配置 gerrit修改GERRIT_DIR/etc/gerrit.config 文件 [gerrit] basePath = git canonicalWebUrl = http://10.14.132.179:9080/ [database] type = H2 database = db/ReviewDB [auth] type = HTTP logoutUrl = http://aa:aa@10.14.132.179:8080/ [sendemail] smtpServer = smtp.163.com smtpUser = useremail@163.com smtpPass = userpass from = useremail@163.com [container] user = admin javaHome = C:\\\\Program Files\\\\Java\\\\jdk1.6.0_27\\\\jre [sshd] listenAddress = *:29418 [httpd] listenUrl = http://10.14.132.179:9080/ [cache] directory = cache需要修改的内容: - canonicalWebUrl - auth/type 需要注意: canonicalWebUrl 和 listenAddress 不是8080. apache 的端口和 gerrit 的端口是不同的, 用户访问地址为 apache 的地址 启动 gerritGERRIT_DIR/bin/gerrit.sh start 启动 apacheservice httpd start 添加账号和密码htpasswd -cm /home/gerrit/repository/htpasswd USER_NAME htpasswd 为apache 的命令工具 参数c 意为新建文件, 即 /home/gerrit/repository/htpasswd 文件不存在时, 新建名为htpasswd的文件 参数m 为使用md5 加密 当htpasswd文件存在时, 可以使用htpasswd -m /PATH_TO_HTPASSWD USER_NAME 添加账号 保存账号密码信息的文件(htpasswd), 名字为自定义的, 但需要与apache 的conf 配置文件里 AuthUserFile 一致","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://lxb.wiki/tags/工具/"}]},{"title":"Qt 工程的几种文件","slug":"Qt 工程的几种文件","date":"2018-04-20T14:25:51.000Z","updated":"2019-10-03T16:59:43.570Z","comments":true,"path":"924904f6/","link":"","permalink":"https://lxb.wiki/924904f6/","excerpt":"","text":"*.proqmake的工程(project)文件 例子: TEMPLATE = app CONFIG += QT QT += core gui TARGET = somename SOURCES += main.cpp \\ widget.cpp HEADERS += widget.h FORMS += widget.ui 前三行是qmake的默认值, 都可以省略 TARGET 行指定工程名, 也可以省略 *.priinclude 文件 接上面的例子, 我们可以将源文件的设置独立处理, 放到somename.pri文件内: SOURCES += main.cpp \\ widget.cpp HEADERS += widget.h FORMS += widget.ui这时, pro 文件就可以简化为: TEMPLATE = app CONFIG += QT QT += core gui TARGET = somename include(somename.pri)*.prf特性(feature) 文件 和pri文件类似, prf文件也是要被包含进pro文件. 只是它更隐蔽. 在上面的例子中, 其实已经用到了prf, 就是 CONFIG += QT 当在CONFIG 中指定一个值时, qmake就会尝试去加载相应的feature文件: - Qt安装目录下的mkspecs/features/qt.prf - features 文件的文件名必须小写 例子: win32:CONFIG += console // 为win32程序添加控制台把该文件命名为a.prf, 放到前面提到的目录中, 然后在pro文件内添加 CONFIG += a 也可以使用load命令来加载prf文件 load(a)","categories":[{"name":"Qt","slug":"Qt","permalink":"https://lxb.wiki/categories/Qt/"}],"tags":[]},{"title":"Qt UI 编码规范","slug":"Qt UI 编码规范","date":"2018-04-02T03:27:52.000Z","updated":"2019-10-03T07:57:15.438Z","comments":true,"path":"9c64b54d/","link":"","permalink":"https://lxb.wiki/9c64b54d/","excerpt":"","text":"变量声明 每行只声明一个变量 避免使用短的/无意义的命名 当一个变量被用到时再声明 // Wrong int a, b; char* c, * d; // Correct int height; int width; char* nameOfOne; char* nameOfOther; 变量命名 变量名/函数名采用驼峰命名法(lowerCaseCamel), 首字母缩写词出现的命名中, 缩写也用驼峰命名 // Wrong short Cntr; char ITEM_DELIM = &apos;&apos;; void myXMLStreamReader(); // Correct short counter; char itemDelimiter = &apos;&apos;; void myXmlStreamReader(); 空行/空格 用一个且仅用一个空行在适当的地方划分代码块 在关键词和小括号之间总是只用一个空格符 // Wrong if(foo) { } // Correct if (foo) { } 指针/引用 在类型名和*或&amp;之间没有空格, 在*或&amp;与变量名之间有一个空格 char* someValue; const QString&amp; myString; const char* const WOR = &quot;hello&quot;; 符号与空格 二元操作符左右两边都有一个空格 一元操作符与变量之间不留空格 逗号左右没有空格, 右边一个空格 分号左边没有空格; 分号作为语句的结束符, 右边一般不再有内容 #号右边没有空格 左引号的左边和右引号的各一个空格, 左引号的右边和右引号的左边没有空格 如果右引号右边是右括号, 它们之间没有空格 cast 避免C语言的cast, 尽量用C++的cast(static_cast, const_cast, reinterpret_cast). reinterpret_cast 和 C风格的cast用起来都是危险的，但至少 reinterpret_cast 不会把const修饰符去掉 涉及到QObjects或重构自己的代码时，不要使用dynamic_cast,而是用qobject_cast，例如在引进一个类型的方法时 用构造函数去cast简单类型,例如：用int(myFloat)代替(int)myFloat // Wrong char* blockOfMemory = (char* ) malloc(data.size()); // Correct char *blockOfMemory = reinterpret_cast&lt;char *&gt;(malloc(data.size())); 语句 不要在一行写多条语句 括号 每个大括号单独一行 不论条件语句的执行部分有几行, 必须使用大括号 小括号用来给语句分组 // Wrong if (address.isEmpty()) { return false; } for (int i = 0; i &lt; 10; +’’i) { qDebug(&quot;%i&quot;, i); } // Correct if (address.isEmpty()) { return false; } else { return true; } for (int i = 0; i &lt; 10;i) { qDebug(&quot;%i&quot;, i); } // Wrong if (a &amp;&amp; b || c) // Correct if ((a &amp;&amp; b) || c) // Wrong a + b &amp; c // Correct (a + b) &amp; cswitch语句 case缩进 除enum外, 每组case最后都要加default; switch (myEnum) { case Value1: doSomething(); break; case Value2: case Value3: doSomethingElse(); // fall through break; default: defaultHandling(); break; } goto 禁止使用goto 换行 每行代码不多于120字符 逗号在行尾. 操作符在新行的开头位置 换行时尽量避免行与行之间看起来参差不齐 // Wrong if (longExpression + otherLongExpression + otherOtherLongExpression) { } // Correct if (longExpression + otherLongExpression + otherOtherLongExpression) { } C++特性 不要使用异常处理 不要使用运行时类型识别 理智地使用模板 Qt源码中的规范 所有代码都是ascii，使用者如果不确定的话，只可能是7字节 每一个QObject的子类都必须有Q_OBJECT宏，即使这个类没用到信号或槽。否则qobject_cast将不能使用 在connect语句中，使信号和槽的参数规范化（参看 QMetaObject::normalizedSignature），可以加快信号/槽的查找速度。可以使用qtrepotools/util/normalize规范已有代码 包含头文件顺序 源文件对应的头文件 &lt;分隔&gt; C系统文件 &lt;分隔&gt; C++系统文件 &lt;分隔&gt; Qt库文件 &lt;分隔&gt; 其他目录 每组文件按字母升序排列 编译器/平台 使用三目运算符 ？时要特别小心，如果每次的返回值的类型可能不一样的话，一些编译器会在运行时生成冲突的代码（此时编译器甚至不会报错） QString s; return condition ? s : &quot;nothing&quot;; // crash at runtime - QString vs. const char * 要特别小心对齐问题。无论何时，当一个指针被cast后的对齐数是增加的时候，它都可能会崩溃。例如一个const char 被cast成了cons int，当cast之后的数字不得不在2或4个字节之间对齐时，指针就会在机器上崩溃 任何需要需要执行构造函数或相关代码进行初始化的实例，都不能用作库代码中的全局实例。因为当构造函数或代码将要运行的时候，该实例还没有被定义（在第一次调用该实例时，在加载库时，在执行main()之前） // global scope static const QString x; // Wrong - default constructor needs to be run to initialize x static const QString y = &quot;Hello&quot;; // Wrong - constructor that takes a const char * has to be run QString z; // super wrong static const int i = foo(); // wrong - call time of foo() undefined, might not be called at all 可以使用下面方法: // global scope static const char x[] = &quot;someText&quot;; // Works - no constructor must be run, x set at compile time static int y = 7; // Works - y will be set at compile time static MyStruct s = {1, 2, 3}; // Works - will be initialized statically, no code being run static QString *ptr = 0; // Pointers to objects are ok - no code needed to be run to initialize ptr 用Q_GLOBAL_STATIC定义全局实例 Q_GLOBAL_STATIC(QString, s) void foo() { s()-&gt;append(&quot;moo&quot;); } char型变量是有符号的还是无符号的取决于它运行环境的架构。如果你明确地想使用一个signed或unsinged char，就使用signed char或unsigned char。以下代码运行在把char默认为无符号的平台上时，其条件判断恒为真 char c; // c can&#39;t be negative if it is unsigned /********/ /*******/ if (c &gt; 0) { … } // WRONG - condition is always true on platforms where the default is unsigned 避免64位的枚举值 嵌入式应用系统二进制接口将所有的枚举类型的值硬编码成32位int值 微软的编译器不支持64位的枚举值 编程偏好 用枚举值定义常量而非用const int或defines 枚举值会在编译时被编译器用实际值替换掉，因而运行时得出结果的速度更快 defines不是命名空间安全的（并且看起来很丑） 当重新实现一个虚方法时，在Qt5中，用 Q_DECL_OVERRIDE宏在函数声明之后，分号之前注解它 不要把const-iterator和none-const iterator搞混 for (Container::const_iterator it = c.begin(); it != c.end(); ++it) // Wrong for (Container::const_iterator it = c.cbegin(); it != c.cend(); ++it) // Right 命名空间 除跟UI直接交互的类外, 其他类必须处在命名空间内 float值 用qFuzzyCompare去和delta比较其值 用qIsNull去判断float值是不是二进制0，而不是和0.0比较 [static] bool qFuzzyCompare(double p1, double p2) // Instead of comparing with 0.0 qFuzzyCompare(0.0,1.0e-200); // This will return false // Compare adding 1 to both values will fix the problem qFuzzyCompare(1 + 0.0, 1 + 1.0e-200); // This will return true 类的成员命名 成员变量一般为名词 函数成员一般为动词/动词+名词，但是当动词为get时，get常常省略。当返回值为Bool型变量时，函数名一般以前缀’is’开头 public: void setColor(const QColor&amp; c); QColor color() const; void setDirty(bool b); bool isDirty() const; private Q_SLOTS: void onParentChanged(); 构造函数 为了使构造函数被错误使用的可能性降到最小，每一个构造函数（除了拷贝构函数）都应该检查自己是否需要加上explicit 符号 注意代码陷阱 不要为了图方便少些一些代码。因为代码是一次书写，后期不止一次地要去理解。例如 QSlider *slider = new QSlider(12, 18, 3, 13, Qt::Vertical, 0, &quot;volume&quot;); 改成下面的方式会更容易理解 QSlider *slider = new QSlider(Qt::Vertical); slider-&gt;setRange(12, 18); slider-&gt;setPageStep(3); slider-&gt;setValue(13); slider-&gt;setObjectName(&quot;volume&quot;); 参考资料 https://wiki.qt.io/Qt\\_Contribution\\_Guidelines https://wiki.qt.io/Qt\\_Coding\\_Style https://wiki.qt.io/Coding_Conventions https://community.kde.org/Policies/Library\\_Code\\_Policy https://wiki.qt.io/UI\\_Text\\_Conventions https://wiki.qt.io/API\\_Design\\_Principles http://doc.qt.io/qt-5/qml-codingconventions.html https://google.github.io/styleguide/cppguide.html","categories":[{"name":"Qt","slug":"Qt","permalink":"https://lxb.wiki/categories/Qt/"}],"tags":[]},{"title":"CentOS7升级gcc 和gdb","slug":"CentOS7升级gcc和gdb","date":"2018-03-28T03:37:20.000Z","updated":"2019-10-03T16:40:38.981Z","comments":true,"path":"ec9feff6/","link":"","permalink":"https://lxb.wiki/ec9feff6/","excerpt":"","text":"升级后版本: gcc-5.4.0 gdb-7.11.1 安装开发必备环境yum groupinstall &quot;Development Tools&quot; yum install glibc-static libstdc++-static编译安装gcc-5.4.0gcc下载地址 tar -xvf gcc-5.4.0.tar.bz2 cd gcc-5.4.0 ./contrib/download_prerequisits mkdir build cd build ../configure --enable-checking=release --enable-languages=c,c++ --disable-multilib make（建议不要使用make -j来编译，虽然可以缩短编译时间，但极大可能会编译失败） make install其中执行./contrib/download_prerequisits将自动下载以下几个文件，这个几个文件在gcc编译时需要： - mpfr-2.4.2.tar.bz2 - gmp-4.3.2.tar.bz2 - mpc-0.8.1.tar.gz - isl-0.15.tar.bz2 make install 时, 自动安装到/usr/local/gcc-5.40 解决运行程序时, gcc 报错’GLIBCXX_3.4.21’ not found这是因为升级gcc时，生成的动态库没有替换老版本gcc的动态库导致的，将gcc最新版本的动态库替换系统中老版本的动态库即可解决，运行以下命令检查动态库： strings /lib64/libstdc++.so.6 | grep GLIBC 以下是输出结果： GLIBCXX_3.4 GLIBCXX_3.4.1 GLIBCXX_3.4.2 GLIBCXX_3.4.3 GLIBCXX_3.4.4 GLIBCXX_3.4.5 GLIBCXX_3.4.6 GLIBCXX_3.4.7 GLIBCXX_3.4.8 GLIBCXX_3.4.9 GLIBCXX_3.4.10 GLIBCXX_3.4.11 GLIBCXX_3.4.12 GLIBCXX_3.4.13 GLIBCXX_3.4.14 GLIBCXX_3.4.15 GLIBCXX_3.4.16 GLIBCXX_3.4.17 GLIBCXX_3.4.18 GLIBCXX_3.4.19 GLIBC_2.3 GLIBC_2.2.5 GLIBC_2.14 GLIBC_2.4 GLIBC_2.3.2 GLIBCXX_DEBUG_MESSAGE_LENGTH从输出结果可以看到并没有“GLIBCXX_3.4.21“,所以可以断定我们的程序运行时动态加载的是老的动态库，解决这个问题需要将当前链接文件的链接指向改成最新的动态库地址： cp /usr/local/lib64/libstdc++.so.6.0.21 /lib64 cd /lib64 rm -rf libstdc++.so.6 ln -s libstdc++.so.6.0.21 libstdc++.so.6然后你可以执行以下命令来查看’GLIBCXX_3.4.21’已经可以找到了: strings /lib64/libstdc++.so.6 | grep GLIBC 解决了这个问题终于可以执行程序了，然后又测试了-g选项来编译程序，编译好程序调试程序时并不能够设置断点以及print变量的值，gdb调试中出现：Missing separate debuginfos, use: debuginfo-install glibc-2.17-106.e17_2.6.x86_4 libgcc-4.8.5-4.e17.x86_64的问题，通过上网查阅资料，是因为gcc版本和gdb版本并不匹配，或者说gdb版本过低 编译安装gdb-7.11.1gdb下载地址 tar -xvf gdb-7.11.1.tar.gz cd gdb-7.11.1 ./configure make make install当执行make install时gdb安装出现了错误：WARNING: &#39;makeinfo&#39; is missing on your sysem，则需安装相关依赖程序: yum install texinfo libncurses5-dev 如果调试程序时出现下面信息时： warning: File &quot;/usr/local/lib64/libstdc++.so.6.0.21-gdb.py&quot; auto-loading has been declined by your `auto-load safe-path&apos; set to &quot;$debugdir:$datadir/auto-load&quot;. To enable execution of this file add add-auto-load-safe-path /usr/local/lib64/libstdc++.so.6.0.21-gdb.py line to your configuration file &quot;/root/.gdbinit&quot;. To completely disable this security protection add set auto-load safe-path / line to your configuration file &quot;/root/.gdbinit&quot;.解决方法: 将以下信息放入~/.gdbinit add-auto-load-safe-path /usr/local/lib64/libstdc++.so.6.0.21-gdb.py set auto-load safe-path /若想通过gdb来调试STL容器，则还需要做一些配置，可以通过GDB Python pretty printers来解决这个问题： svn checkout svn://gcc.gnu.org/svn/gcc/trunk/libstdc++-v3/python stlPrettyPrinter mv stlPrettyPrinter /usr/local然后将下面的配置信息放入~/.gdbinit python import sys sys.path.insert(0, &apos;/usr/local/stlPrettyPrinter&apos;) from libstdcxx.v6.printers import register_libstdcxx_printers register_libstdcxx_printers (None) end","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[]},{"title":"ffmpeg 视音频同步","slug":"ffmpeg 视音频同步","date":"2018-03-27T06:41:52.000Z","updated":"2019-10-03T07:57:15.441Z","comments":true,"path":"aeb01c06/","link":"","permalink":"https://lxb.wiki/aeb01c06/","excerpt":"","text":"原文地址: https://blog.csdn.net/nonmarking/article/details/50522413 对于直播流来说, 只考虑发送端的同步问题, 原理如下: 1. 解析视音频, 讲视频流和音频流的时间戳用同样的时间基准表示 2. 比较转换后的两个时间戳, 找出较小值, 对应发送偏慢的流 3. 读取, 转码, 发送相应的流, 同时, 若该流的转码时间很快, 超前于wall clock, 则还需要进行相应的延时 4. 重复以上过程 下文包括两部分, 一是音频转码部分, 二是视音频同步 音频转码基本流程首先是一些音频输入输出的基本设置 //Set own audio device&apos;s name if (avformat_open_input(&amp;ifmt_ctx_a, device_name_a, ifmt, &amp;device_param) != 0){ printf(&quot;Couldn&apos;t open input audio stream.（无法打开输入流）\\n&quot;); return -1; } …… //input audio initialize if (avformat_find_stream_info(ifmt_ctx_a, NULL) &lt; 0) { printf(&quot;Couldn&apos;t find audio stream information.（无法获取流信息）\\n&quot;); return -1; } audioindex = -1; for (i = 0; i &lt; ifmt_ctx_a-&gt;nb_streams; i++) if (ifmt_ctx_a-&gt;streams[i]-&gt;codec-&gt;codec_type == AVMEDIA_TYPE_AUDIO) { audioindex = i; break; } if (audioindex == -1) { printf(&quot;Couldn&apos;t find a audio stream.（没有找到视频流）\\n&quot;); return -1; } if (avcodec_open2(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec, avcodec_find_decoder(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;codec_id), NULL) &lt; 0) { printf(&quot;Could not open audio codec.（无法打开解码器）\\n&quot;); return -1; } …… //output audio encoder initialize pCodec_a = avcodec_find_encoder(AV_CODEC_ID_AAC); if (!pCodec_a){ printf(&quot;Can not find output audio encoder! (没有找到合适的编码器！)\\n&quot;); return -1; } pCodecCtx_a = avcodec_alloc_context3(pCodec_a); pCodecCtx_a-&gt;channels = 2; pCodecCtx_a-&gt;channel_layout = av_get_default_channel_layout(2); pCodecCtx_a-&gt;sample_rate = ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_rate; pCodecCtx_a-&gt;sample_fmt = pCodec_a-&gt;sample_fmts[0]; pCodecCtx_a-&gt;bit_rate = 32000; pCodecCtx_a-&gt;time_base.num = 1; pCodecCtx_a-&gt;time_base.den = pCodecCtx_a-&gt;sample_rate; /** Allow the use of the experimental AAC encoder */ pCodecCtx_a-&gt;strict_std_compliance = FF_COMPLIANCE_EXPERIMENTAL; /* Some formats want stream headers to be separate. */ if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER) pCodecCtx_a-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER; if (avcodec_open2(pCodecCtx_a, pCodec_a, NULL) &lt; 0){ printf(&quot;Failed to open ouput audio encoder! (编码器打开失败！)\\n&quot;); return -1; } //Add a new stream to output,should be called by the user before avformat_write_header() for muxing audio_st = avformat_new_stream(ofmt_ctx, pCodec_a); if (audio_st == NULL){ return -1; } audio_st-&gt;time_base.num = 1; audio_st-&gt;time_base.den = pCodecCtx_a-&gt;sample_rate; audio_st-&gt;codec = pCodecCtx_a;接下来, 考虑到输入音频的sample format 可能需要进行转换, 需要用到swresample库的功能 先做好相应的初始化 // Initialize the resampler to be able to convert audio sample formats aud_convert_ctx = swr_alloc_set_opts(NULL, av_get_default_channel_layout(pCodecCtx_a-&gt;channels), pCodecCtx_a-&gt;sample_fmt, pCodecCtx_a-&gt;sample_rate, av_get_default_channel_layout(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;channels), ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_fmt, ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_rate, 0, NULL); swr_init(aud_convert_ctx);此外, 参照transcode_aac.c的做法, 使用FIFO buffer存储从输入端解码得到的音频采样数据, 这些数据在后续将转换sample format并进行编码, 由此即完成了一个音频转码功. 此外, 还需要另外的一个buffer来存储转换合适之后的音频数据 //Initialize the FIFO buffer to store audio samples to be encoded. AVAudioFifo *fifo = NULL; fifo = av_audio_fifo_alloc(pCodecCtx_a-&gt;sample_fmt, pCodecCtx_a-&gt;channels, 1); //Initialize the buffer to store converted samples to be encoded. uint8_t **converted_input_samples = NULL; /** * Allocate as many pointers as there are audio channels. * Each pointer will later point to the audio samples of the corresponding * channels (although it may be NULL for interleaved formats). */ if (!(converted_input_samples = (uint8_t**)calloc(pCodecCtx_a-&gt;channels, sizeof(**converted_input_samples)))) { printf(&quot;Could not allocate converted input sample pointers\\n&quot;); return AVERROR(ENOMEM); }至此, 一些基本的初始化工作完成. 音频计算pts的方法和视频类似. 即先通过sample rate算出每两个音频sample之间的时间间隔, 再通过计数当前已编码的音频sample总数(nb_samples变量的作用) 来算出当前编码音频帧的时间戳. 如果和视频的流程做类比, 大概为: framerate 相当于sample rate, framecnt相当于nb_samples. //audio trancoding here const int output_frame_size = pCodecCtx_a-&gt;frame_size; /** * Make sure that there is one frame worth of samples in the FIFO * buffer so that the encoder can do its work. * Since the decoder&apos;s and the encoder&apos;s frame size may differ, we * need to FIFO buffer to store as many frames worth of input samples * that they make up at least one frame worth of output samples. */ while (av_audio_fifo_size(fifo) &lt; output_frame_size) { /** * Decode one frame worth of audio samples, convert it to the * output sample format and put it into the FIFO buffer. */ AVFrame *input_frame = av_frame_alloc(); if (!input_frame) { ret = AVERROR(ENOMEM); return ret; } /** Decode one frame worth of audio samples. */ /** Packet used for temporary storage. */ AVPacket input_packet; av_init_packet(&amp;input_packet); input_packet.data = NULL; input_packet.size = 0; /** Read one audio frame from the input file into a temporary packet. */ if ((ret = av_read_frame(ifmt_ctx_a, &amp;input_packet)) &lt; 0) { /** If we are at the end of the file, flush the decoder below. */ if (ret == AVERROR_EOF) { encode_audio = 0; } else { printf(&quot;Could not read audio frame\\n&quot;); return ret; } } /** * Decode the audio frame stored in the temporary packet. * The input audio stream decoder is used to do this. * If we are at the end of the file, pass an empty packet to the decoder * to flush it. */ if ((ret = avcodec_decode_audio4(ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec, input_frame, &amp;dec_got_frame_a, &amp;input_packet)) &lt; 0) { printf(&quot;Could not decode audio frame\\n&quot;); return ret; } av_packet_unref(&amp;input_packet); /** If there is decoded data, convert and store it */ if (dec_got_frame_a) { /** * Allocate memory for the samples of all channels in one consecutive * block for convenience. */ if ((ret = av_samples_alloc(converted_input_samples, NULL, pCodecCtx_a-&gt;channels, input_frame-&gt;nb_samples, pCodecCtx_a-&gt;sample_fmt, 0)) &lt; 0) { printf(&quot;Could not allocate converted input samples\\n&quot;); av_freep(&amp;(*converted_input_samples)[0]); free(*converted_input_samples); return ret; } /** * Convert the input samples to the desired output sample format. * This requires a temporary storage provided by converted_input_samples. */ /** Convert the samples using the resampler. */ if ((ret = swr_convert(aud_convert_ctx, converted_input_samples, input_frame-&gt;nb_samples, (const uint8_t**)input_frame-&gt;extended_data, input_frame-&gt;nb_samples)) &lt; 0) { printf(&quot;Could not convert input samples\\n&quot;); return ret; } /** Add the converted input samples to the FIFO buffer for later processing. */ /** * Make the FIFO as large as it needs to be to hold both, * the old and the new samples. */ if ((ret = av_audio_fifo_realloc(fifo, av_audio_fifo_size(fifo) + input_frame-&gt;nb_samples)) &lt; 0) { printf(&quot;Could not reallocate FIFO\\n&quot;); return ret; } /** Store the new samples in the FIFO buffer. */ if (av_audio_fifo_write(fifo, (void **)converted_input_samples, input_frame-&gt;nb_samples) &lt; input_frame-&gt;nb_samples) { printf(&quot;Could not write data to FIFO\\n&quot;); return AVERROR_EXIT; } } } /** * If we have enough samples for the encoder, we encode them. * At the end of the file, we pass the remaining samples to * the encoder. */ if (av_audio_fifo_size(fifo) &gt;= output_frame_size) /** * Take one frame worth of audio samples from the FIFO buffer, * encode it and write it to the output file. */ { /** Temporary storage of the output samples of the frame written to the file. */ AVFrame *output_frame=av_frame_alloc(); if (!output_frame) { ret = AVERROR(ENOMEM); return ret; } /** * Use the maximum number of possible samples per frame. * If there is less than the maximum possible frame size in the FIFO * buffer use this number. Otherwise, use the maximum possible frame size */ const int frame_size = FFMIN(av_audio_fifo_size(fifo), pCodecCtx_a-&gt;frame_size); /** Initialize temporary storage for one output frame. */ /** * Set the frame&apos;s parameters, especially its size and format. * av_frame_get_buffer needs this to allocate memory for the * audio samples of the frame. * Default channel layouts based on the number of channels * are assumed for simplicity. */ output_frame-&gt;nb_samples = frame_size; output_frame-&gt;channel_layout = pCodecCtx_a-&gt;channel_layout; output_frame-&gt;format = pCodecCtx_a-&gt;sample_fmt; output_frame-&gt;sample_rate = pCodecCtx_a-&gt;sample_rate; /** * Allocate the samples of the created frame. This call will make * sure that the audio frame can hold as many samples as specified. */ if ((ret = av_frame_get_buffer(output_frame, 0)) &lt; 0) { printf(&quot;Could not allocate output frame samples\\n&quot;); av_frame_free(&amp;output_frame); return ret; } /** * Read as many samples from the FIFO buffer as required to fill the frame. * The samples are stored in the frame temporarily. */ if (av_audio_fifo_read(fifo, (void **)output_frame-&gt;data, frame_size) &lt; frame_size) { printf(&quot;Could not read data from FIFO\\n&quot;); return AVERROR_EXIT; } /** Encode one frame worth of audio samples. */ /** Packet used for temporary storage. */ AVPacket output_packet; av_init_packet(&amp;output_packet); output_packet.data = NULL; output_packet.size = 0; /** Set a timestamp based on the sample rate for the container. */ if (output_frame) { nb_samples += output_frame-&gt;nb_samples; } /** * Encode the audio frame and store it in the temporary packet. * The output audio stream encoder is used to do this. */ if ((ret = avcodec_encode_audio2(pCodecCtx_a, &amp;output_packet, output_frame, &amp;enc_got_frame_a)) &lt; 0) { printf(&quot;Could not encode frame\\n&quot;); av_packet_unref(&amp;output_packet); return ret; } /** Write one audio frame from the temporary packet to the output file. */ if (enc_got_frame_a) { output_packet.stream_index = 1; AVRational time_base = ofmt_ctx-&gt;streams[1]-&gt;time_base; AVRational r_framerate1 = { ifmt_ctx_a-&gt;streams[audioindex]-&gt;codec-&gt;sample_rate, 1 };// { 44100, 1}; int64_t calc_duration = (double)(AV_TIME_BASE)*(1 / av_q2d(r_framerate1)); //内部时间戳 output_packet.pts = av_rescale_q(nb_samples*calc_duration, time_base_q, time_base); output_packet.dts = output_packet.pts; output_packet.duration = output_frame-&gt;nb_samples; //printf(&quot;audio pts : %d\\n&quot;, output_packet.pts); aud_next_pts = nb_samples*calc_duration; int64_t pts_time = av_rescale_q(output_packet.pts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if ((pts_time &gt; now_time) &amp;&amp; ((aud_next_pts + pts_time - now_time)&lt;vid_next_pts)) av_usleep(pts_time - now_time); if ((ret = av_interleaved_write_frame(ofmt_ctx, &amp;output_packet)) &lt; 0) { printf(&quot;Could not write frame\\n&quot;); av_packet_unref(&amp;output_packet); return ret; } av_packet_unref(&amp;output_packet); } av_frame_free(&amp;output_frame); } 视音频同步首先定义几个变量 int aud_next_pts = 0;//视频流目前的pts,可以理解为目前的进度 int vid_next_pts = 0;//音频流目前的pts int encode_video = 1, encode_audio = 1;//是否要编码视频、音频则相应的视音频同步方法如下: 1. 确定视频, 音频二者中至少有一个是需要进行转码的 2. 比较两个流的进度, 使用av_compare_ts函数, 注意：此时的vid_next_pts和aud_next_pts的time base都是ffmpeg内部基准，即AVRational time_base_q = { 1, AV_TIME_BASE }; 3. 对进度落后的流进行转码, 并相应地对进度进行更新. 对于视频，有 vid_next_pts=framecnt_calc_duration;，对于音频，有 aud_next_pts = nb_samples_calc_duration;这里framecnt和nb_samples都相当于计数器，而calc_duration是对应流每两个frame或sample之间的时间间隔，也是以ffmpeg内部时间基准为单位的 4. 若转码进度很快完成, 则不能急于写入输出流, 而是需要先进行延时, 但是也要保证延时后的时间不会超过另一个流的进度 综上, 流程如下: //start decode and encode int64_t start_time = av_gettime(); while (encode_video || encode_audio) { if (encode_video &amp;&amp; (!encode_audio || av_compare_ts(vid_next_pts, time_base_q, aud_next_pts, time_base_q) &lt;= 0)) { 进行视频转码； 转码完成后； vid_next_pts=framecnt*calc_duration; //general timebase //Delay int64_t pts_time = av_rescale_q(enc_pkt.pts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if ((pts_time &gt; now_time) &amp;&amp; ((vid_next_pts + pts_time - now_time)&lt;aud_next_pts)) av_usleep(pts_time - now_time); 写入流； } else { 进行音频转码； 转码完成后； aud_next_pts = nb_samples*calc_duration; int64_t pts_time = av_rescale_q(output_packet.pts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if ((pts_time &gt; now_time) &amp;&amp; ((aud_next_pts + pts_time - now_time)&lt;vid_next_pts)) av_usleep(pts_time - now_time); 写入流； }至此, 视音频同步完成. 最后再完成一些flush_encoder的工作即可.","categories":[{"name":"A&amp;V","slug":"A-amp-V","permalink":"https://lxb.wiki/categories/A-amp-V/"}],"tags":[]},{"title":"ffmpeg 推流报错","slug":"ffmpeg 推流报错","date":"2018-03-27T06:39:47.000Z","updated":"2019-10-03T07:57:15.307Z","comments":true,"path":"eaddfbfe/","link":"","permalink":"https://lxb.wiki/eaddfbfe/","excerpt":"","text":"在使用dshow设备推流时，经常会报出real time buffer too full dropping frames的错误信息，其原因在这篇文章里有写到，可以通过添加rtbufsize参数来解决，码率越高对应的rtbufsize就需要越高，但过高的rtbufsize会带来视频的延时，若要保持同步，可能就需要对音频人为增加一定的延时。而根据我的测试，即使不添加rtbufszie参数，虽然会报出错误信息，但并不影响直播流的观看或录制，而且可以保持同步。这就是一个trade off的问题了。","categories":[{"name":"A&amp;V","slug":"A-amp-V","permalink":"https://lxb.wiki/categories/A-amp-V/"}],"tags":[]},{"title":"protocol buffer","slug":"protocol-buffer","date":"2018-03-21T09:47:31.000Z","updated":"2019-10-03T16:59:06.529Z","comments":true,"path":"66065582/","link":"","permalink":"https://lxb.wiki/66065582/","excerpt":"","text":"Developer Guide.proto 文件 message Person { required string name = 1; required int32 id = 2; optional string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { required string number = 1; optional PhoneType type = 2 [default = HOME]; } repeated PhoneNumber phone = 4; }Once you’ve defined your messages, you run the protocol buffer compiler for your application’s language on your .proto file to generate data access classes. These provide simple accessors for each field (like name() and set_name()) as well as methods to serialize/parse the whole structure to/from raw bytes You can add new fields to your message formats without breaking backwards-compatibility; old binaries simply ignore the new field when parsing. So if you have a communications protocol that uses protocol buffers as its data format, you can extend your protocol without having to worry about breaking existing code. Language GuideDefining A Message Typesyntax = &quot;proto3&quot;; // First non-empty; first non-comment line message SearchRequest { string query = 1; // unique numbered tag int32 page_number = 2; int32 result_per_page = 3; }Specifying Field TypesAssigning Tags1-15 one byte 16-2047 two bytes you should reserve the tags 1 through 15 for very frequently occurring message elements. Remember to leave some room for frequently occurring elements that might be added in the future. range: 1 to 536,870,911 You also cannot use the numbers 19000 through 19999 (FieldDescriptor::kFirstReservedNumber through FieldDescriptor::kLastReservedNumber) Specifying Field Rules singular zero or one of this field repeated any number of times Adding More Message TypesReserved Fieldsmessage Foo { reserved 2, 15, 9 to 11; reserved &quot;foo&quot;, &quot;bar&quot;; }Note that you can’t mix field names and tag numbers in the same reserved statement. What’s Generated From Your .proto?Default Valuessigular: - string - byte - bool - numeric type - enum - message field repeated: - repeated filed Enumerationsmessage SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } Corpus corpus = 4; }You can define aliases by assigning the same value to different enum constants enum EnumAllowingAlias { option allow_alias = true; UNKNOWN = 0; STARTED = 1; RUNNING = 1; } enum EnumNotAllowingAlias { UNKNOWN = 0; STARTED = 1; // RUNNING = 1; // Uncommenting this line will cause a compile error inside Google and a warning message outside. }Reserved Valuesenum Foo { reserved 2, 15, 9 to 11, 40 to max; reserved &quot;FOO&quot;, &quot;BAR&quot;; }Note that you can’t mix field names and numeric values in the same reserved statement. Using Other Message TypesDefine a message in the same .proto file. message SearchResponse { repeated Result results = 1; } message Result { string url = 1; string title = 2; repeated string snippets = 3; }Importing DefinitionsBy default you can only use definitions from directly imported .proto files. import &quot;myproject/other_protos.proto&quot;; // new.proto // All definitions are moved here ====================================================== // old.proto // This is the proto that all clients are importing. import public &quot;new.proto&quot;; import &quot;other.proto&quot;; ====================================================== // client.proto import &quot;old.proto&quot;; // You use definitions from old.proto and new.proto, but not other.protoThe protocol compiler searches for imported files in a set of directories specified on the protocol compiler command line using the -I/–proto_path flag. If no flag was given, it looks in the directory in which the compiler was invoked. In general you should set the –proto_path flag to the root of your project and use fully qualified names for all imports. Using proto2 Message TypesIt’s possible to import proto2 message types and use them in your proto3 messages, and vice versa. However, proto2 enums cannot be used directly in proto3 syntax (it’s okay if an imported proto2 message uses them). Nested Typesmessage SearchResponse { message Result { string url = 1; string title = 2; repeated string snippets = 3; } repeated Result results = 1; }If you want to reuse this message type outside its parent message type, you refer to it as Parent.Type: message SomeOtherMessage { SearchResponse.Result result = 1; }You can nest messages as deeply as you like message Outer { // Level 0 message MiddleAA { // Level 1 message Inner { // Level 2 int64 ival = 1; bool booly = 2; } } message MiddleBB { // Level 1 message Inner { // Level 2 int32 ival = 1; bool booly = 2; } } }Updating A Message Type Don’t change the numeric tags for any existing fields If you add new fields, any messages serialized by code using your “old” message format can still be parsed by your new generated code Fields can be removed, as long as the tag number is not used again in your updated message type You may want to rename the field instead, perhaps adding the prefix “OBSOLETE_”, or make the tag reserved, so that future users of your .proto can’t accidentally reuse the number. Compatibility int32, uint32, int64, uint64, and bool are all compatible sint32 and sint64 are compatible with each other but are not compatible with the other integer types string and bytes are compatible as long as the bytes are valid UTF-8 Embedded messages are compatible with bytes if the bytes contain an encoded version of the message fixed32 is compatible with sfixed32, and fixed64 with sfixed64 enum is compatible with int32, uint32, int64, and uint64 in terms of wire format (note that values will be truncated if they don’t fit) Moving any fields into an existing oneof is not safe Anyimport &quot;google/protobuf/any.proto&quot;; message ErrorStatus { string message = 1; repeated google.protobuf.Any details = 2; }OneofYou can add fields of any type, but cannot use repeated fields Features: - Setting a oneof field will automatically clear all other members of the oneof - If the parser encounters multiple members of the same oneof on the wire, only the last member seen is used in the parsed message - If you’re using C++, make sure your code doesn’t cause memory crashes - Again in C++, if you Swap() two messages with oneofs, each message will end up with the other’s oneof case Mapsmap&lt;key_type, value_type&gt; map_field = N The key_type can be any integral or string type. The value_type can be any type except another map. Map fields cannot be repeated Wire format ordering and map iteration ordering of map values is undefined When generating text format for a .proto, maps are sorted by key When parsing from the wire or when merging, if there are duplicate map keys the last key seen is used. When parsing a map from text format, parsing may fail if there are duplicate keys backwords compatibility: message MapFieldEntry { key_type key = 1; value_type value = 2; } repeated MapFieldEntry map_field = N;PackagesJSON Mapping","categories":[{"name":"Web","slug":"Web","permalink":"https://lxb.wiki/categories/Web/"}],"tags":[]},{"title":"qt 的 pro 文件","slug":"qt 的 pro 文件","date":"2018-03-21T01:49:25.000Z","updated":"2019-10-03T17:00:22.523Z","comments":true,"path":"8304997e/","link":"","permalink":"https://lxb.wiki/8304997e/","excerpt":"","text":"注释使用# 进行行注释 模板TEMPLATE = app 告诉qmake为这个应用程序生成哪种makefile. - app 默认值. 生成app的makefile - lib 生成一个库的makefile - vcapp 生成一个应用程序的VisualStudio项目文件 - vclib 生成一个库的VisualStudio 项目文件 - subdirs 生成makefile文件编译subdirs指定的子文件夹 应用程序目录指定生成的应用程序放置的目录 DESTDIR += ../bin 配置信息COFNIG 用来告诉qmake 关于应用程序的配置信息 CONFIG += qt warn_on release ui目录指定uic命令将.ui文件转化成的ui_*.h文件的存放目录 UI_DIR += forms rcc目录指定rcc命令将.qrc文件转换成的qrc_*.h文件的存放目录 RCC_DIR += ../tmp moc目录指定moc命令将含Q_OBJECT的头文件转换成标准.h文件的存放目录 MOC_DIR += ../tmp 目标文件目录指定目标文件(obj)的存放目录 OBJECTS_DIR += ../tmp 依赖相关路径程序编译时依赖的相关路径 DEPENDPATH += . forms include qrc sources 头文件包含路径INCLUDEPATH += . qmake时产生的信息message($$(PATH)) 源文件编码方式CODECFORSRC = GBK 工程中包含的头文件HEADERS += include/aa.h 工程中包含的.ui文件FORMS += forms/aa.ui 工程中包含的源文件SOURCES += sources/main.cpp sources/aa.cpp 工程中包含的资源文件RESOURCES += qrc/aa.qrc LIBS += -LfolderPath Release: LIBS += -LfolderReleasePath Debug: LIBS += -LfolderDebugPath DEFINES += XX_XX_XXX // 定义编译选项, 在.h文件中就可以用 #ifdefine XX_XX_XXX RC_FIELS = xxx.icns平台相关性处理根据qmake所运行的平台来使用相应的作用域来进行处理. 为Windows平台添加的依赖平台的文件示例: win32{ SOURCES += hello_win.cpp }生成Makefileqmake -oMakefile hello.pro 对于VisualStudio用户, qmake也可以生成.dsp文件 qmake -tvcapp -o hello.dsp hello.pro pro文件实例TEMPLATE = app #模块配置 LANGUAGE = C++ #C++语言 CONFIG += qt warn_on debug release #引入的lib文件,用于引入动态链接库 LIBS += qaxcontainer.lib #头文件包含路径 INCLUDEPATH += ../../qtcompnent/qtchklisten/inc ../../qtcompnent/qtclearfile/inc ../../validator/inc/validerrcode ../../qtcompnent/qtdir/inc ../inc ../../utillib/inc/xmlapi ../../utillib/inc/util ../../xercesc ../../qtcompnent/qteditor/inc ../../qtcompnent/qtfunreview/inc ../../qtcompnent/qttable/inc ../../qtcompnent/qtversion/inc ../../qtcompnent/qtini/inc ../../icdtool/icdservices/inc ../../icdtool/dataset/inc ../../icdtool/doi/inc ../../icdtool/reportcontrol/inc ../../icdtool/GSEconctrol/inc ../../icdtool/inputs/inc ../../icdtool/SMVconctrol/inc ../../icdtool/logcontrol/inc ../../scdpreview/inc/scdpreviewtoollib ../../scdpreview/form ../../icdtool/sclcontrol/inc ../../icdtool/log/inc ../../icdtool/settingcontrol/inc ../../qtcompnent/qteditor/inc ../../qtcompnent/qttreeview/inc ../../qtcompnent/qttabwidget/inc ../../communication/inc ../../qtcompnent/qtabout/inc ../iedmanage/inc ../ldmanage/inc ../foriecrun/inc ../../qtcompnent/validset/inc #工程中包含的头文件 HEADERS += ../inc/exportstable.h / ../inc/maintabwidget.h / ../inc/outputtab.h / ../inc/strutil.h / ../inc/treeeditview.h / ../inc/MainForm.h / ../inc/recenfileini.h / ../inc/ExportCIDFunction.h #工程中包含的源文件 SOURCES += ../src/main.cpp / ../src/exportstable.cpp / ../src/maintabwidget.cpp / ../src/outputtab.cpp / ../src/treeeditview.cpp / ../src/MainForm.cpp / ../src/recenfileini.cpp / ../src/ExportCIDFunction.cpp #工程中包含的.ui设计文件 FORMS = ../form/scdmainform.ui / ../form/exportiedform.ui / ../form/Exportsedform.ui / ../form/Importsedform.ui / ../form/formiminputs.ui #图像文件 IMAGES = images/substation.png / images/communication.png / images/autocom.png / images/reportcfg.png / images/comcfg.png / images/filetrans.png / images/review.png / images/setting.png #工程中包含的资源文件 RESOURCES = Scintilla.qrc #CONFIG -= release CONFIG -= debug RC_FILE = scdtool.rc BINLIB = ../../bin ../../xercesc/lib UI_HEADERS_DIR = ../inc # .ui文件转会为**.h 存放的目录 UI_SOURCES_DIR = ../src # .ui文件转会为**.cpp 存放的目录 QMAKE_LIBDIR = $${BINLIB} release { TARGET = scdtool #指定生成的应用程序名 OBJECTS_DIR = ../../obj/scdtool/release #指定目标文件(obj)的存放目录 } debug { TARGET = scdtool_d #指定生成的应用程序名 OBJECTS_DIR = ../../obj/scdtool/debug #指定目标文件(obj)的存放目录 } MOC_DIR = $${OBJECTS_DIR} DESTDIR = ../../bin #指定生成的应用程序放置的目录补充: cnblogs","categories":[{"name":"Qt","slug":"Qt","permalink":"https://lxb.wiki/categories/Qt/"}],"tags":[]},{"title":"基于FFmpeg的摄像头直播(推流)","slug":"基于FFmpeg的摄像头直播(推流)","date":"2018-01-13T12:50:43.000Z","updated":"2019-10-03T07:57:15.431Z","comments":true,"path":"ae1aac27/","link":"","permalink":"https://lxb.wiki/ae1aac27/","excerpt":"","text":"原文地址: http://blog.csdn.net/wh8_2011/article/details/73506154 本文实现: 读取PC摄像头视频数据并以RTMP协议发送为直播流. 示例包含 1. FFmpeg的libavdevice的使用 2. 视频编码, 解码, 推流的基本流程 要使用libavdevice的相关函数, 首先需要注册相关组件 avdevice_register_all() 列出电脑中可用的DShow设备 AVFormatContext *pFmtCtx = avformat_alloc_context(); AVDeviceInfoList *device_info = NULL; AVDictionary* options = NULL; av_dict_set(&amp;options, &quot;list_devices&quot;, &quot;true&quot;, 0); AVInputFormat *iformat = av_find_input_format(&quot;dshow&quot;); printf(&quot;Device Info=============\\n&quot;); avformat_open_input(&amp;pFmtCtx, &quot;video=dummy&quot;, iformat, &amp;options); printf(&quot;========================\\n&quot;); 也可以直接使用FFmpeg的工具 ffmpeg -list_devices true -f dshow -i dummy PS: avdevice有一个avdevice_list_devices函数可以枚举系统的采集设备, 包括设备名和设备描述, 可以让用户选择要使用的设备, 但是不支持DShow设备. 像打开普通文件一样将上面的具体设备名作为输入打开, 并进行相应的初始化设置 av_register_all(); //Register Device avdevice_register_all(); avformat_network_init(); //Show Dshow Device show_dshow_device(); printf(&quot;\\nChoose capture device: &quot;); if (gets(capture_name) == 0) { printf(&quot;Error in gets()\\n&quot;); return -1; } sprintf(device_name, &quot;video=%s&quot;, capture_name); ifmt=av_find_input_format(&quot;dshow&quot;); //Set own video device&apos;s name if (avformat_open_input(&amp;ifmt_ctx, device_name, ifmt, NULL) != 0){ printf(&quot;Couldn&apos;t open input stream.（无法打开输入流）\\n&quot;); return -1; } //input initialize if (avformat_find_stream_info(ifmt_ctx, NULL)&lt;0) { printf(&quot;Couldn&apos;t find stream information.（无法获取流信息）\\n&quot;); return -1; } videoindex = -1; for (i = 0; i&lt;ifmt_ctx-&gt;nb_streams; i++) if (ifmt_ctx-&gt;streams[i]-&gt;codec-&gt;codec_type == AVMEDIA_TYPE_VIDEO) { videoindex = i; break; } if (videoindex == -1) { printf(&quot;Couldn&apos;t find a video stream.（没有找到视频流）\\n&quot;); return -1; } if (avcodec_open2(ifmt_ctx-&gt;streams[videoindex]-&gt;codec, avcodec_find_decoder(ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;codec_id), NULL)&lt;0) { printf(&quot;Could not open codec.（无法打开解码器）\\n&quot;); return -1; }输入设备初始化后, 需要对输出做相应的初始化. FFmpeg将网络协议和文件同等看待, 同时因为使用RTMP协议进行传输, 因此制定输出为flv格式, 编码器使用H.264 //output initialize avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, &quot;flv&quot;, out_path); //output encoder initialize pCodec = avcodec_find_encoder(AV_CODEC_ID_H264); if (!pCodec){ printf(&quot;Can not find encoder! (没有找到合适的编码器！)\\n&quot;); return -1; } pCodecCtx=avcodec_alloc_context3(pCodec); pCodecCtx-&gt;pix_fmt = PIX_FMT_YUV420P; pCodecCtx-&gt;width = ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;width; pCodecCtx-&gt;height = ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;height; pCodecCtx-&gt;time_base.num = 1; pCodecCtx-&gt;time_base.den = 25; pCodecCtx-&gt;bit_rate = 400000; pCodecCtx-&gt;gop_size = 250; /* Some formats,for example,flv, want stream headers to be separate. */ if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER) pCodecCtx-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER; //H264 codec param //pCodecCtx-&gt;me_range = 16; //pCodecCtx-&gt;max_qdiff = 4; //pCodecCtx-&gt;qcompress = 0.6; pCodecCtx-&gt;qmin = 10; pCodecCtx-&gt;qmax = 51; //Optional Param pCodecCtx-&gt;max_b_frames = 3; // Set H264 preset and tune AVDictionary *param = 0; av_dict_set(&amp;param, &quot;preset&quot;, &quot;fast&quot;, 0); av_dict_set(&amp;param, &quot;tune&quot;, &quot;zerolatency&quot;, 0); if (avcodec_open2(pCodecCtx, pCodec,&amp;param) &lt; 0){ printf(&quot;Failed to open encoder! (编码器打开失败！)\\n&quot;); return -1; } //Add a new stream to output,should be called by the user before avformat_write_header() for muxing video_st = avformat_new_stream(ofmt_ctx, pCodec); if (video_st == NULL){ return -1; } video_st-&gt;time_base.num = 1; video_st-&gt;time_base.den = 25; video_st-&gt;codec = pCodecCtx; //Open output URL,set before avformat_write_header() for muxing if (avio_open(&amp;ofmt_ctx-&gt;pb,out_path, AVIO_FLAG_READ_WRITE) &lt; 0){ printf(&quot;Failed to open output file! (输出文件打开失败！)\\n&quot;); return -1; } //Show some Information av_dump_format(ofmt_ctx, 0, out_path, 1); //Write File Header avformat_write_header(ofmt_ctx,NULL); 完成输入和输出的初始化后, 就可以正式开始解码和编码并推流的流程了. 需要注意的是, 摄像头数据往往是RGB格式的, 需要将其转换为YUV420P格式, 才能推流, 因此要先做如下的准备工作 //prepare before decode and encode dec_pkt = (AVPacket *)av_malloc(sizeof(AVPacket)); //enc_pkt = (AVPacket *)av_malloc(sizeof(AVPacket)); //camera data has a pix fmt of RGB,convert it to YUV420 img_convert_ctx = sws_getContext(ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;width, ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;height, ifmt_ctx-&gt;streams[videoindex]-&gt;codec-&gt;pix_fmt, pCodecCtx-&gt;width, pCodecCtx-&gt;height, PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); pFrameYUV = avcodec_alloc_frame(); uint8_t *out_buffer = (uint8_t *)av_malloc(avpicture_get_size(PIX_FMT_YUV420P, pCodecCtx-&gt;width, pCodecCtx-&gt;height)); avpicture_fill((AVPicture *)pFrameYUV, out_buffer, PIX_FMT_YUV420P, pCodecCtx-&gt;width, pCodecCtx-&gt;height); 现在, 就可以正式开始解码, 编码 和推流了 //start decode and encode int64_t start_time=av_gettime(); while (av_read_frame(ifmt_ctx, dec_pkt) &gt;= 0){ if (exit_thread) break; av_log(NULL, AV_LOG_DEBUG, &quot;Going to reencode the frame\\n&quot;); pframe = av_frame_alloc(); if (!pframe) { ret = AVERROR(ENOMEM); return -1; } //av_packet_rescale_ts(dec_pkt, ifmt_ctx-&gt;streams[dec_pkt-&gt;stream_index]-&gt;time_base, // ifmt_ctx-&gt;streams[dec_pkt-&gt;stream_index]-&gt;codec-&gt;time_base); ret = avcodec_decode_video2(ifmt_ctx-&gt;streams[dec_pkt-&gt;stream_index]-&gt;codec, pframe, &amp;dec_got_frame, dec_pkt); if (ret &lt; 0) { av_frame_free(&amp;pframe); av_log(NULL, AV_LOG_ERROR, &quot;Decoding failed\\n&quot;); break; } if (dec_got_frame){ sws_scale(img_convert_ctx, (const uint8_t* const*)pframe-&gt;data, pframe-&gt;linesize, 0, pCodecCtx-&gt;height, pFrameYUV-&gt;data, pFrameYUV-&gt;linesize); enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&amp;enc_pkt); ret = avcodec_encode_video2(pCodecCtx, &amp;enc_pkt, pFrameYUV, &amp;enc_got_frame); av_frame_free(&amp;pframe); if (enc_got_frame == 1){ //printf(&quot;Succeed to encode frame: %5d\\tsize:%5d\\n&quot;, framecnt, enc_pkt.size); framecnt++; enc_pkt.stream_index = video_st-&gt;index; //Write PTS AVRational time_base = ofmt_ctx-&gt;streams[videoindex]-&gt;time_base;//{ 1, 1000 }; AVRational r_framerate1 = ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate;// { 50, 2 }; AVRational time_base_q = { 1, AV_TIME_BASE }; //Duration between 2 frames (us) int64_t calc_duration = (double)(AV_TIME_BASE)*(1 / av_q2d(r_framerate1)); //内部时间戳 //Parameters //enc_pkt.pts = (double)(framecnt*calc_duration)*(double)(av_q2d(time_base_q)) / (double)(av_q2d(time_base)); enc_pkt.pts = av_rescale_q(framecnt*calc_duration, time_base_q, time_base); enc_pkt.dts = enc_pkt.pts; enc_pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base); //(double)(calc_duration)*(double)(av_q2d(time_base_q)) / (double)(av_q2d(time_base)); enc_pkt.pos = -1; //Delay int64_t pts_time = av_rescale_q(enc_pkt.dts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if (pts_time &gt; now_time) av_usleep(pts_time - now_time); ret = av_interleaved_write_frame(ofmt_ctx, &amp;enc_pkt); av_free_packet(&amp;enc_pkt); } } else { av_frame_free(&amp;pframe); } av_free_packet(dec_pkt); } 解码比较简单, 编码部分需要自己计算PTS, DTS, 比较复杂 这里通过帧率计算PTS和DTS, 首先通过帧率计算两帧之间的时间间隔, 但是要换算","categories":[{"name":"A&amp;V","slug":"A-amp-V","permalink":"https://lxb.wiki/categories/A-amp-V/"}],"tags":[]},{"title":"基于FFmpeg的推送文件到RTMP服务器","slug":"基于FFmpeg的推送文件到RTMP服务器","date":"2018-01-13T03:51:28.000Z","updated":"2019-10-03T07:57:15.424Z","comments":true,"path":"5722b57a/","link":"","permalink":"https://lxb.wiki/5722b57a/","excerpt":"","text":"原文地址: http://blog.csdn.net/leixiaohua1020/article/details/39803457 将本地的MOV/AVI/MKV/MP4/FLV等格式的媒体文件， 通过流媒体协议(RTMP, HTTP, UDP, TCP, RTP等)以直播流的形式推送出去. 在这个推流器的基础上, 可以进行以下几种方式的修改, 实现各式各样的推流器. 例如: * 将输入文件改为网络流URL, 可以显示转流器 * 将输入文件改为回调函数(内存读取)的形式, 可以推送内存中的视频数据 * 将输入文件改为系统设备(通过libavdevice), 同时加上编码的功能, 可以实现实时推流器(现场直播) 需要注意的地方封装格式RTMP采用的封装格式FLV, 因此在指定输出流媒体的时候需要制定其封装格式为”flv”. 同理, 其他流媒体协议也需要指定其封装格式. 例如采用UDP推送流媒体的时候, 可以指定其封装格式为”mpegts”. 延时发送流媒体的数据的时候需要延时. 否则, FFmpeg处理数据速度很快, 瞬间就能把所有的数据发送出去, 流媒体服务器是承受不了的. 因此需要按照视频实际的帧率发送数据. 本文的推流器在视频帧与帧之间采用av_usleep()函数休眠的方式来延迟发送. 这样就可以按照视频的帧率发送数据了, 代码如下 //… int64_t start_time=av_gettime(); while (1) { //… //Important:Delay if(pkt.stream_index==videoindex){ AVRational time_base=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; AVRational time_base_q={1,AV_TIME_BASE}; int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if (pts_time &gt; now_time) av_usleep(pts_time - now_time); } //… } //… PTS/DTS问题没有封装格式的裸流(例如H.264裸流)是不包含PTS, DTS这些参数的. 在发送这种数据的时候, 需要自己计算并写入AVPacket的PTS, DTS, duration等参数. //FIX：No PTS (Example: Raw H.264) //Simple Write PTS if(pkt.pts==AV_NOPTS_VALUE){ //Write PTS AVRational time_base1=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; //Duration between 2 frames (us) int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate); //Parameters pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE); pkt.dts=pkt.pts; pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE); } sequence 代码 /** * 最简单的基于FFmpeg的推流器（推送RTMP） * Simplest FFmpeg Streamer (Send RTMP) * * 雷霄骅 Lei Xiaohua * leixiaohua1020@126.com * 中国传媒大学/数字电视技术 * Communication University of China / Digital TV Technology * http://blog.csdn.net/leixiaohua1020 * * 本例子实现了推送本地视频至流媒体服务器（以RTMP为例）。 * 是使用FFmpeg进行流媒体推送最简单的教程。 * * This example stream local media files to streaming media * server (Use RTMP as example). * It&apos;s the simplest FFmpeg streamer. * */ #include &lt;stdio.h&gt; #define __STDC_CONSTANT_MACROS #ifdef _WIN32 //Windows extern &quot;C&quot; { #include &quot;libavformat/avformat.h&quot; #include &quot;libavutil/mathematics.h&quot; #include &quot;libavutil/time.h&quot; }; #else //Linux... #ifdef __cplusplus extern &quot;C&quot; { #endif #include &lt;libavformat/avformat.h&gt; #include &lt;libavutil/mathematics.h&gt; #include &lt;libavutil/time.h&gt; #ifdef __cplusplus }; #endif #endif int main(int argc, char* argv[]) { AVOutputFormat *ofmt = NULL; //输入对应一个AVFormatContext，输出对应一个AVFormatContext //（Input AVFormatContext and Output AVFormatContext） AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL; AVPacket pkt; const char *in_filename, *out_filename; int ret, i; int videoindex=-1; int frame_index=0; int64_t start_time=0; //in_filename = &quot;cuc_ieschool.mov&quot;; //in_filename = &quot;cuc_ieschool.mkv&quot;; //in_filename = &quot;cuc_ieschool.ts&quot;; //in_filename = &quot;cuc_ieschool.mp4&quot;; //in_filename = &quot;cuc_ieschool.h264&quot;; in_filename = &quot;cuc_ieschool.flv&quot;;//输入URL（Input file URL） //in_filename = &quot;shanghai03_p.h264&quot;; out_filename = &quot;rtmp://localhost/publishlive/livestream&quot;;//输出 URL（Output URL）[RTMP] //out_filename = &quot;rtp://233.233.233.233:6666&quot;;//输出 URL（Output URL）[UDP] av_register_all(); //Network avformat_network_init(); //输入（Input） if ((ret = avformat_open_input(&amp;ifmt_ctx, in_filename, 0, 0)) &lt; 0) { printf( &quot;Could not open input file.&quot;); goto end; } if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) &lt; 0) { printf( &quot;Failed to retrieve input stream information&quot;); goto end; } for(i=0; i&lt;ifmt_ctx-&gt;nb_streams; i++) if(ifmt_ctx-&gt;streams[i]-&gt;codec-&gt;codec_type==AVMEDIA_TYPE_VIDEO){ videoindex=i; break; } av_dump_format(ifmt_ctx, 0, in_filename, 0); //输出（Output） avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, &quot;flv&quot;, out_filename); //RTMP //avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, &quot;mpegts&quot;, out_filename);//UDP if (!ofmt_ctx) { printf( &quot;Could not create output context\\n&quot;); ret = AVERROR_UNKNOWN; goto end; } ofmt = ofmt_ctx-&gt;oformat; for (i = 0; i &lt; ifmt_ctx-&gt;nb_streams; i++) { //根据输入流创建输出流（Create output AVStream according to input AVStream） AVStream *in_stream = ifmt_ctx-&gt;streams[i]; AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream-&gt;codec-&gt;codec); if (!out_stream) { printf( &quot;Failed allocating output stream\\n&quot;); ret = AVERROR_UNKNOWN; goto end; } //复制AVCodecContext的设置（Copy the settings of AVCodecContext） ret = avcodec_copy_context(out_stream-&gt;codec, in_stream-&gt;codec); if (ret &lt; 0) { printf( &quot;Failed to copy context from input to output stream codec context\\n&quot;); goto end; } out_stream-&gt;codec-&gt;codec_tag = 0; if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER) out_stream-&gt;codec-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER; } //Dump Format------------------ av_dump_format(ofmt_ctx, 0, out_filename, 1); //打开输出URL（Open output URL） if (!(ofmt-&gt;flags &amp; AVFMT_NOFILE)) { ret = avio_open(&amp;ofmt_ctx-&gt;pb, out_filename, AVIO_FLAG_WRITE); if (ret &lt; 0) { printf( &quot;Could not open output URL &apos;%s&apos;&quot;, out_filename); goto end; } } //写文件头（Write file header） ret = avformat_write_header(ofmt_ctx, NULL); if (ret &lt; 0) { printf( &quot;Error occurred when opening output URL\\n&quot;); goto end; } start_time=av_gettime(); while (1) { AVStream *in_stream, *out_stream; //获取一个AVPacket（Get an AVPacket） ret = av_read_frame(ifmt_ctx, &amp;pkt); if (ret &lt; 0) break; //FIX：No PTS (Example: Raw H.264) //Simple Write PTS if(pkt.pts==AV_NOPTS_VALUE){ //Write PTS AVRational time_base1=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; //Duration between 2 frames (us) int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate); //Parameters pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE); pkt.dts=pkt.pts; pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE); } //Important:Delay if(pkt.stream_index==videoindex){ AVRational time_base=ifmt_ctx-&gt;streams[videoindex]-&gt;time_base; AVRational time_base_q={1,AV_TIME_BASE}; int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q); int64_t now_time = av_gettime() - start_time; if (pts_time &gt; now_time) av_usleep(pts_time - now_time); } in_stream = ifmt_ctx-&gt;streams[pkt.stream_index]; out_stream = ofmt_ctx-&gt;streams[pkt.stream_index]; /* copy packet */ //转换PTS/DTS（Convert PTS/DTS） pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream-&gt;time_base, out_stream-&gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream-&gt;time_base, out_stream-&gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); pkt.duration = av_rescale_q(pkt.duration, in_stream-&gt;time_base, out_stream-&gt;time_base); pkt.pos = -1; //Print to Screen if(pkt.stream_index==videoindex){ printf(&quot;Send %8d video frames to output URL\\n&quot;,frame_index); frame_index++; } //ret = av_write_frame(ofmt_ctx, &amp;pkt); ret = av_interleaved_write_frame(ofmt_ctx, &amp;pkt); if (ret &lt; 0) { printf( &quot;Error muxing packet\\n&quot;); break; } av_free_packet(&amp;pkt); } //写文件尾（Write file trailer） av_write_trailer(ofmt_ctx); end: avformat_close_input(&amp;ifmt_ctx); /* close output */ if (ofmt_ctx &amp;&amp; !(ofmt-&gt;flags &amp; AVFMT_NOFILE)) avio_close(ofmt_ctx-&gt;pb); avformat_free_context(ofmt_ctx); if (ret &lt; 0 &amp;&amp; ret != AVERROR_EOF) { printf( &quot;Error occurred.\\n&quot;); return -1; } return 0; }","categories":[{"name":"A&amp;V","slug":"A-amp-V","permalink":"https://lxb.wiki/categories/A-amp-V/"}],"tags":[]},{"title":"ffmpeg 推流工具","slug":"ffmpeg 推流工具","date":"2018-01-12T11:52:44.000Z","updated":"2019-10-03T07:57:15.306Z","comments":true,"path":"b3ed22dc/","link":"","permalink":"https://lxb.wiki/b3ed22dc/","excerpt":"","text":"SRR测试网址 http://www.ossrs.net/srs.release/trunk/research/players/srs_player.html 获取 git clone https://github.com/ossrs/srs.git configure make cd srs/trunk ./configure &amp;&amp; make 开启服务器 ./objs/srs -c conf/srs.conf 列出设备 ./ffmpeg.exe -list_devices true -f dshow -i dummy ffmpeg采集摄像头推流 ffmpeg.exe -f dshow -i video=&quot;EasyCamera&quot; -q 4 -s 640*480 -aspect 4:3 -r 10 -vcodec flv -ar 22050 -ab 64k -ac 1 -acodec libmp3lame -threads 4 -f flv rtmp://192.168.1.102/RTMP/RtmpVideo ffmpeg采集摄像头和麦克风推流 ffmpeg -f dshow -i video=&quot;USB2.0 PC CAMERA&quot; -f dshow -i audio=&quot;麦克风 (2- USB2.0 MIC)&quot; -b:a 600k -ab 128k -f flv rtmp://192.168.1.102/RTMP/RtmpVideo","categories":[{"name":"A&amp;V","slug":"A-amp-V","permalink":"https://lxb.wiki/categories/A-amp-V/"}],"tags":[]},{"title":"命令组和代码块","slug":"命令组和代码块","date":"2017-10-02T03:54:01.000Z","updated":"2019-10-03T07:57:15.291Z","comments":true,"path":"8cb7d3c0/","link":"","permalink":"https://lxb.wiki/8cb7d3c0/","excerpt":"","text":"命令组 和 代码块() 命令组. 如 (a=hello,echo $a) 在()中的命令列表, 将作为一个子Shell来运行 在()中的变量, 由于是在子Shell总运行的, 因此对脚本剩下的部分是不可见的 如 a=123 (a=321;) echo &quot;a=$a&quot; # a=123 # 在()中的a变量, 更像是一个局部变量{} 代码块, 又称内部组. 这个结构创建了一个匿名的函数, 与函数不同的是, 在{}中声明的变量, 对于脚本剩余的代码是可见的, 如 { local a; a=123; } # bash中的local申请的变量只能用在函数中 a=123; {a=321;} echo &quot;a=$a&quot; # a=321()也可用作初始化数组 array=(element1,element2,element3) {xxx,yyy,zzz} 大括号扩展, 例 cat {file1,file2,file3} &gt; combined_file # 把file1 file2 file3连接在一起, 重定向到combined_file cp file1.{txt,bak} # 把file1.txt 复制到file1.bak一个命令会对大括号中以逗号分隔的文件列表起作用, file globbing会对大括号中的文件名作扩展 # 大括号中不允许有空白, 除非这个空白是有意义的 echo {file1,file2}\\ :{\\ A,&quot; B&quot;,&apos; C&apos;} # file1 : A file1 : B file1 : C file2 : A file2 : B file2 : C","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"Shell 中的冒号","slug":"Shell 中的冒号","date":"2017-09-07T09:55:50.000Z","updated":"2019-10-03T07:57:15.386Z","comments":true,"path":"9aa135a6/","link":"","permalink":"https://lxb.wiki/9aa135a6/","excerpt":"","text":"冒号(:) 是一个空命令. 作用与true相同. “:”是一个bash内建命令, 返回值为0, 即与true相同. 例: : echo $? # 0死循环 while : do list_1 list_2 doneif/then 中的占位符 if list then : # 什么都不做, 引出分支 else take-some-action fi在一个2元命令中, 提供一个占位符, 表明后面的表达式, 不是一个命令, 如 :$((n=$n+1)如果没有:, bash会尝试把”$((n=$n+1))” 解释成一个命令 使用”参数替换” 来评估字符串变量 :${HOSTNAME?}${USER?}${MAIL?} # 如果一个或多个环境变量没有设置, 则打印错误信息在和&gt;(重定向符号)结合使用时, 把一个文件截断到0长度, 不修改它的权限. 如果文件不存在, 则创建它 : &gt; data.xxx # 文件&quot;data.xxx&quot; 被清空 # 与 cat /dev/null &gt; data.xxx 作用相同, 但是不会产生一个新的进程, 因为:是一个内建命令.只适用于普通文件, 不适用于管道, 符号链接, 和其他特殊文件. 也可以用作注释, :与#不同的是, :不会关闭剩余行的错误检查.","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"Shell编程中select用法","slug":"Shell编程中select用法","date":"2017-09-05T08:02:11.000Z","updated":"2019-10-03T07:57:15.388Z","comments":true,"path":"53b3b0c1/","link":"","permalink":"https://lxb.wiki/53b3b0c1/","excerpt":"","text":"select提供了一个构建交互式菜单程序的方式, 语法结构: select name [ in word ] ; do list ; done 例: #!/bin/bash select i in a b c d do echo $i done执行结果 $ ./select.sh 1) a 2) b 3) c 4) d #? 选择索引 $ ./select.sh 1) a 2) b 3) c 4) d #? 1 a #? 2 b #? 3 c #? 4 d #? 6 #? 1) a 2) b 3) c 4) d #? 1) a 2) b 3) c 4) d #? 如果输入的不是菜单描述的范围就会echo一个空行，如果直接输入回车，就会再显示一遍菜单本身。当然我们会发现这样一个菜单程序似乎没有什么意义，实际程序中，select大多数情况是跟case配合使用的。 #!/bin/bash select i in a b c d do case $i in a) echo &quot;Your choice is a&quot; ;; b) echo &quot;Your choice is b&quot; ;; c) echo &quot;Your choice is c&quot; ;; d) echo &quot;Your choice is d&quot; ;; *) echo &quot;Wrong choice! exit!&quot; ;; esac done执行结果 $ ./select.sh 1) a 2) b 3) c 4) d #? 1 Your choice is a #? 2 Your choice is b #? 3 Your choice is c #? 4 Your choice is d #? 5 Wrong choice! exit!","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[]},{"title":"在shell脚本中使用ls命令的注意事项","slug":"在shell脚本中使用ls命令的注意事项","date":"2017-09-05T07:03:02.000Z","updated":"2020-05-05T11:49:56.250Z","comments":true,"path":"12994d1/","link":"","permalink":"https://lxb.wiki/12994d1/","excerpt":"","text":"请对比如下两个测试： $ for i in `ls /etc`;do echo $i;done adjtime adobe appstream.conf arch-release asound.conf avahi bash.bash_logout bash.bashrc bind.keys binfmt.d ...... $ for i in /etc/*;do echo $i;done /etc/adjtime /etc/adobe /etc/appstream.conf /etc/arch-release /etc/asound.conf /etc/avahi /etc/bash.bash_logout /etc/bash.bashrc /etc/bind.keys /etc/binfmt.d ......像ls这样的命令很多时候是设计给人用的，它的很多显示是有特殊设定的，可能并不是纯文本。 比如可能包含一些格式化字符，也可能包含可以让终端显示出颜色的标记字符等等。 当我们在程序里面使用类似这样的命令的时候要格外小心，说不定什么时候在什么不同环境配置的系统上， 你的程序就会有意想不到的异常出现，到时候排查起来非常麻烦。 所以这里我们应该尽量避免使用ls这样的命令来做类似的行为，用通配符可能更好。 当然，如果你要操作的是多层目录文件的话，那么ls就更不能帮你的忙了，它遇到目录之后显示成这样： $ ls /etc/* /etc/adobe: mms.cfg /etc/avahi: avahi-autoipd.action avahi-daemon.conf avahi-dnsconfd.action hosts services /etc/binfmt.d: /etc/bluetooth: main.conf /etc/ca-certificates: extracted trust-source所以遍历一个目录还是要用两个连续的**，如果不是bash 4.0之后的版本的话，可以使用find。 我推荐用find，因为它更通用。 有时候你会发现，使用find之后，绝大多数原来需要写脚本解决的问题可能都用不着了，一个find命令解决很多问题","categories":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/categories/Shell/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://lxb.wiki/tags/Shell/"}]},{"title":"邮件发送原理","slug":"邮件发送原理","date":"2017-06-08T02:34:08.000Z","updated":"2019-10-03T16:50:54.247Z","comments":true,"path":"b0248f59/","link":"","permalink":"https://lxb.wiki/b0248f59/","excerpt":"","text":"SMTP(Simple Mail Transfer Protocol)是电子邮件从客户机传输到服务器或从某一个服务器传输到另一个服务器使用的传输协议。SMTP 是请求/响应协议，命令和响应都是基于 ASCII 文本，并以 CR 和 LF 符结束。响应包括一个表示返回状态的三位数字代码。在 TCP 协议 25 端口监听连接请求。其命令如下： SMTP命令 命令说明 HELO ＜domain＞＜CRLF＞ 识别发送方到接收SMTP的一个HELO命令 AUTH LOGIN 登陆服务器的命令。在这条命令之后，要发送用Base64编码后的用户名与密码进行登陆 MAIL FROM:＜reverse-path＞＜CRLF＞ ＜reverse-path＞为发送者地址。此命令告诉接收方一个新邮件发送的开始，并对所有的状态和缓冲区进行初始化。此命令开始一个邮件传输处理，最终完成将邮件数据传送到一个或多个邮箱中 RCPT TO:＜forward-path＞＜CRLF＞ ＜forward-path＞标识各个邮件接收者的地址 DATA ＜CRLF＞ 接收SMTP将把其后的行为看作邮件数据去处理，以＜CRLF＞.＜CRLF＞标识数据的结尾 REST ＜CRLF＞ 退出/复位当前的邮件传输 NOOP ＜CRLF＞ 要求接收SMTP仅做OK应答。（用于测试） QUIT ＜CRLF＞ 要求接收SMTP返回一个OK应答并关闭传输。 VRFY ＜string＞ ＜CRLF＞ 验证指定的邮箱是否存在，由于安全因素，服务器多禁止此命令。 EXPN ＜string＞ ＜CRLF＞ 验证给定的邮箱列表是否存在，扩充邮箱列表，也常禁止使用。 HELP ＜CRLF＞ 查询服务器支持什么命令 邮件交互图 A-&gt;B: 1. 建立TCP连接(host:port, 默认port为25) B-&gt;A: 220. Anti-spam GT for Coremail System Note over A: A-&gt;B: 2. 向服务器标识用户身份(HELO host\\r\\/n) B-&gt;A: 250 OK Note over A: A-&gt;B: 3. 登录服务器(AUTH LOGIN\\r\\/n) B-&gt;A: 334. username: (这里是解密后的信息) A-&gt;B: &lt;my_username&gt;(要用Base64加密) B-&gt;A: 334. password: (这里是解密后的信息) A-&gt;B: &lt;my_password&gt;(要用Base64加密) B-&gt;A: 235. Authentication successful Note over A: A-&gt;B: 4. 指定发信者(MAIL FROM: &lt;my_sender@gmail.com&gt;\\r\\/n) B-&gt;A: 250. Mail OK Note over A: A-&gt;B: 5. 指定收信者(RCPT TO: &lt;my_receiver@gmail.com&gt;\\r\\/n) B-&gt;A: 250. Mail OK Note over A: A-&gt;B: 6. 发送数据(DATA\\r\\/n) B-&gt;A: 354. End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt; Note over A: A-&gt;B: 7. to: &lt;my_receiver@gmail.com\\r\\/nsubject:&lt;my_subject&gt;\\r\\/nSome Context\\r\\/n.\\r\\/n&gt; B-&gt;A: 250. Mail OK Note over A: A-&gt;B: 8. QUIT\\r\\/n B-&gt;A: 221. Bye因markdown里不能打出”\\n”, 因此使用”\\/n” 代替”\\n” SMTP发信操作及返回码 [crazywill@localhost crazywill]$ telnet smtp.163.com 25 #telnet登录25端口 Trying 202.108.5.81... Connected to smtp.163.com. Escape character is &apos;^]&apos;. 220 163.com Coremail SMTP(Anti Spam) System EHLO smtp.163.com # 握手 :) 250-mail 250-PIPELINING 250-AUTH LOGIN PLAIN 250-AUTH=LOGIN PLAIN 250 8BITMIME AUTH LOGIN # 开始认证登录 334 dXNlcm5hbWU6 crazywill 334 UGFzc3dvcmQ6 mypassword 535 Error: authentication failed # 直接用户名密码不能登录 AUTH LOGIN 334 dXNlcm5hbWU6 Y3Jhenl3aWxs 334 UGFzc3dvcmQ6 bXlwYXNzd29yZA== 235 Authentication successful # 使用Base64编码则成功登录 MAIL FROM:&lt;test@163.com&gt; # 邮件发送方 553 You are not authorized to send mail, authentication is required # 不可伪造发送邮件 MAIL FROM:&lt;crazywill@163.com&gt; # 邮件发送方 250 Mail OK RCPT TO:&lt;crazywill@163.com&gt; # 邮件的接收方，若有多个收件人，则重复这一语句多次。 250 Mail OK DATA # 邮件体内容 354 Please start mail input. TO: crazywill@163.com # 此处的TO，FROM，等内容，可以随便造假 :) 可以骗人但骗不了懂得查看邮件源码的。 FROM: cccc@163.com SUBJECT: test by telnet/smtp test, just a test. # 邮件正文内容，与Header部分空一行开始写 . # 邮件写完，以一个句点加回车结果。 250 Mail OK queued as smtp10,wKjADQ2ApxRnnqBE0CWaEw==.38326S3 # 返回250 表示发送成功。 NOOP # 空语句，不执行任何操作，一般用来保持和服务器连接，不要掉线 250 OK QUIT # 退出 221 Closing connection. Good bye. Connection closed by foreign host. [crazywill@localhost crazywill]$ 参考资料: 用c++发邮件 电子邮件发送的原理以及简易实现 邮件正文及其附件的发送的C++实现 C++通过SMTP发送邮件总结 C++实现向多人发送邮件","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[{"name":"邮件","slug":"邮件","permalink":"https://lxb.wiki/tags/邮件/"}]},{"title":"[译]How to split a string in C++","slug":"How to split a string in C++","date":"2017-06-04T10:40:14.000Z","updated":"2020-05-10T03:29:59.579Z","comments":true,"path":"9747854a/","link":"","permalink":"https://lxb.wiki/9747854a/","excerpt":"","text":"这个问题是说, 怎么得到组成一句话的各个单词, 或者得到CSV中的各个数据片段. 这在C++中是个很简单的问题, 却有很多种答案. 有3种方案, 每种有利有弊. 使用时请自己选择最佳方案. 这篇文章的目的是说明 迭代器的接口是如何优胜于简单的容器的, 并且阐明 design of the STL 是何等强大. 方案1使用的标准组件(虽然方案1.2 做了微调). 方案2相对好点但使用了boost. 而方案3 更好但使用了ranges. 所以到底应该用哪个, 取决于你需要什么和你能使用什么. Solution 1: Iterating on a streamStepping into the world of streams“流” 是一个 能生成 与源或希望连接的目标 的联系 的对象. 流可以从源中获取信息(std::istream), 或为目标提供信息(std::ostream), 或者两者皆可(std::iostream). 源和目标可以是标准输入(std::cin), 标准输出(std::cout), 一个文件, 或者一个字符串, 前提是方式得当. 对流的主要操作包括: - 对于输入流: 使用操作符&gt;&gt; 从里面读取信息 - 对于输出流: 使用操作符&lt;&lt;, 向它推入信息 一个指向字符串的输入流, std::istringstream, 有个有趣的特性: 它的操作符&gt;&gt; 在源字符串中制造出去向下一个空格的字符串. istream_iteratorstd::istream_iterator 是连接输入流的迭代器. 它代表了输入迭代器的普遍接口, 但它的操作符++ 更像是输入流. istream_iterator 以它从流里读取的类型为模板. 我们现在使用istream_iterator&lt;std::string&gt;, 它从流里读取字符串, 分离时为我们提供一个字符串. 当到达流的终点时, 流向它的迭代器发送信号, 然后迭代器被标记为结束. Solution 1.1现在, 我们可以借迭代器的接口使用算法, 这真切地证明了STL 设计的灵活性. 为了使用STL, 我们需要一个begin 和一个end (请参考Inserting several elements into an STL container efficiently). begin 是一个 还没开始着手分割的字符串的istreamstream 的迭代器: std::istream_iterator&lt;std::string&gt;(iss) . 按照惯例, end 的默认值也是个istream_iterator : std::istream_iterator&lt;string&gt;(). 代码如下: std::string text = &quot;Let me split this into words&quot;; std::istringstream iss(text); std::vector&lt;std::string&gt; results((std::istream_iterator&lt;std::string&gt;(iss)), std::istream_iterator&lt;std::string&gt;());第一个参数的额外的括号是为了避免与一个函数调用的歧义–请参考Scott Meyers的著作Effective STL 条目6 “most vexing parse” 优: - 仅使用标准组件 - 除字符串外, 对所有流都适用 劣: - 只能以空格为分隔符进行分割, 而且这在解析CSV时会是个至关重要的问题 - 在性能方面有待优化(但如果这不是影响你整个程序的瓶颈, 这也不是个大问题) - 很多人认为仅为了分割一个字符串, 写了太多代码 Solution1.2: Pimp my operator&gt;&gt;导致上面两条劣势的原因是同一个: istream_iterator 从流里读取字符串时调用的操作符&gt;&gt;. 这个操作符做了很多事: 在下一个空格处停止(这是我们的最初的需求, 但这个不能自定义), 格式化, 读取然后设置一些标志位, 构造对象, 等等. 而以上这些, 大部分我们是不需要的. 所以我们希望自己实现下面的函数: std::istream&amp; operator&gt;&gt;(std::istream&amp; is, std::string&amp; output) { // ...does lots of things... }实际上, 我们无法改变这些, 因为这是在标注库里的. 我们可以用另一个类型重载它, 但是这个类型需要是string 的一种. 所以现在的需求就是, 用另一种类型伪装成string. 有两种方案: 继承std::string 和 用显式转换封装string. 这里我们选择继承. 假如我们希望以逗号为分割符分割一个字符串: class WordDelimitedByCommas: pulic std::string {};我必须承认这是有争议的. 有人会说:”std::string 没有虚析构函数, 所以你不应该继承它!” 这可能, 大概, 也许是有一点点点点武断. 这里我要说的是, 继承本身不会产生问题. 诚然, 当一个指向WordDelimitedByCommas 的指针以std::string 的形式被delete 掉时, 会产生问题. 继续读, 你会发现, 我们不会这么做. 现在我们可以阻止写代码的人借WordDelimitedByCommas 突发冷箭破坏程序吗? 我们不能. 但是这个险值得我们冒吗? 请继续读, 然后你自己判断. 现在为了仅实现我们需要的功能, 我们可以重载操作符&gt;&gt; : 获取下一个逗号之前的所有字符. 这个可以借用getline 函数实现: std::istream&amp; operator&gt;&gt;(std::istream* is, std::WordDelimitedByCommas&amp;) { std::getline(is, output, &apos;,&apos;); return is; }返回值is 保证了可以连续调用操作符&gt;&gt; 现在我们可以写初级代码了: std::string text = &quot;Let,me,split,this,into,words&quot;; std::istringstream iss(text); std::vector&lt;std::string&gt; results((std::istream_iterator&lt;WordDelimitedByCommas&gt;(iss)), std::istream_iterator&lt;WordDelimitedByCommas&gt;());我们可以通过模板化WordDelimitedByCommas 泛华所有的分隔符: template&lt;char delemiter&gt; class WordDelimitedBy: pulic std::string {};现在以分号举例: std::string text = &quot;Let;me;split;this;into;words&quot;; std::istringstream iss(text); std::vector&lt;std::string&gt; results((std::istream_iterator&lt;WordDelimitedBy&lt;&apos;;&apos;&gt;&gt;(iss)), std::istream_iterator&lt;WordDelimitedBy&lt;&apos;;&apos;&gt;&gt;());优: - 编译时允许任何分隔符 - 不仅是字符串, 对任何流都可以操作 - 比方案1更快(快20%到30%) 劣: - 虽然可以很方便的复用, 但仍不是标准 - 仅仅为了分割一个字符串, 这个方案仍然使用了大量代码 Solution2: Using boost::split这个方案比方案1高级, 除非你需要对所有的流都进行操作. #include &lt;boost/algorithm/string.hpp&gt; std::string text = &quot;Let me split this into words&quot;; std::vector&lt;std::string&gt; result; boost::split&lt;results, text, [](char c){return &apos; &apos; == c;});传给boost::split 的第三个参数是一个函数或函数对象, 确定一个字符是不是分隔符. 上面的例子是使用lambda 表达式, 传入一个char, 返回这个char 是否是空格. boost::split 的实现很简单: 在到达字符串的结束位置之前, 重复地调用find_if . 优: - 非常直观的接口 - 允许任何分隔符, 甚至是多个 - 高效: 比方案1.1 快 60% 劣: - 暂不是标准: 需要用到boost Solution 3(未来): Usingranges虽然它们现在还没有像标准库甚至boost 里的组件一样被广泛使用, ranges 是future of the STL . 在未来几年, 会大量面世. Eric Neiber 的 range-v3 库 提供了非常友好的接口. 为了生成一个字符串的分割view, 代码如下: std::string text = &quot;Let me split this into words&quot;; auto splitText = text | view::split(&apos; &apos;);它有很多有趣的特性, 诸如 使用一个子字符串作为分隔符. ranges 会被C++20 引入, 所以我们应该能在几年之内就可以使用这个功能了. So, how do I split my string?如果你能使用boost, 务必使用方案2. 或者你可以自己写算法, 像boost 那样基于find_if 分割字符串. 如果你不想这么做, 你可以使用标准, 即方案1.1, 如果你需要自定义分隔符, 或者发现1.1是个瓶颈, 那么你可以选择方案1.2 . 如果你可以使用ranges , 那么就应该选择方案3. 翻译原文: http://www.fluentcpp.com/2017/04/21/how-to-split-a-string-in-c/","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"翻译","slug":"翻译","permalink":"https://lxb.wiki/tags/翻译/"}]},{"title":"strict weak ordering","slug":"strict-weak-ordering","date":"2017-05-27T06:42:41.000Z","updated":"2019-10-03T17:02:29.799Z","comments":true,"path":"22f34ac7/","link":"","permalink":"https://lxb.wiki/22f34ac7/","excerpt":"","text":"A strict weak ordering is a binary relation &lt; on a set S that is a strict partial order (a transitive relation that is irreflexive, or equivalently, that is asymmetric) in which the relation neither a &lt; b nor b &lt; a is transitive. Therefore, a strict weak ordering has the following properties: For all x in S, it is not the case that x &lt; x (irreflexivity). For all x, y in S, if x &lt; y then it is not the case that y &lt; x (asymmetry). For all x, y, z in S, if x &lt; y and y &lt; z then x &lt; z (transitivity). For all x, y, z in S, if x is incomparable with y (neither x &lt; y nor y &lt; x hold), and y is incomparable with z, then x is incomparable with z (transitivity of incomparability). This list of properties is somewhat redundant, as asymmetry follows readily from irreflexivity and transitivity. 离散数学中的relation: Given a function f (which models a binary relation) over a domain D, and a, b ∈ D: Reflexivity: f (a, a) is true. Asymmetry: For a ≠ b, if f(a, b) is true, f(b,a) is false Anti-symmetry: If f(a, b) and f(b, a) are both true iff a ≡ b Transitivity: If f(a, b) and f(b, c) are true, then f(a, c) is true Incomparability: Neither f(a, b) nor f(b, a) is true Transitivity of incomparability: If a and b are incomparable, and so are b and c, then a and c are incomparable. 摘自WikiPedia","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[]},{"title":"Linux 禁止普通用户su切换root","slug":"linux-e7-a6-81-e6-ad-a2-e6-99-ae-e9-80-9a-e7-94-a8-e6-88-b7su-e5-88-87-e6-8d-a2root","date":"2017-05-23T01:26:31.000Z","updated":"2019-10-03T07:57:15.355Z","comments":true,"path":"e5131675/","link":"","permalink":"https://lxb.wiki/e5131675/","excerpt":"","text":"一般情况下, 普通用户执行”su -“命令, 可以登录为root. 为了加强系统的安全性, 有必要建立一个管理员的组, 只允许这个组的用户执行”su -“ 命令登录为root, 而让其他组的用户即使执行”su -“ 输入了正确的密码, 也无法登录为root用户. 在Unix 和Linux 下, 这个组的名称通常为”wheel”. 1 添加一个用户, 把这个用户加入wheel组 2 修改/etc/pam.d/su #auth required pam_wheel.so use_uid 这行注释打开 3 修改/etc/login.defs 在文件末添加一行 SU_WHEEL_ONLY yes","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[]},{"title":"禁止root用户ssh登录机器","slug":"禁止root用户ssh登录机器","date":"2017-05-23T01:07:42.000Z","updated":"2019-10-03T07:57:15.297Z","comments":true,"path":"d29ba5b9/","link":"","permalink":"https://lxb.wiki/d29ba5b9/","excerpt":"","text":"1 修改 /etc/ssh/sshd_config #PermitRootLogin yes 取消注释并改为 PermitRootLogin no 2 重启ssh /etc/init.d/sshd restart","categories":[{"name":"Linux","slug":"Linux","permalink":"https://lxb.wiki/categories/Linux/"}],"tags":[]},{"title":"Move semantics of RapidJSON","slug":"move-semantics-of-rapidjson","date":"2017-05-10T02:10:27.000Z","updated":"2020-05-10T03:28:52.840Z","comments":true,"path":"e5a4892c/","link":"","permalink":"https://lxb.wiki/e5a4892c/","excerpt":"","text":"RapidJSON 的设计有一个特性, 进行赋值操作时, 不是把源value复制(copy)到目的 value, 而是转移(move)到目的value. 例如 Value a(123); Value b(456); b = a; // a becomes a Null value, b becomes number 123. 这样的设计的目的是 为了提高性能. 对于固定大小的JSON类型(Number, True, False, Null), 复制很简单快捷. 而对于可变大小的类型(String, Array, Object), 复制时会产生大量不容易被察觉的开销. 尤其是当我们需要创建一个临时的值, 把它复制给另一个变量, 然后析构它. 若使用正常的复制 语义: Document d; Value o(kObjectType); { Value contacts(kArrayType); // Adding elements to contacts array. // ... o.AddMember(&quot;contacts&quot;, contacts, d.GetAllocator(); // deep clone contacts(may be with lots of allocations) // destruct contact } o 需要分配跟contacts 大小一样的缓冲区, 做深度复制, 然后析构contacts . 这样会产生大量不必要的内存分配/释放 和内存复制. 有一些方案可以避免实质的复制这些数据, 如引用计数, 垃圾回收等等. 为了使RapidJSON简单和快速, 我们选择使用转移语义来进行赋值. 这与std::auto_ptr类似, 都是在赋值时转移拥有权. 转移比复制简捷地多, 它只需 析构原来的值, 把源值memcpy() 到目的值, 最后再把源值 设为Null类型. 使用转移语义, 上面的例子变成: Document d; Value o(kObjectType); { Value contacts(kArraryType); // Adding elements to contacts array. o.AddMember(&quot;contacts&quot;, contacts, d.GetAllocator()); // Just memcpy() of contacts itself to the value of new member(16 bytes) // contacts became Null here. Its destructiong is trivial. } 转移语义和临时值 有时, 我们想直接构造一个临时变量传给”转移”函数, 如PushBack() , AddMember() . 由于临时对象不能直接转化成正常的值引用, 我们可以调用Move() 函数 Value a(kArrayType); Document::AllocatorType&amp; allocator = document.GetAllocator(); // a.PushBack(Value(42), allocator); // Compiling error a.PushBack(Value().SetInt(42), allocator); // fluent API a.PushBack(Value(42).Move(), allocator); // same as above翻译原文: http://rapidjson.org/md\\_doc\\_tutorial.html#MoveSemantics","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[]},{"title":"[译]C++11 sort using function objects","slug":"c11-sort-using-function-objects","date":"2017-04-19T13:58:53.000Z","updated":"2020-05-10T03:30:10.894Z","comments":true,"path":"e754bcbe/","link":"","permalink":"https://lxb.wiki/e754bcbe/","excerpt":"","text":"如果你用C++编码， 需要对容器内的元素进行排序， 这个容器提供任意访问的迭代器， 比如std::vector， 那么简单快捷的方法是使用里的std::sort 函数. Basic sorting std::sort 函数需要两个参数, 这两个参数分别指向你要排序的序列容器的开始(initial)和终点(final). 这个序列容易内除final指向的那个元素外 所有元素都会被排序. 下面是一个简单的排序例子: #include &lt;algorithm&gt; #include &lt;vector&gt;&lt;/vector&gt;&lt;/algorithm&gt; const int array[] {10, 20, 5, 15, 0}; std::vector&lt;int&gt; vec(array, array + 5);&lt;/int&gt; std::sort(vec.begin(), vec.end());输出: 0 5 10 15 20 More complex sorting 在某些时候, 根据数值升序排序已经足够解决问题了, 但是当我们需要按某个特定的参数进行排序, 或者降序排列时, 就需要一些其他的东西了. 对于这种需求, std::sort 需要引入第三个参数: 比较函数. 这个比较函数有两个参数, 分别是序列容器的两个元素, 返回值可以隐式地转为bool. 如果第一个参数应该排在第二个参数前面, 则返回true. 例: #include &lt;algorithm&gt; #include &lt;vector&gt;&lt;/vector&gt;&lt;/algorithm&gt; bool DescOrderInt(int a, int b); ... const int array[] = {10, 20, 5, 15, 0}; std::vector&lt;int&gt; vec(array, array + 5);&lt;/int&gt; std::sort(vec.begin(), vec.end(), DescOrderInt);DescOrderInt的实现: bool DescOrderInt(int a, int b) { return a &amp;gt; b; }输出: 20 15 10 5 0 C++11 sort using function objects 网上很多例子说, 为了排列元素, 可以使用std::binary_function 定义比较函数, 但不幸的是, std::binary_function 在C++11 中已经被标为 “将被弃用的”, 在C++17中会被完全移除, 所以写新的C++代码时, 最好不要用这个. 我们可以使用C++11中引入的std::function 来定义这个函数指针. 例: #include &lt;algorithm&gt; #include &lt;function&gt; #include &lt;vector&gt;&lt;/vector&gt;&lt;/function&gt;&lt;/algorithm&gt; struct StrDescOrderInt { bool operator()(int a, int b) const { return a &amp;gt; b; } }; ... const int array[] = {10, 20, 5, 15, 0}; std::vector&lt;int&gt; vec(array, array + 5);&lt;/int&gt; std::function&lt;bool(int, int)=&quot;&quot;&gt; sorter = StrDescOrderInt();&lt;/bool(int,&gt; std::sort(vec.begin(), vec.end(), sorter);输出: 20 15 10 5 0 A real-life example: providing multiple sorting options 我们假设有一队足球运动员, 我们想让用户按他们自己的意愿去排列这些运动员. 有一个图表的UI, 上面有几个按钮, 每个按钮对应不用的排序规则. Plaer 类的代码: // -- Player.h -- #include &lt;string&gt;&lt;/string&gt; class Player { public: Player(const char * name, int caps, int goals); const std::string &amp;amp; GetName() const; int GetCaps() const; int GetGoals() const; private: std::string mName; int mCaps; int mGoals; };现在我们新写一个类或结构体来列出所有的比较函数. 比较函数是一个结构体并实现操作符(), 操作符() 带有两个参数, 分别为两个指向Player的指针, 返回bool值. class Player; struct PlayerSorting { // name struct SortPlayerByNameAsc (bool operator()(Player* p1, Player* p2) const;); struct SortPlayerByNameDes (bool operator()(Player* p1, Player* p2) const;); // caps struct SortPlayerByCapsAsc (bool operator()(Player* p1, Player* p2) const;); struct SortPlayerByCapsDes (bool operator()(Player* p1, Player* p2) const;); // goals struct SortPlayerByGoalsAsc (bool operator()(Player* p1, Player* p2) const;); struct SortPlayerByGoalsDes (bool operator()(Player* p1, Player* p2) const;); }然后, 在调用它的地方, 我们可以先把所有的std::function 存在一个std::vector 里, 使用的时候, 用索引访问vector的元素. std::vector&amp;lt; std::function&lt;bool(player *,=&quot;&quot; player=&quot;&quot; *)=&quot;&quot;&gt; &amp;gt; sorters; sorters.push_back(PlayerSorting::SortPlayerByNameAsc()); sorters.push_back(PlayerSorting::SortPlayerByCapsAsc()); sorters.push_back(PlayerSorting::SortPlayerByGoalsAsc()); sorters.push_back(PlayerSorting::SortPlayerByNameDes()); sorters.push_back(PlayerSorting::SortPlayerByCapsDes()); sorters.push_back(PlayerSorting::SortPlayerByGoalsDes());&lt;/bool(player&gt;例如, 根据得分降序排列: std::vector&lt;player *=&quot;&quot;&gt; players;&lt;/player&gt; // ...init players... std::sort(players.begin(), players.end(), sorters[5]);输出: NAME CAPS GOALS Lionel Messi 21 20 David Villa 13 16 Asamoah Gyan 22 15 Arjen Robben 11 12 Mesut Oezil 19 10 Diego Forlan 20 10 Andres Iniesta 15 9 Wesley Sneijder 24 6 Xavi 17 5 Bastian Schweinsteiger 23 4假如需要实现一种新的排序方式, 我们只需要在PlayerSorting类中添加一个新的仿函数即可. 原文地址: http://blog.davidecoppola.com/2015/01/cpp11-sort-using-function-objects/","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"翻译","slug":"翻译","permalink":"https://lxb.wiki/tags/翻译/"}]},{"title":"[译]String&#039;s interface","slug":"strings-interface","date":"2017-04-09T14:00:41.000Z","updated":"2020-05-10T03:30:27.341Z","comments":true,"path":"4c3249cd/","link":"","permalink":"https://lxb.wiki/4c3249cd/","excerpt":"","text":"考虑以下代码: bool fun(const string&amp;amp; code) { assert(code.length() &amp;gt;= 2); if (code.substr(0, 2) == string(&quot;XX&quot;)) { // ... } // ... }有没有发现什么问题? 不要纠结于assert(), 它只是为了保证 string “code” 长度大于2而已. 很显然, 这段代码用来检查string是否以”XX”开头. 基于它长度大于2 的前提, 这段代码能正常运行. 我们的关心的问题是, 表达式能否达到正确的结果. 绝大多数情况下, 我们之所以使用C++, 是希望能使我们的程序达到最优的性能. 基于这个目标, 上面的代码看起来就不是很正确了. 为了检查”code”是否以”XX”开头, 我们生成了两个临时的string, 每个string都可能潜在地申请堆上的内存. 有人可能会为此辩解: std::string应该能为一个 2字母的序列实现 短字符串最优化(SSO). 就算这个辩解是正确的, 这段代码也已经 耗费了 一些不能被优化掉的开销, 更何况, 并不是所有的都会实现SSO. 例如, 我使用的GCC 4.4.7 就不会为string实现SSO. 类模板std::basic_string 的接口很复杂. 它提供了大量的成员函数, 似乎不用它们显得不领情, 同时开发者也不会有自己一遍遍重新解析的冲动. 因为开发者模糊地记得应用于NTBS(null-terminated byte strings)(可以被隐式地转为const char* )的 操作符 == 会使结果出错, 所以他通过 确保参与比较的两个值都是std::string 类型来避开这个错误. 他可能在想, 在运行操作符== 前文本”XX” 已经被显式地转成了std::string, 那么这么做也没有坏处. 但是, 这是错误的, 因为对于操作符==, 标准提供了两种版本: bool operator==(const std::string&amp;amp; lhs, const char* rhs); bool operator==(const char* lhs, const std::string&amp;amp; rhs);当然实际上他们是带有多个参数的函数模板, 远比这个复杂. std::string 可以直接跟NTBS比较, 没有必要生成临时的std::string. 我们开头的例子, 可以通过去除显式生成的临时副本 进行优化: if (code.substr(0, 2) == &quot;XX&quot;) 更进一步, 不可否认, 在有些地方使用操作符== 看起来很高雅, 但是仅仅为了检查一个string 本身的一部分而去新申请一部分资源(生成一个新的string) 这种做法是错误的. 开发者的初衷, 并不是要是程序看起来高雅. 实际上, 如果我们深入研究std::basic_string 的官方文档, 就会发现, std::basic_string提供了一种比较它的子字符串和NTBS的方法: if(code.compare(0, 2, &quot;XX&quot;) == 0) 这个比较是三方比较, 结果等于0表示相等. 它可以达到目的, 并且不需要生成任何临时的string. 尽管这个compare() 使性能达到了很大的优化, 但我并不满足于此. 虽然它做了正确的事情, 但如果我们是第一次遇到他, 很难抓住他的精髓. 如果你可以使用boost库, 我的建议性的解决方案是使用Boost String Algorithms Library 中的算法: #include &lt;boost algorithm=&quot;&quot; string=&quot;&quot; predicate.hpp=&quot;&quot;&gt;&lt;/boost&gt; bool func(const string&amp;amp; code) { if (boost::algorithm::starts_with(code, &quot;XX&quot;)) }这段代码很好地体现了我想说的意思, 没有任何多余的开销. 原文地址: https://akrzemi1.wordpress.com/2015/04/15/strings-interface/","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"翻译","slug":"翻译","permalink":"https://lxb.wiki/tags/翻译/"}]},{"title":"[译]Custom comparison, equality and equivalence with the STL","slug":"Custom comparison, equality and equivalence with the STL","date":"2017-03-29T11:01:51.000Z","updated":"2020-05-10T03:34:16.770Z","comments":true,"path":"55f488ad/","link":"","permalink":"https://lxb.wiki/55f488ad/","excerpt":"","text":"从一段代码引用开始: std::vector&amp;lt; std::pair&lt;int, std::string=&quot;&quot;&gt; &amp;gt; v1 = ... // v1 is filled with data std::vector&amp;lt; std::pair&lt;int, std::string=&quot;&quot;&gt; &amp;gt; v2 = ... // v2 is filled with data std::vector&amp;lt; std::pair&lt;int, std::string=&quot;&quot;&gt; &amp;gt; results;&lt;/int,&gt;&lt;/int,&gt;&lt;/int,&gt; std::sort(v1.begin(), v1.end()); std::sort(v2.begin(), v2.end()); std::set_difference(v1.begin(), v1.end(), v2.begin(), v2.end(), std::back_inserter(result), compareFirst);我们在两个排好序的vector v1 和 v2上调用std::set_difference. std::set_difference 把结果写入 result, std::back_inserter 确保输出的结果从result 的后面添入. 自定义的compareFirst 作为比较函数提供给std::set_difference 默认地, std::set_difference 通过 std::pair 默认的比较函数来比较里面的元素(比较pair的first和second), 我们自定义了compareFirst, 希望只比较pair的first. compareFirst不是STL的函数, 需要我们自己实现. std::set_difference 使用的前提是input已经排好序, 倘若我们自定义比较函数C, 而通过C我们能把元素排好序, 那么我们使用这个C代替sort的默认排序也是可以的. 在此例中, 我们使用std::set_difference 只对pair的first进行排序, 尽管它们已经通过”first + second”的方式排序完了. 下面来实现compareFirst. 初版: bool compareFirst(const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp;amp; p1, const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp;amp; p2) { return p1.first == p2.first; // not final code, bug lurking here! }实际上, 上面的代码不会得到我们预期的结果. 为什么? 毕竟std::set_difference 会检查元素跟另一个容器的元素是否相等(equal), 不是吗?&lt;/int,&gt;&lt;/int,&gt; 为了理解上面的内容, 我们把STL大概地分为两类: 操作排序元素的 和操作乱序元素的. Comparing elements C++中描述”a is the same as b” 有两种方法 - the natural way: a == b. This is called equality. Equality is based on operator==. - the other way: a is not smaller than b and b is not smaller than a, so !(a&lt;b) &amp;&amp;=&quot;&quot; !(b&lt;a).=&quot;&quot; this=&quot;&quot; is=&quot;&quot; called=&quot;&quot; equivalence.=&quot;&quot; equivalence=&quot;&quot; based=&quot;&quot; on=&quot;&quot; operator&lt;.=&quot;&quot; 12345678910111213141516171819 对于基本类型如int, 甚至实践中大多数类型, `equivalence` 和`quality` 是相通的. 但是正如*Scott Meyers* 在&amp;lt;&amp;lt; Effective STL&amp;gt;&amp;gt; 一书条目19中指出的, 对于有一些类型, 即使&quot;并非罕见&quot;, `equivalence` 和 `equality` 是不同的, 如 大小写不敏感的string类型. &lt;u&gt;Why such a far-fetched way to express a simple thing?&lt;/u&gt; 当我们使用算法对容器内元素进行排序时, 很容易理解必须有独一无二的排序方法(如有多种排序方法, 会很笨重, 并可能产生不一致的结果). 所以对于一个特定的容器, 排序时, &quot;==&quot; 和&quot;&amp;lt;&quot; 只能选一个. 对于STL中排序的部分, 我们别无选择: 排序时必须使用&quot;&amp;lt;&quot;; 而乱序部分, 则没有这个约束, 我们可以使用&quot;==&quot;. **Implementing the comparator** STL的乱序部分使用&quot;==&quot;, 而排序部分使用&quot;&amp;lt;&quot;. 我们自定义的比较函数也必须遵循这种逻辑. 现在我们可以理解怎么自定义实现`std::set_difference` 的比较函数`compareFirst` 了. bool compareFirst(const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp; p1, const std::pair&lt;int, std::string=&quot;&quot;&gt;&amp; p2) &#123; return p1.first &lt; p2.first; // correct, STL-compatible code. &#125;原文地址: http://www.fluentcpp.com/2017/02/16/custom-comparison-equality-equivalence-stl/ &lt;/int,&gt;&lt;/int,&gt;","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"翻译","slug":"翻译","permalink":"https://lxb.wiki/tags/翻译/"},{"name":"STL","slug":"STL","permalink":"https://lxb.wiki/tags/STL/"}]},{"title":"[译]How to (std::)find something efficiently with the STL","slug":"how-to-stdfind-something-efficiently-with-the-stl","date":"2017-03-16T14:07:59.000Z","updated":"2020-05-10T03:35:16.545Z","comments":true,"path":"56dc57bb/","link":"","permalink":"https://lxb.wiki/56dc57bb/","excerpt":"","text":"本文分3部分: 1. 怎么使用STL进行高效的查找: 借用传统STL算法对元素进行范围搜索 2. 搜索STL容器: 当你有直接读取STL容器里元素的权限时, 怎么进行高效准确的搜索(与简单的范围搜索相比较) 3. STL搜索算法的秘密: 向公众展示不为人知的算法, 这些算法在已经学习过的人眼里确实是很有用的 STL根据查看方式的不同, 一共分为两种: 排序的和不排序的. * 排序集合的遍历, 通常需要对数时长, 而乱序集合的遍历, 需要线性时长 * 排序容器中比较元素大小的函数根据equivalence(comparing with &lt;), 而乱序容器中的函数根据equality(comparing with ==). 本文将展示对于在一个范围内搜索一个给定的值, C++怎么样去阐述下面3个问题: * 它存在否 * 它在哪 * 它应该在什么位置(排序容器) Is it there?乱序容器的元素这个问题可以用std::find来表达(需要和与范围的终点值的比较相结合): vector&lt;int&gt; v = ... // v filled with values if (std::find(v.begin(), v.end(), 42) != v.end()) { ...“Is it there”这个问题也可以用std::count来表达: vector&lt;int&gt; v = ... // v filled with values if (std::count(v.begin(), v.end(), 42)) { ...std::count()的返回值会被隐式地转换成if条件里的bool值: 如果该范围里有至少一个值为42, 则返回true. 与std::find相比, std::count的优劣: 优势: std::count避免了与范围的end值相比较 弊端: std::count遍历整个集合, 而std::find在第一个与要查找的值相等的位置停下 可以证明, 对于”想要查找某个值”这件事, std::find 表达得更明确 基于以上, std::find用得更多. Note 若要确认某个值存在而非是与要搜索的值相等, 请使用std::count_if, std::find_if, std::find_if_not 排序容器的元素使用的算法是std::binary_search, 此函数返回一个bool值, 此bool值表示在集合中是否存在与搜索的值相等的元素. std::set&lt;int&gt; numbers = // sorted elements bool is42InThere = std::binary_search(numbers.begin(), numbers.end(), 42); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 ### Where is it? (当确定了要搜索的值存在后,) 我们想更进一步, 得到指向那个元素的迭代器. #### 乱序容器的元素 使用std::find. 返回指向第一个与搜索的值相等的元素的迭代器, 如果找不到, 则返回集合的终点. std::vector numbers = ... auto searchResult = std::find(numbers.begin(), numbers.end(), 42);if (searchResult != numbers.end()) &#123; ... #### 排序容器的元素 对于排序集合, STL并没有像std::find一样直接的算法. std::find并不是为排序容器设计的, 因为它依据的是&quot;==&quot;而不是&quot;&amp;lt;&quot;, 消耗的时间为线性时长而不是对数时长. 对于一个给定的容器, 如果容器内元素的&quot;equality&quot;和&quot;equivalence&quot;是相同的, 且你能接受消耗的线性时长, 那么std::find会为你返回正确的结果, 你也能从它简单直接的接口中获益. **但是,** 不能忘记, std::find并不是为排序容器设计的. 这里推荐使用`std::equal_range`. (并非`std::lower_bound`) 函数原型: template&lt; class ForwardIt, class T &gt; std::pair&lt;forwardit,forwardit&gt; equal_range( ForwardIt first, ForwardIt last, const T&amp; value ); `std::equal_range` 返回与搜索值相等的元素的范围, 这个范围用一对集合内的迭代器表示. 这两个迭代器分别指向 与搜索值相等的范围里第一个元素和最后一个元素的下一个位置.&lt;/forwardit,forwardit&gt; 然而, 它的接口有些笨重: 例A: std::vector v = &#123;3, 7, 3, 11, 3, 3, 2&#125;; sort(v.begin(), v.end());// equal_range, attempt 1: natively clumsy std::pair&lt;std::vector::iterator, std::vector::iterator&gt; range1 = equal\\_range(v.begin(), v.end(), 3); std::for\\_each(range1.first, range1.second, doSomething); 用一个`typedef` 或者`using`让它更简洁: 例B: std::vector v = &#123;3, 7, 3, 11, 3, 3, 2&#125;; sort(v.begin(), v.end());&lt;/std::vectorusing IteratorPair = std::pair&lt;std::vector::iterator, std::vector::iterator&gt;;&lt;/std::vector// equal\\_range, attempt 2: with the classical typedef IteratorPair range2 = equal\\_range(v.begin(), v.end(), 3); std::for_each(range2.first, range2.second, doSomething); 例B确实简洁了很多, 但是仍有一个根本问题: 没有考虑 抽象等级. 尽管返回的是一个范围, 但这对迭代器强迫我们在操作返回的范围时必须按照&quot;第一&quot;&quot;第二&quot;这种方式来写代码. 范围就应该用&quot;首&quot;&quot;尾&quot;这种方式来表达. 这不仅给我们在其他地方使用这个返回值时造成很大的麻烦, 而且使代码很别扭. 为了解决这个问题, 我么可以把`std::equal_range` 返回的迭代器对封装进一个有&quot;范围&quot;这种语义的`object` templateclass Range&#123;public:Range(std::pair range)m\\_begin(range.first), m\\_end(range.second) &#123;&#125; typename Container::iterator begin() &#123; return m\\_begin; &#125; typename Container::iterator end() &#123; return m\\_end; &#125;private: typename Container::iterator m\\_begin; typename Container::iterator m\\_end; &#125;; 注意: 尽管`std::equal_range` 返回的结果是一个&quot;范围&quot;, 但是`std::begin` 和 `std::end` 不能用在这个结果上. 而上面的封装解决了这个问题. 可以像下面这样使用: std::vector v = &#123;3, 7, 3, 11, 3, 3, 2&#125;; sort(v.begin(), v.end());// equal_range, attempt 3: natural al last Range&lt;std::vector\\&gt; range3 = equal\\_range(v.begin(), v.end(), 3); std::for\\_each(range3.begin(), range3.end(), doSomething); 不管你使用上面的哪种方式, `std::equal_range` 都会返回一个范围, 要确定它是否为空, 可以通过检查那两个迭代器(是否相等)或者使用`std::distance` 检查它的大小. &lt;/std::vector&lt;int&gt; bool noElementFound = range3.begin() == range3.end(); size_t numberOfElementFound = std::distance(range3.begin(), range3.end())Where should it be?这个问题仅仅针对排序的范围, 因为对于乱序的范围, 某个元素可能会存在任何位置. 对于排序的范围, 这个问题可以简化为: 如果它存在, 那么它在哪儿? 如果它不存在, 那么它应该在哪儿? 这个问题可以用算法std::lower_bound 和std::upper_bound 来解释. 当你理解了std::equal_range 后, 上面这句话就很容易理解了: std::lower_bound 和std::upper_bound 都会返回 std::equal_range 返回的那个迭代器对的第一个和第二个迭代器. 要插入某个值x, 使用std::lower_bound 得到指向 在范围里与x相等的元素之前的位置的迭代器, 使用std::upper_bound 得到指向 在范围里与x相等的元素之后的位置的迭代器. 注意: 如果仅仅是搜索某个元素, 永远不要使用std::lower_bound 与std::find 相反, 你不能根据 判断std::lower_bound 返回的迭代器是否与终点的迭代器相等 来判断要搜索的值是否存在于这个集合. 事实上, 如果这个值在集合里不存在, 则std::lower_bound 返回它应该在的位置, 而不是终点的迭代器. 所以, 你不仅需要确认返回的迭代器不是终点的迭代器, 还要确认它指向的元素跟要搜索的值是相等的. 总结Question to express in C++ NOT SORTED SORTED Is it there? std::find != end std::binary_search Where is it? std::find std::equal_range Where should it be? - std::lower_bound / std::upper_bound 原文地址: http://www.fluentcpp.com/2017/01/16/how-to-stdfind-something-efficiently-with-the-stl/?hmsr=toutiao.io&amp;utm\\_medium=toutiao.io&amp;utm\\_source=toutiao.io","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"翻译","slug":"翻译","permalink":"https://lxb.wiki/tags/翻译/"},{"name":"STL","slug":"STL","permalink":"https://lxb.wiki/tags/STL/"}]},{"title":"[译]Effective STL 9","slug":"Effective STL 9","date":"2017-03-09T13:09:09.000Z","updated":"2020-05-10T03:31:39.332Z","comments":true,"path":"978f4b48/","link":"","permalink":"https://lxb.wiki/978f4b48/","excerpt":"","text":"条款9：在删除选项中仔细选择 假定你有一个标准STL容器，c，容纳int， Container&lt;int&gt; c; 而你想把c中所有值为1963的对象都去掉。令人吃惊的是，完成这项任务的方法因不同的容 器类型而不同：没有一种方法是通用的。 如果你有一个连续内存容器（vector、deque或string——参见条款1），最好的方法是erase-remove惯用法（参见条款32）： c.erase(remove(c.begin(), c.end(), 1963), // 当c是vector、string c.end()); // 或deque时， // erase-remove惯用法 // 是去除特定值的元素 // 的最佳方法这方法也适合于list，但是，正如条款44解释的，list的成员函数remove更高效： c.remove(1963); // 当c是list时， // remove成员函数是去除 // 特定值的元素的最佳方法当c是标准关联容器（即，set、multiset、map或multimap）时，使用任何叫做remove的东 西都是完全错误的。这样的容器没有叫做remove的成员函数，而且使用remove算法可能覆 盖容器值（参见条款32），潜在地破坏容器。（关于这样的破坏的细节，参考条款22，那 个条款也解释了为什么试图在map和multimap上使用remove肯定不能编译，而试图在set和 multiset上使用可能不能编译。） 不，对于关联容器，解决问题的适当方法是调用erase： c.erase(1963); // 当c是标准关联容器时 // erase成员函数是去除 // 特定值的元素的最佳方法这不仅是正确的，而且很高效，只花费对数时间。（序列容器的基于删除的技术需要线性 时间。）并且，关联容器的erase成员函数有基于等价而不是相等的优势，条款19解释了这 一区别的重要性。 让我们现在稍微修改一下这个问题。不是从c中除去每个有特定值的物体，让我们消除下面 判断式（参见条款39）返回真的每个对象： bool badValue(int x); // 返回x是否是“bad” 对于序列容器（vector、string、deque和list），我们要做的只是把每个remove()替换为remove_if()，然后就完成了： c.erase(remove_if(c.begin(), c.end(), badValue), // 当c是vector、string c.end()); // 或deque时这是去掉 // badValue返回真 // 的对象的最佳方法 c.remove_if(badValue); // 当c是list时这是去掉 // badValue返回真 // 的对象的最佳方法对于标准关联容器，它不是很直截了当。有两种方法处理该问题，一个更容易编码，另一 个更高效。“更容易但效率较低”的解决方案用remove_copy_if()把我们需要的值拷贝到一 个新容器中，然后把原容器的内容和新的交换： AssocContainer&lt;int&gt; c; // c现在是一种 ... // 标准关联容器 AssocContainer&lt;int&gt; goodValues; // 用于容纳不删除 // 的值的临时容器 remove_copy_if(c.begin(), c.end(), // 从c拷贝不删除 inserter(goodValues, // 的值到 goodValues.end()), // goodValues badValue); c.swap(goodValues); // 交换c和goodValues // 的内容这种方法的缺点是它拷贝了所有不删除的元素，而这样的拷贝开销可能大于我们期望的底线。 我们可以通过直接从原容器删除元素来避开拷贝的开销。不过，因为关联容器没有提供类似remove_if()的成员函数，所以我们必须写一个循环来迭代c中的元素，和原来一样删除元素. 看起来，这个任务很简单，而且实际上，代码也很简单。不幸的是，那些正确工作的代码 很少是跃出脑海的代码。例如，这是很多程序员首先想到的： AssocContainer&lt;int&gt; c; ... for (AssocContainer&lt;int&gt;::iterator i = c.begin(); // 清晰，直截了当 i!= c.end(); // 而漏洞百出的用于 ++i) { // 删除c中badValue返回真 if (badValue(*i)) c.erase(i); // 的每个元素的代码 } // 不要这么做！&lt;/int&gt;&lt;/int&gt;唉，这有未定义的行为。当容器的一个元素被删时，指向那个元素的所有迭代器都失效了 。当c.erase(i)返回时，i已经失效。那对于这个循环是个坏消息，因为在erase()返回后， i通过for循环的++i部分自增。 为了避免这个问题，我们必须保证在调用erase之前就得到了c中下一元素的迭代器。最容 易的方法是当我们调用时在i上使用后置递增： AssocContainer&lt;int&gt; c; ... for (AssocContainer&lt;int&gt;::iterator i = c.begin(); // for循环的第三部分 i != c.end(); // 是空的；i现在在下面 /*nothing*/ ){ // 自增 if (badValue(*i)) c.erase(i++); // 对于坏的值，把当前的 else ++i; // i传给erase，然后 } // 作为副作用增加i； // 对于好的值， // 只增加i这种调用erase()的解决方法可以工作，因为表达式i++的值是i的旧值，但作为副作用，i增 加了。因此，我们把i的旧值（没增加的）传给erase，但在erase开始执行前i已经自增了 。那正好是我们想要的。正如我所说的，代码很简单，只不过不是大多数程序员在第一次 尝试时想到的。 现在让我们进一步修改该问题。不仅删除badValue返回真的每个元素，而且每当一个元素 被删掉时，我们也想把一条消息写到日志文件中。 对于关联容器，这说多容易就有多容易，因为只需要对我们刚才开发的循环做一个微不足 道的修改就行了： ofstream logFile; // 要写入的日志文件 AssocContainer&lt;int&gt; c; ... for (AssocContainer&lt;int&gt;::iterator i = c.begin(); // 循环条件和前面一样 i !=c.end();){ if (badValue(*i)){ logFile &amp;lt;&amp;lt; &quot;Erasing &quot; &amp;lt;&amp;lt; *i &amp;lt;&amp;lt;&apos;\\n&apos;; // 写日志文件 c.erase(i++); // 删除元素 } else ++i; }现在是vector、string和deque给我们带来麻烦。我们不能再使用erase-remove惯用法，因为没有办法让erase()或remove()写日志文件。而且，我们不能使用刚刚为关联容器开发的循环, 因为它为vector、string和deque产生未定义的行为！要记得对于那样的容器，调用erase不仅使所有指向被删元素的迭代器失效，也使被删元素之后的所有迭代器失效。在我们的情况里，那包括所有i之后的迭代器。我们写i++，++i或你能想起的其它任何东西都没有用，因为没有能导致迭代器有效的。 我们必须对vector、string和deque采用不同的战略。特别是，我们必须利用erase()的返回值。那个返回值正是我们需要的：一旦删除完成，它就是指向紧接在被删元素之后的元素的有效迭代器。换句话说，我们这么写： for (SeqContainer&lt;int&gt;::iterator i = c.begin(); i != c.end();){ if (badValue(*i)){ logFile &amp;lt;&amp;lt; &quot;Erasing &quot; &amp;lt;&amp;lt; *i &amp;lt;&amp;lt; &apos;\\n&apos;; i = c.erase(i); // 通过把erase的返回值 } // 赋给i来保持i有效 else ++i; }这可以很好地工作，但只用于标准序列容器。由于论证一个可能的问题（条款5做了），标准关联容器的erase()的返回类型是void[1]。对于那些容器，你必须使用“后置递增你要传给erase()的迭代器”技术。（顺便说说，在为序列容器编码和为关联容器编码之间的这种差别是为什么写容器无关代码一般缺乏考虑的一个例子——参见条款2。) 为了避免你奇怪list的适当方法是什么，事实表明对于迭代和删除，你可以像vector/str ing/deque一样或像关联容器一样对待list；两种方法都可以为list工作。 如果我们观察在本条款中提到的所有东西，我们得出下列结论： 去除一个容器中有特定值的所有对象： 如果容器是vector、string或deque，使用erase-remove惯用法。 如果容器是list，使用list::remove。 如果容器是标准关联容器，使用它的erase成员函数。 去除一个容器中满足一个特定判定式的所有对象： 如果容器是vector、string或deque，使用erase-remove_if惯用法。 如果容器是list，使用list::remove_if。 如果容器是标准关联容器，使用remove_copy_if和swap，或写一个循环来遍历容器元素， 当你把迭代器传给erase时记得后置递增它。 在循环内做某些事情（除了删除对象之外）： 如果容器是标准序列容器，写一个循环来遍历容器元素，每当调用erase时记得都用它的返回值更新你的迭代器。 如果容器是标准关联容器，写一个循环来遍历容器元素，当你把迭代器传给erase时记得后置递增它。 如你所见，与仅仅调用erase相比，有效地删除容器元素有更多的东西。解决问题的最好方法取决于你是怎样鉴别出哪个对象是要被去掉的，储存它们的容器的类型，和当你删除它们的时候你还想要做什么（如果有的话）。只要你小心而且注意了本条款的建议，你将毫不费力。如果你不小心，你将冒着产生不必要低效的代码或未定义行为的危险。 ------------------------------------------------------------------------------[1] 这仅对带有迭代器实参的erase()形式是正确的。关联容器也提供一个带有一个值的实参 的erase()形式，而那种形式返回被删掉的元素个数。但这里，我们只关心通过迭代器删除东 西。 参考地址","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"翻译","slug":"翻译","permalink":"https://lxb.wiki/tags/翻译/"},{"name":"STL","slug":"STL","permalink":"https://lxb.wiki/tags/STL/"}]},{"title":"STL 的 erase( ) 陷阱-迭代器失效总结","slug":"STL 的 erase( ) 陷阱-迭代器失效总结","date":"2017-03-02T15:10:19.000Z","updated":"2020-05-10T03:31:51.717Z","comments":true,"path":"b12cd95a/","link":"","permalink":"https://lxb.wiki/b12cd95a/","excerpt":"","text":"STL中的容器按存储方式分为两类，一类是按以数组形式存储的容器（如：vector 、deque)；另一类是以不连续的节点形式存储的容器（如：list、set、map）。在使用erase方法来删除元素时，需要注意一些问题。 1.list,set,map容器在使用 list、set 或 map遍历删除某些元素时可以这样使用： 1.1 正确写法 1 std::list&lt;int&gt; list; std::list&lt;int&gt;::iterator it_list; for (it_list = list.begin(); it_list != list.end();) { if (willDelete(*it_list)) { it_list = list.erase(it_list); } else { ++it_list; } }Note: 以上方法仅适用于standard sequence container, 因为对于standard associative container, erase()的返回类型为void. (查阅Effective STL Item 9)以下为原文: This works wonderfully, but only for the standard sequence containers. Due to reasoning one might question, erase()&apos;s return type for the standard associative containers is void. For those containers, you have to use the postincrement-the-iterator-you-pass-to-erase technique.1.2 正确写法2 查阅原版Effctive STL Item 9, 证实, 下面这种写法不能用于标准序列容器, 而适用于标准关联容器, 而List也可以使用这种方法. std::list&lt;int&gt; list; std::list&lt;int&gt;::iterator it_list; for (it_list = list.begin(); it_list != list.end();) { if (willDelete(*it_list)) { list.erase(it_list++); // 必须使用后缀自增, 不能使用前缀自增 } else { ++it_list; } } 123456789101112131415161718192021222324252627282930 **1.3 错误写法 1** std::list&lt; int&gt; List; std::list&lt; int&gt;::iterator itList; for( itList = List.begin(); itList != List.end(); itList++) &#123; if( WillDelete( *itList) ) &#123; List.erase( itList); &#125; &#125; **1.4 错误写法 2** std::list&lt; int&gt; List; std::list&lt; int&gt;::iterator itList; for( itList = List.begin(); itList != List.end(); ) &#123; if( WillDelete( *itList) ) &#123; itList = List.erase( ++itList); &#125; else itList++; &#125; **1.5 分析** 正确方法1: 通过erase()方法的返回值来获取下一个元素的位置; 正确方法2: 在调用erase()方法之前先使用&quot;++&quot; 来获取下一个元素的位置; 错误使用方法1: 在调用erase()方法之后使用&quot;++&quot; 来获取下一个元素的位置, 由于在调用erase()方法之后, 该元素的位置已经被删除, 如果再根据这个旧的位置来获取下一个位置, 则会出现异常; 错误使用方法2: 同上 ####**2. vector,deque 容器** 在使用 vector、deque遍历删除元素时，也可以通过erase的返回值来获取下一个元素的位置： **2.1 正确写法:** std::vector vec; std::vector::iterator it\\_vec; for (it\\_vec = vec.begin(); it\\_vec != vec.end();) &#123; if (willDelete(*it\\_vec)) &#123; it\\_vec = vec.erase(it\\_vec); &#125; else &#123; ++it_vec; &#125; &#125;2.2 注意 vector, deque 不能像上面的”正确方法2” 的办法来遍历删除. 原因请参考Effective STL条款9。摘录到下面： 1) 对于关联容器(如map, set, multimap, multiset)，删除当前的iterator，仅仅会使当前的iterator失效，只要在erase时，递增当前iterator即可。这是因为map之类的容器，使用了红黑树来实现，插入、删除一个结点不会对其他结点造成影响。 for (iter = cont.begin(); it != cont.end();) { (*iter)-&amp;gt;doSomething(); if (shouldDelete(*iter)) cont.erase(iter++); else ++iter; }因为iter传给erase方法的是一个副本，iter++会指向下一个元素。 2) 对于序列式容器(如vector, deque)，删除当前的iterator会使后面所有元素的iterator都失效。这是因为vetor, deque使用了连续分配的内存，删除一个元素导致后面所有的元素会向前移动一个位置。还好erase()方法可以返回下一个有效的iterator。 for (iter = cont.begin(); iter != cont.end();) { (*it)-&amp;gt;doSomething(); if (shouldDelete(*iter)) iter = cont.erase(iter); else ++iter; }3)对于list来说，它使用了不连续分配的内存，并且它的erase()方法也会返回下一个有效的iterator，因此上面两种方法都可以使用。 3. 其他set 键和值相等。 键唯一。 元素默认按升序排列。 如果迭代器所指向的元素被删除，则该迭代器失效。其它任何增加、删除元素的操作都不会使迭代器失效 map 键唯一。 元素默认按键的升序排列。 如果迭代器所指向的元素被删除，则该迭代器失效。其它任何增加、删除元素的操作都不会使迭代器失效。 作成参考地址","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"STL","slug":"STL","permalink":"https://lxb.wiki/tags/STL/"}]},{"title":"用GDB调试程序","slug":"用GDB调试程序","date":"2017-02-23T11:12:02.000Z","updated":"2020-05-05T11:37:40.347Z","comments":true,"path":"13b68d49/","link":"","permalink":"https://lxb.wiki/13b68d49/","excerpt":"","text":"使用GDB一般来说GDB主要调试的是C/C++的程序。要调试C/C++的程序，首先在编译时，我们必须要把调试信息加到可执行文件中。使用编译器（cc/gcc/g++）的 -g 参数可以做到这一点。如： $gcc -g -Wall hello.c -o hello $g++ -g -Wall hello.cpp -o hello如果没有-g，你将看不见程序的函数名、变量名，所代替的全是运行时的内存地址。当你用-g把调试信息加入之后，并成功编译目标代码以后，让我们来看看如何用gdb来调试他。 启动GDB的方法有以下几种： gdb &lt;program&gt; program也就是你的执行文件，一般在当前目录下。 gdb &lt;program&gt; core 用gdb同时调试一个运行程序和core文件，core是程序非法执行后core dump后产生的文件。 gdb &lt;program&gt; &lt;pid&gt; 如果你的程序是一个服务程序，那么你可以指定这个服务程序运行时的进程ID。gdb会自动attach上去，并调试他。program应该在PATH环境变量中搜索得到。 以上三种都是进入gdb环境和加载被调试程序同时进行的。也可以先进入gdb环境，在加载被调试程序，方法如下： *在终端输入：gdb *在gdb环境中：file &lt;program&gt; 这两步等价于：gdb &lt;program&gt;GDB启动时，可以加上一些GDB的启动开关，详细的开关可以用gdb -help查看。我在下面只例举一些比较常用的参数： -symbols &lt;file&gt; -s &lt;file&gt; 从指定文件中读取符号表。 -se file 从指定文件中读取符号表信息，并把他用在可执行文件中。 -core &lt;file&gt; -c &lt;file&gt; 调试时core dump的core文件。 -directory &lt;directory&gt; -d &lt;directory&gt; 加入一个源文件的搜索路径。默认搜索路径是环境变量中PATH所定义的路径。 ```&lt;/directory&gt;&lt;/directory&gt;&lt;/file&gt;&lt;/file&gt;&lt;/file&gt;&lt;/file&gt;&lt;/program&gt;&lt;/program&gt;&lt;/pid&gt;&lt;/program&gt;&lt;/program&gt;&lt;/program&gt; ###GDB的命令概貌 启动gdb后，你就被带入gdb的调试环境中，就可以使用gdb的命令开始调试程序了，gdb的命令可以使用help命令来查看，如下所示： ```bash $ gdb GNU gdb 6.7.1-debian Copyright (C) 2007 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http: gnu.org=&quot;&quot; licenses=&quot;&quot; gpl.html=&quot;&quot;&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot; and &quot;show warranty&quot; for details. This GDB was configured as &quot;i486-linux-gnu&quot;. (gdb) help List of classes of commands:&lt;/http:&gt; aliases -- Aliases of other commands breakpoints -- Making program stop at certain points data -- Examining data files -- Specifying and examining files internals -- Maintenance commands obscure -- Obscure features running -- Running the program stack -- Examining the stack status -- Status inquiries support -- Support facilities tracepoints -- Tracing of program execution without stopping the program user-defined -- User-defined commands Type &quot;help&quot; followed by a class name for a list of commands in that class. Type &quot;help all&quot; for the list of all commands. Type &quot;help&quot; followed by command name for full documentation. Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;. Command name abbreviations are allowed if unambiguous. (gdb)gdb 的命令很多，gdb把之分成许多个种类。help命令只是例出gdb的命令种类，如果要看种类中的命令，可以使用help &lt;class&gt;命令，如：help breakpoints，查看设置断点的所有命令。也可以直接help &lt;command&gt;&lt;/command&gt;来查看命令的帮助。 gdb中，输入命令时，可以不用打全命令，只用打命令的前几个字符就可以了，当然，命令的前几个字符应该要标志着一个唯一的命令，在Linux下，你可以敲击两次TAB键来补齐命令的全称，如果有重复的，那么gdb会把其列出来。 示例一：在进入函数func时，设置一个断点。可以敲入break func，或是直接就是b func (gdb) b func Breakpoint 1 at 0x804837a: file tst.c, line 5.示例二：敲入b按两次TAB键，你会看到所有b打头的命令： (gdb) b backtrace break bt (gdb)示例三：只记得函数的前缀，可以这样： (gdb) b make_ &amp;lt;按TAB键&amp;gt; （再按下一次TAB键，你会看到:） make_a_section_from_file make_environ make_abs_section make_function_type make_blockvector make_pointer_type make_cleanup make_reference_type make_command make_symbol_completion_list (gdb) b make_ GDB把所有make开头的函数全部列出来给你查看。示例四：调试C++的程序时，有可以函数名一样。如： (gdb) b &apos;bubble( M-? bubble(double,double) bubble(int,int) (gdb) b &apos;bubble(你可以查看到C++中的所有的重载函数及参数。（注：M-?和“按两次TAB键”是一个意思） 要退出gdb时，只用发quit或命令简称q就行了 GDB中运行UNIX的shell程序在gdb环境中，你可以执行UNIX的shell的命令，使用gdb的shell命令来完成： shell &lt;command string=&quot;&quot;&gt;&lt;/command&gt; 调用UNIX的shell来执行&lt;command string=&quot;&quot;&gt;&lt;/command&gt;，环境变量SHELL中定义的UNIX的shell将会被用来执行&lt;command string=&quot;&quot;&gt;&lt;/command&gt;，如果SHELL没有定义，那就使用UNIX的标准shell：/bin/sh。（在Windows中使用Command.com或cmd.exe） 还有一个gdb命令是make： make &lt;make-args&gt; 可以在gdb中执行make命令来重新build自己的程序。这个命令等价于shell make &lt;make-args&gt;。 在GDB中运行程序当以gdb &lt;program&gt;方式启动gdb后，gdb会在PATH路径和当前目录中搜索&lt;program&gt;的源文件。如要确认gdb是否读到源文件，可使用l或list命令，看看gdb是否能列出源代码。 在gdb中，运行程序使用r或是run命令。程序的运行，你有可能需要设置下面四方面的事。 1、程序运行参数。 set args 可指定运行时参数。（如：set args 10 20 30 40 50） show args 命令可以查看设置好的运行参数。 2、运行环境。 `path 可设定程序的运行路径。 show paths 查看程序的运行路径。 set environment varname [=value] 设置环境变量。如：set env USER=hchen show environment [varname] 查看环境变量。 **3、工作目录。**cd ` 相当于shell的cd命令。 pwd 显示当前的所在目录。 4、程序的输入输出。 info terminal 显示你程序用到的终端的模式。 使用重定向控制程序输出。如：run &gt; outfile tty命令可以指写输入输出的终端设备。如：tty /dev/ttyb 调试已运行的程序两种方法： 1. 在UNIX下用ps查看正在运行的程序的PID（进程ID），然后用gdb &lt;program&gt; PID格式挂接正在运行的程序。 2. 先用gdb &lt;program&gt;关联上源代码，并进行gdb，在gdb中用attach命令来挂接进程的PID。并用detach来取消挂接的进程。 暂停/恢复程序运行调试程序中，暂停程序运行是必须的，GDB可以方便地暂停程序的运行。你可以设置程序的在哪行停住，在什么条件下停住，在收到什么信号时停往等等。以便于你查看运行时的变量，以及运行时的流程。 当进程被gdb停住时，你可以使用info program 来查看程序的是否在运行，进程号，被暂停的原因。 在gdb中，我们可以有以下几种暂停方式：断点（BreakPoint）、观察点（Watch Point）、捕捉点（Catch Point）、信号（Signals）、线程停止（Thread Stops）。如果要恢复程序运行，可以使用c或是 continue命令。 下面为重要的使用步骤, 只摘抄了部分必要的信息, 如设置断点, 查看栈信息, 其余操作, 可以在wiki.ubuntu查看 设置断点（Break Points） 我们用break命令来设置断点。下面有几点设置断点的方法： break &lt;function&gt; 在进入指定函数时停住。C++中可以使用class::function或function(type,type)格式来指定函数名。 break &lt;linenum&gt; 在指定行号停住。 break +offset break -offset 在当前行号的前面或后面的offset行停住。offiset为自然数。 break filename：linenum 在源文件filename的linenum行处停住。 break filename：function 在源文件filename的function函数的入口处停住。 break *address 在程序运行的内存地址处停住。 break break命令没有参数时，表示在下一条指令处停住。 break ... if &lt;condition&gt; …可以是上述的参数，condition表示条件，在条件成立时停住。比如在循环体中，可以设置break if i==100，表示当i为100时停住程序。 查看断点时，可使用info命令，如下所示：（注：n表示断点号） info breakpoints [n] info break [n] 维护停止点 上面说了如何设置程序的停止点，GDB中的停止点也就是上述的三类。在GDB中，如果你觉得已定义好的停止点没有用了，你可以使用delete、clear、disable、enable这几个命令来进行维护。 clear 清除所有的已定义的停止点。 clear &lt;function&gt; clear &lt;filename：function&gt; 清除所有设置在函数上的停止点。 clear &lt;linenum&gt; clear &lt;filename：linenum&gt; 清除所有设置在指定行上的停止点。 delete [breakpoints] [range...] 删除指定的断点，breakpoints为断点号。如果不指定断点号，则表示删除所有的断点。range 表示断点号的范围（如：3-7）。其简写命令为d。&lt;/filename：linenum&gt;&lt;/filename：function&gt; 比删除更好的一种方法是disable停止点，disable了的停止点，GDB不会删除，当你还需要时，enable即可，就好像回收站一样。 disable [breakpoints] [range...] disable所指定的停止点，breakpoints为停止点号。如果什么都不指定，表示disable所有的停止点。简写命令是dis. enable [breakpoints] [range...] enable所指定的停止点，breakpoints为停止点号。 enable [breakpoints] once range... enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动disable。 enable [breakpoints] delete range... enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动删除。 恢复程序运行和单步调试 当程序被停住了，你可以用continue命令恢复程序的运行直到程序结束，或下一个断点到来。也可以使用step或next命令单步跟踪程序。 continue [ignore-count] c [ignore-count] fg [ignore-count] 恢复程序运行，直到程序结束，或是下一个断点到来。ignore-count表示忽略其后的断点次数。continue，c，fg三个命令都是一样的意思。 step &lt;count&gt; 单步跟踪，如果有函数调用，他会进入该函数。进入函数的前提是，此函数被编译有debug信息。很像VC等工具中的step in。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。 next &lt;count&gt; 同样单步跟踪，如果有函数调用，他不会进入该函数。很像VC等工具中的step over。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。 set step-mode set step-mode on 打开step-mode模式，于是，在进行单步跟踪时，程序不会因为没有debug信息而不停住。这个参数很有利于查看机器码。 set step-mode off 关闭step-mode模式。 finish 运行程序，直到当前函数完成返回。并打印函数返回时的堆栈地址和返回值及参数值等信息。 until 或 u 当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。 stepi 或 si nexti 或 ni 单步跟踪一条机器指令！一条程序代码有可能由数条机器指令完成，stepi和nexti可以单步执行机器指令。与之一样有相同功能的命令是“display/i $pc” ，当运行完这个命令后，单步跟踪会在打出程序代码的同时打出机器指令（也就是汇编代码） 查看栈信息 当程序被停住了，你需要做的第一件事就是查看程序是在哪里停住的。当你的程序调用了一个函数，函数的地址，函数参数，函数内的局部变量都会被压入“栈”（Stack）中。你可以用GDB命令来查看当前的栈中的信息。 下面是一些查看函数调用栈信息的GDB命令： backtrace bt 打印当前的函数调用栈的所有信息。如： (gdb) bt #0 func (n=250) at tst.c:6 #1 0x08048524 in main (argc=1, argv=0xbffff674) at tst.c:30 #2 0x400409ed in __libc_start_main () from /lib/libc.so.6从上可以看出函数的调用栈信息：__libc_start_main --&amp;gt; main() --&amp;gt; func() backtrace &lt;n&gt; bt &lt;n&gt; n是一个正整数，表示只打印栈顶上n层的栈信息。 backtrace &amp;lt;-n&amp;gt; bt &amp;lt;-n&amp;gt; -n表一个负整数，表示只打印栈底下n层的栈信息。 如果你要查看某一层的信息，你需要切换当前栈，一般来说，程序停止时，最顶层的栈就是当前栈，如果你要查看栈下面层的详细信息，首先要做的是切换当前栈。 frame &lt;n&gt; f &lt;n&gt; n是一个从0开始的整数，是栈中的层编号。比如：frame 0，表示栈顶，frame 1，表示栈的第二层。 up &lt;n&gt; 表示向栈的上面移动n层，可以不打n，表示向上移动一层。 down &lt;n&gt; 表示向栈的下面移动n层，可以不打n，表示向下移动一层。 上面的命令，都会打印出移动到的栈层的信息。如果你不想让其打出信息。你可以使用这三个命令： select-frame &lt;n&gt; 对应于 frame 命令。 up-silently &lt;n&gt;对应于 up 命令。 down-silently &lt;n&gt; 对应于 down 命令。 查看当前栈层的信息，你可以用以下GDB命令： frame 或 f 会打印出这些信息：栈的层编号，当前的函数名，函数参数值，函数所在文件及行号，函数执行到的语句。 info frame info f 这个命令会打印出更为详细的当前栈层的信息，只不过，大多数都是运行时的内存地址。比如：函数地址，调用函数的地址，被调用函数的地址，目前的函数是由什么样的程序语言写成的、函数参数地址及值、局部变量的地址等等。如： bash (gdb) info f Stack level 0, frame at 0xbffff5d4: eip = 0x804845d in func (tst.c:6); saved eip 0x8048524 called by frame at 0xbffff60c source language c. Arglist at 0xbffff5d4, args: n=250 Locals at 0xbffff5d4, Previous frame&#39;s sp is 0x0 Saved registers: ebp at 0xbffff5d4, eip at 0xbffff5d8 info args 打印出当前函数的参数名及其值。 info locals 打印出当前函数中所有局部变量及其值。 info catch 打印出当前的函数中的异常处理信息。","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://lxb.wiki/tags/工具/"}]},{"title":"编译器工作过程","slug":"编译器工作过程","date":"2017-02-09T14:13:33.000Z","updated":"2019-10-03T16:49:14.891Z","comments":true,"path":"49fab9fa/","link":"","permalink":"https://lxb.wiki/49fab9fa/","excerpt":"","text":"代码要运行，必须先转成二进制的机器码。这是编译器的任务。 比如，下面这段源码（假定文件名叫做test.c）。 #include &lt;stdio.h&gt; int main(void) { fputs(&quot;Hello, world!\\n&quot;, stdout); return 0; } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980 要先用编译器处理一下，才能运行。 ```bash $ gcc test.c $ ./a.out Hello, world! 对于复杂的项目，编译过程还必须分成三步。 $ ./configure $ make $ make install 这些命令到底在干什么？大多数的书籍和资料，都语焉不详，只说这样就可以编译了，没有进一步的解释。本文将介绍编译器的工作过程，也就是上面这三个命令各自的任务。我主要参考了Alex Smith的文章《Building C Projects》。需要声明的是，本文主要针对gcc编译器，也就是针对C和C++，不一定适用于其他语言的编译。![这里写图片描述](http://img.blog.csdn.net/20170105233325494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbHhid29sZg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)#### 第一步 配置（configure）编译器在开始工作之前，需要知道当前的系统环境，比如标准库在哪里、软件的安装位置在哪里、需要安装哪些组件等等。这是因为不同计算机的系统环境不一样，通过指定编译参数，编译器就可以灵活适应环境，编译出各种环境都能运行的机器码。这个确定编译参数的步骤，就叫做”配置”（configure）。这些配置信息保存在一个配置文件之中，约定俗成是一个叫做configure的脚本文件。通常它是由autoconf工具生成的。编译器通过运行这个脚本，获知编译参数。configure脚本已经尽量考虑到不同系统的差异，并且对各种编译参数给出了默认值。如果用户的系统环境比较特别，或者有一些特定的需求，就需要手动向configure脚本提供编译参数。`$ ./configure --prefix=/www --with-mysql`上面代码是php源码的一种编译配置，用户指定安装后的文件保存在www目录，并且编译时加入mysql模块的支持。#### 第二步 确定标准库和头文件的位置源码肯定会用到标准库函数（standard library）和头文件（header）。它们可以存放在系统的任意目录中，编译器实际上没办法自动检测它们的位置，只有通过配置文件才能知道。编译的第二步，就是从配置文件中知道标准库和头文件的位置。一般来说，配置文件会给出一个清单，列出几个具体的目录。等到编译时，编译器就按顺序到这几个目录中，寻找目标。#### 第三步 确定依赖关系对于大型项目来说，源码文件之间往往存在依赖关系，编译器需要确定编译的先后顺序。假定A文件依赖于B文件，编译器应该保证做到下面两点。（1）只有在B文件编译完成后，才开始编译A文件。 （2）当B文件发生变化时，A文件会被重新编译。编译顺序保存在一个叫做makefile的文件中，里面列出哪个文件先编译，哪个文件后编译。而makefile文件由configure脚本运行生成，这就是为什么编译时configure必须首先运行的原因。在确定依赖关系的同时，编译器也确定了，编译时会用到哪些头文件。#### 第四步 头文件的预编译（precompilation）不同的源码文件，可能引用同一个头文件（比如stdio.h）。编译的时候，头文件也必须一起编译。为了节省时间，编译器会在编译源码之前，先编译头文件。这保证了头文件只需编译一次，不必每次用到的时候，都重新编译了。不过，并不是头文件的所有内容，都会被预编译。用来声明宏的#define命令，就不会被预编译。#### 第五步 预处理（Preprocessing）预编译完成后，编译器就开始替换掉源码中bash的头文件和宏。以本文开头的那段源码为例，它包含头文件stdio.h，替换后的样子如下。 extern int fputs(const char *, FILE *); extern FILE *stdout; int main(void) &#123; fputs(&quot;Hello, world!\\n&quot;, stdout); return 0; &#125; 为了便于阅读，上面代码只截取了头文件中与源码相关的那部分，即fputs和FILE的声明，省略了stdio.h的其他部分（因为它们非常长）。另外，上面代码的头文件没有经过预编译，而实际上，插入源码的是预编译后的结果。编译器在这一步还会移除注释。这一步称为”预处理”（Preprocessing），因为完成之后，就要开始真正的处理了。#### 第六步 编译（Compilation）预处理之后，编译器就开始生成机器码。对于某些编译器来说，还存在一个中间步骤，会先把源码转为汇编码（assembly），然后再把汇编码转为机器码。下面是本文开头的那段源码转成的汇编码。```` .file &quot;test.c&quot; .section .rodata .LC0: .string &quot;Hello, world!\\\\n&quot; .text .globl main .type main, @function main: .LFB0: .cfi\\_startproc pushq %rbp .cfi\\_def\\_cfa\\_offset 16 .cfi\\_offset 6, -16 movq %rsp, %rbp .cfi\\_def\\_cfa\\_register 6 movq stdout(%rip), %rax movq %rax, %rcx movl $14, %edx movl $1, %esi movl $.LC0, %edi call fwrite movl $0, %eax popq %rbp .cfi\\_def\\_cfa 7, 8 ret .cfi_endproc .LFE0: .size main, .-main .ident &quot;GCC: (Debian 4.9.1-19) 4.9.1&quot; .section .note.GNU-stack,&quot;&quot;,@progbits这种转码后的文件称为对象文件（object file）。 第七步 连接（Linking）对象文件还不能运行，必须进一步转成可执行文件。如果你仔细看上一步的转码结果，会发现其中引用了stdout函数和fwrite函数。也就是说，程序要正常运行，除了上面的代码以外，还必须有stdout和fwrite这两个函数的代码，它们是由C语言的标准库提供的。 编译器的下一步工作，就是把外部函数的代码（通常是后缀名为.lib和.a的文件），添加到可执行文件中。这就叫做连接（linking）。这种通过拷贝，将外部函数库添加到可执行文件的方式，叫做静态连接（static linking），后文会提到还有动态连接（dynamic linking）。 make命令的作用，就是从第四步头文件预编译开始，一直到做完这一步。 第八步 安装（Installation）上一步的连接是在内存中进行的，即编译器在内存中生成了可执行文件。下一步，必须将可执行文件保存到用户事先指定的安装目录。 表面上，这一步很简单，就是将可执行文件（连带相关的数据文件）拷贝过去就行了。但是实际上，这一步还必须完成创建目录、保存文件、设置权限等步骤。这整个的保存过程就称为”安装”（Installation）。 第九步 操作系统连接可执行文件安装后，必须以某种方式通知操作系统，让其知道可以使用这个程序了。比如，我们安装了一个文本阅读程序，往往希望双击txt文件，该程序就会自动运行。 这就要求在操作系统中，登记这个程序的元数据：文件名、文件描述、关联后缀名等等。Linux系统中，这些信息通常保存在/usr/share/applications目录下的.desktop文件中。另外，在Windows操作系统中，还需要在Start启动菜单中，建立一个快捷方式。 这些事情就叫做”操作系统连接”。make install命令，就用来完成”安装”和”操作系统连接”这两步。 第十步 生成安装包写到这里，源码编译的整个过程就基本完成了。但是只有很少一部分用户，愿意耐着性子，从头到尾做一遍这个过程。事实上，如果你只有源码可以交给用户，他们会认定你是一个不友好的家伙。大部分用户要的是一个二进制的可执行程序，立刻就能运行。这就要求开发者，将上一步生成的可执行文件，做成可以分发的安装包。 所以，编译器还必须有生成安装包的功能。通常是将可执行文件（连带相关的数据文件），以某种目录结构，保存成压缩文件包，交给用户。 第十一步 动态连接（Dynamic linking）正常情况下，到这一步，程序已经可以运行了。至于运行期间（runtime）发生的事情，与编译器一概无关。但是，开发者可以在编译阶段选择可执行文件连接外部函数库的方式，到底是静态连接（编译时连接），还是动态连接（运行时连接）。所以，最后还要提一下，什么叫做动态连接。 前面已经说过，静态连接就是把外部函数库，拷贝到可执行文件中。这样做的好处是，适用范围比较广，不用担心用户机器缺少某个库文件；缺点是安装包会比较大，而且多个应用程序之间，无法共享库文件。动态连接的做法正好相反，外部函数库不进入安装包，只在运行时动态引用。好处是安装包会比较小，多个应用程序可以共享库文件；缺点是用户必须事先安装好库文件，而且版本和安装位置都必须符合要求，否则就不能正常运行。 现实中，大部分软件采用动态连接，共享库文件。这种动态共享的库文件，Linux平台是后缀名为.so的文件，Windows平台是.dll文件，Mac平台是.dylib文件。","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"编译器","slug":"编译器","permalink":"https://lxb.wiki/tags/编译器/"}]},{"title":"unordered_map笔记","slug":"unordered_map笔记","date":"2017-01-12T12:05:01.000Z","updated":"2019-10-03T17:03:03.760Z","comments":true,"path":"d97ca7/","link":"","permalink":"https://lxb.wiki/d97ca7/","excerpt":"","text":"unordered_map与map的区别 boost::unordered_map， 它与 stl::map的区别就是，stl::map是按照operator&lt;比较判断元素是否相同，以及比较元素的大小，然后选择合适的位置插入到树中。所以，如果对map进行遍历（中序遍历）的话，输出的结果是有序的。顺序就是按照operator&lt; 定义的大小排序。 而boost::unordered_map是计算元素的Hash值，根据Hash值判断元素是否相同。所以，对unordered_map进行遍历，结果是无序的。 用法的区别就是，stl::map 的key需要定义operator&lt; 。 而boost::unordered_map需要定义hash_value函数并且重载operator==。对于内置类型，如string，这些都不用操心。对于自定义的类型做key，就需要自己重载operator== 或者hash_value()了。 最后，说，当不需要结果排好序时，最好用unordered_map。 linux下使用 普通的key就不说了和map一样 看一下用sockaddr_in 作为key的方法 #ifndef CSESSION_H #define CSESSION_H #include &lt;netinet in.h=&quot;&quot;&gt; #include &lt;time.h&gt; #include &lt;/time.h&gt;&lt;/netinet&gt; &lt;map&gt; #include &lt;string.h&gt; #include &lt;tr1 unordered_map=&quot;&quot;&gt; //头文件 #include &lt;iostream&gt; using namespace std; using namespace std::tr1; struct Terminal { int nid ; //id the key for terminal sockaddr_in addr; //ip the key for Client time_t tm; //last alive time enTerminalStat enStat;//status Terminal(); ~Terminal(); Terminal &amp;amp;operator =(const Terminal&amp;amp; term); }; struct hash_func //hash 函数 { size_t operator()(const sockaddr_in &amp;amp;addr) const { return addr.sin_port*9999 + addr.sin_addr.s_addr; } }; struct cmp_fun //比较函数 == { bool operator()(const sockaddr_in &amp;amp;addr1, const sockaddr_in &amp;amp;addr2) const { return memcmp(&amp;amp;addr1, &amp;amp;addr2, sizeof(sockaddr_in)) == 0 ? true:false; } }; //typedef unordered_map&lt;int,terminal*&gt; MapTerminal; // Terminal socket 作为key //typedef unordered_map&lt;int,terminal*&gt;::iterator MapTerminal_It; // &lt;/int,terminal*&gt;&lt;/int,terminal*&gt; typedef unordered_map&lt;sockaddr_in, terminal*,hash_func,=&quot;&quot; cmp_fun=&quot;&quot;&gt; MapClientSession; // sockaddr_in作为key typedef unordered_map&lt;sockaddr_in, terminal*,hash_func,=&quot;&quot; cmp_fun=&quot;&quot;&gt;::iterator MapClientSession_It; // &lt;/sockaddr_in,&gt;&lt;/sockaddr_in,&gt; #endif // CSESSION_Hoperator==有两种方式 一种是 struct st { bool operator==(const st &amp;amp;s) const ... }；另一种就是自定义函数体，代码中 struct cmp_fun { bool operator()(...) ... }必须要自定义operator==和hash_value。 重载operator==是因为，如果两个元素的hash_value的值相同，并不能断定这两个元素就相同，必须再调用operator==。 当然，如果hash_value的值不同，就不需要调用operator==了。 &lt;/string.h&gt;","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"STL","slug":"STL","permalink":"https://lxb.wiki/tags/STL/"}]},{"title":"unordered_set笔记","slug":"unordered_set笔记","date":"2017-01-05T15:14:47.000Z","updated":"2019-10-03T17:03:15.960Z","comments":true,"path":"14decfad/","link":"","permalink":"https://lxb.wiki/14decfad/","excerpt":"","text":"http://www.cplusplus.com/reference/unordered\\_set/unordered\\_set/ unordered_set 模板原型: [cpp] template &amp;lt; class Key, class Hash = hash&lt;key&gt;, class Pred = equal_to&lt;key&gt;, class Alloc = allocator&lt;key&gt; &amp;gt; class unordered_set; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 当比较unordered_set中某两个元素时, 先调用`hash&lt;key&gt;`, 如果`hash&lt;key&gt;` 不相等, 说明两个元素不同, 如果`hash&lt;key&gt;` 值相等, 则调用`equal_to&lt;key&gt;`, 判断两个元素是否完全相等. (Hash函数和Compare函数都可以自定义)&lt;/key&gt;&lt;/key&gt;&lt;/key&gt;&lt;/key&gt; C++ 11中对unordered_set描述大体如下：无序集合容器（unordered_set）是一个存储唯一(unique，即无重复）的关联容器（Associative container），容器中的元素无特别的秩序关系，该容器允许基于值的快速元素检索，同时也支持正向迭代。 在一个unordered_set内部，元素不会按任何顺序排序，而是通过元素值的hash值将元素分组放置到各个槽(Bucker，也可以译为“桶”），这样就能通过元素值快速访问各个对应的元素（均摊耗时为O（1））。 原型中的Key代表要存储的类型，而hash&lt;key&gt;也就是你的hash函数，equal_to&lt;key&gt;用来判断两个元素是否相等，allocator&lt;key&gt;是内存的分配策略。一般情况下，我们只关心hash&lt;key&gt;和equal_to&lt;key&gt;参数，下面将介绍这两部分。&lt;/key&gt;&lt;/key&gt;&lt;/key&gt;&lt;/key&gt;&lt;/key&gt; **`hash&lt;key&gt;`** ` hash&lt;key&gt;`通过相应的hash函数，将传入的参数转换为一个size_t类型值，然后用该值对当前hashtable的bucket取模算得其对应的hash值。而C++标准库，为我们提供了基本数据类型的hash函数：&lt;/key&gt;&lt;/key&gt; \\[cpp\\] /// Primary class template hash. template struct hash;/// Partial specializations for pointer types. template struct hash&lt;\\_Tp*&gt; : public \\_\\_hash\\_base&lt;size\\_t, \\_tp*=&quot;&quot;&gt; &#123; size\\_t operator()(\\_Tp* \\_\\_p) const noexcept &#123; return reinterpret_cast(__p); &#125; &#125;; &lt;/size_t,&gt;// Explicit specializations for integer types.define \\_Cxx\\_hashtable\\_define\\_trivial\\_hash(\\_Tp) \\======================================================template&lt;&gt; \\ struct hash&lt;\\_Tp&gt; : public \\_\\_hash\\_base&lt;size\\_t, \\_tp=&quot;&quot;&gt; \\ &#123; \\ size\\_t \\ operator()(\\_Tp \\_\\_val) const noexcept \\ &#123; return static_cast(__val); &#125; \\ &#125;; &lt;/size_t,&gt;/// Explicit specialization for bool. \\_Cxx\\_hashtable\\_define\\_trivial_hash(bool)/// Explicit specialization for char. \\_Cxx\\_hashtable\\_define\\_trivial_hash(char)/// Explicit specialization for signed char. \\_Cxx\\_hashtable\\_define\\_trivial_hash(signed char)/// Explicit specialization for unsigned char. \\_Cxx\\_hashtable\\_define\\_trivial_hash(unsigned char)/// Explicit specialization for wchar\\_t. \\_Cxx\\_hashtable\\_define\\_trivial\\_hash(wchar_t)/// Explicit specialization for char16\\_t. \\_Cxx\\_hashtable\\_define\\_trivial\\_hash(char16_t)/// Explicit specialization for char32\\_t. \\_Cxx\\_hashtable\\_define\\_trivial\\_hash(char32_t)/// Explicit specialization for short. \\_Cxx\\_hashtable\\_define\\_trivial_hash(short)/// Explicit specialization for int. \\_Cxx\\_hashtable\\_define\\_trivial_hash(int)/// Explicit specialization for long. \\_Cxx\\_hashtable\\_define\\_trivial_hash(long)/// Explicit specialization for long long. \\_Cxx\\_hashtable\\_define\\_trivial_hash(long long)/// Explicit specialization for unsigned short. \\_Cxx\\_hashtable\\_define\\_trivial_hash(unsigned short)/// Explicit specialization for unsigned int. \\_Cxx\\_hashtable\\_define\\_trivial_hash(unsigned int)/// Explicit specialization for unsigned long. \\_Cxx\\_hashtable\\_define\\_trivial_hash(unsigned long)/// Explicit specialization for unsigned long long. \\_Cxx\\_hashtable\\_define\\_trivial_hash(unsigned long long) 对于指针类型，标准库只是单一将地址转换为一个size_t值作为hash值，这里特别需要注意的是`char *`类型的指针，其标准库提供的hash函数只是将指针所指地址转换为一个sieze_t值，如果，你需要用`char *`所指的内容做hash，那么，你需要自己写hash函数或者调用系统提供的`hash&lt;string&gt;`。 标准库为string类型对象提供了一个hash函数，即：Murmur hash，。对于float、double、long double标准库也有相应的hash函数，这里，不做过多的解释，相应的可以参看functional_hash.h头文件。 上述只是介绍了基本数据类型，而在实际应用中，有时，我们需要使用自己写的hash函数，那怎么自定义hash函数？参考标准库基本数据类型的hash函数，我们会发现这些hash函数有个共同的特点：通过定义函数对象，实现相应的hash函数，这也就意味我们可以通过自定义相应的函数对象，来实现自定义hash函数。比如：已知平面上有N，每个点的x轴、y轴范围为[0，100]，现在需要统计有多少个不同点？hash函数设计为：将每个点的x、y值看成是101进制，如下所示:&lt;/string&gt; \\[cpp\\]include&lt;bits\\\\stdc++.h&gt;=======================using namespace std; struct myHash &#123; size\\_t operator()(pair&lt;int, int=&quot;&quot;&gt; \\_\\_val) const &#123; return static_cast(\\_\\_val.first * 101 + \\_\\_val.second); &#125; &#125;; int main() &#123; unordered\\_set&lt;pair&lt;int, int=&quot;&quot;&gt;, myHash&gt; S; int x, y; while (cin &gt;&gt; x &gt;&gt; y) S.insert(make\\_pair(x, y)); for (auto it = S.begin(); it != S.end(); ++it) cout &lt;&lt; it-&gt;first &lt;&lt; &quot; &quot; &lt;&lt; it-&gt;second &lt;&lt; endl; return 0; &#125; **`equal_to&lt;key&gt;`** 该参数用于实现比较两个关键字是否相等，至于为什么需要这个参数？这里做点解释，前面我们说过，当不同关键字，通过hash函数，可能会得到相同的关键字值，每当我们在unordered_set里面做数据插入、删除时，由于unordered_set关键字唯一性，所以我们得确保唯一性。标准库定义了基本类型的比较函数，而对于自定义的数据类型，我们需要自定义比较函数。这里有两种方法:重载==操作符和使用函数对象，下面是STL中实现`equal_to&lt;key&gt;`的源代码：&lt;/key&gt;&lt;/key&gt;&lt;/pair&lt;int,&gt;&lt;/size_t&gt;&lt;/int,&gt;&lt;/bits\\stdc++.h&gt; \\[cpp\\] template struct unary\\_function &#123; /// @c argument\\_type is the type of the argument typedef \\_Arg argument\\_type;/// @c result\\_type is the return type typedef \\_Result result_type; &#125;; template struct equal\\_to : public binary\\_function&lt;\\_Tp, \\_Tp, bool&gt; &#123; bool operator()(const \\_Tp&amp; \\_\\_x, const \\_Tp&amp; \\_\\_y) const &#123; return \\_\\_x == \\_\\_y; &#125; &#125;;扩容与缩容 在vector中，每当我们插入一个新元素时，如果当前的容量（capacity)已不足，需要向系统申请一个更大的空间，然后将原始数据拷贝到新空间中。这种现象在unordered_set中也存在，比如当前的表长为100，而真实存在表中的数据已经大于1000个元素，此时，每个bucker均摊有10个元素，这样就会影响到unordered_set的存取效率，而标准库通过采用某种策略来对当前空间进行扩容，以此来提高存取效率。当然，这里也存在缩容，原理和扩容类似，不过，需要注意的是，每当unordered_set内部进行一次扩容或者缩容，都需要对表中的数据重新计算，也就是说，扩容或者缩容的时间复杂度至少为。 code： [cpp] // unordered_set::find #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;unordered_set&gt; &lt;/unordered_set&gt;&lt;/string&gt;&lt;/iostream&gt; int main () { std::unordered_set&lt;std::string&gt; myset = { &quot;red&quot;,&quot;green&quot;,&quot;blue&quot; }; &lt;/std::string&gt; std::string input; std::cout &amp;lt;&amp;lt; &quot;color? &quot;; getline (std::cin,input); std::unordered_set&lt;std::string&gt;::const_iterator got = myset.find (input); &lt;/std::string&gt; if ( got == myset.end() ) std::cout &amp;lt;&amp;lt; &quot;not found in myset&quot;; else std::cout &amp;lt;&amp;lt; *got &amp;lt;&amp;lt; &quot; is in myset&quot;; std::cout &amp;lt;&amp;lt; std::endl; return 0; }","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"STL","slug":"STL","permalink":"https://lxb.wiki/tags/STL/"}]},{"title":"c++代码优化建议","slug":"c++代码优化建议","date":"2016-12-17T14:02:36.000Z","updated":"2019-10-03T16:40:08.700Z","comments":true,"path":"8e72ff9a/","link":"","permalink":"https://lxb.wiki/8e72ff9a/","excerpt":"","text":"记住阿姆达尔定律： Ahmdal’s rule $$Speedup =\\dfrac{ time_{old}}{time_{new}} = \\dfrac{1}{(1-func_{cost})+func_{cost}/func_{speedup}}$$ func_cost是函数func运行时间百分比，func_speedup是你优化函数的运行的系数。 所以，如果你优化了函数TriangleIntersect执行40%的运行时间，使它运行快了近两倍，而你的程序会运行快25%。 这意味着不经常使用的代码不需要做较多优化考虑（或者完全不优化）。 这里有句俗语：让经常执行的路径运行更加高效，而运行稀少的路径正确运行。 代码先保证正确，然后再考虑优化 这并不意味着用8周时间写一个全功能的射线追踪算法，然后用8周时间去优化它。 分多步来做性能优化。 先写正确的代码，当你意识到这个函数可能会被经常调用，进行明显的优化。 然后再寻找算法的瓶颈，并解决（通过优化或者改进算法）。通常，改进算法能显著地改进瓶颈——也许是采用一个你还没有预想到的方法。所有频繁调用的函数，都需要优化。 我所了解的那些写出非常高效代码的人说，他们优化代码的时间，是写代码时间的两倍。 跳转和分支执行代价高，如果可能，尽量少用。 函数调用需要两次跳转，外加栈内存操作。 优先使用迭代而不是递归。 使用内联函数处理短小的函数来消除函数调用开销。 将循环内的函数调用移动到循环外(例如，将for(i=0;i&lt;100;i++) DoSomething();改为DoSomething() { for(i=0;i&lt;100;i++) { … }})。 if…else if…else if…else if…很长的分支链执行到最后的分支需要很多的跳转。如果可能，将其转换为一个switch声明语句，编译器有时候会将其转换为一个表查询单次跳转。如果switch声明不可行，将最常见的场景放在if分支链的最前面。 5. 仔细思考函数下标的顺序。 两阶或更高阶的数组在内存中还是以一维的方式在存储在内存中，这意味着（对于C/C++数组）array[i][j] 和 array[i][j+1]是相邻的，但是array[i][j] 和array[i+1][j]可能相距很远。以适当的方式访问存储实际内存中的数据，可以显著地提升你代码的执行效率（有时候可以提升一个数量级甚至更多）。 现代处理器从主内存中加载数据到处理器cache，会加载比单个值更多的数据。该操作会获取请求数据和相邻数据（一个cache行大小）的整块数据。这意味着，一旦array[i][j]已经在处理器cache中，array[i][j+1]很大可能也已经在cache中了，而array[i+1][j]可能还在内存中。 6. 使用指令层的并行机制 尽管许多程序还是依赖单线程的执行，现代处理器在单核中也提供了不少的并行性。例如：单个CPU可以同时执行4个浮点数乘，等待4个内存请求并执行一个分支预判。为了最大化利用这种并行性，代码块（在跳转之间的）需要足够的独立指令来允许处理器被充分利用。 考虑展开循环来改进这一点。 这也是使用内联函数的一个好理由。 7. 避免或减少使用本地变量。 本地变量通常都存储在栈上。不过如果数量比较少，它们可以存储在CPU寄存器中。在这种情况下，函数不但得到了更快访问存储在寄存器中的数据的好处，也避免了初始化一个栈帧的开销。不要将大量数据转换为全局变量。 8. 减少函数参数的个数。 和减少使用本地变量的理由一样——它们也是存放在栈上。9. 通过引用传递结构体而不是传值 我在射线追踪中还找不到一个场景需要将结构体使用传值方式（包括一些简单结构如：Vector，Point和Color）。10. 如果你的函数不需要返回值，不要定义。 尽量避免数据转换。 整数和浮点数指令通常操作不同的寄存器，所以转换需要进行一次拷贝操作。 短整型（char和short）仍然使用一整个寄存器，并且它们需要被填充为32/64位，然后在存储回内存时需要再次转换为小字节（不过，这个开销一定比一个更大的数据类型的内存开销要多一点）。 定义C++对象时需要注意。 使用类初始化而不是使用赋值（Color c(black); 比Color c; c = black;更快） 使类构造函数尽可能轻量。 尤其是常用的简单类型（比如，color，vector，point等等），这些类经常被复制。 这些默认构造函数通常都是在隐式执行的，这或许不是你所期望的。 使用类初始化列表(Use Color::Color() : r(0), g(0), b(0) {}，而不是初始化函数Color::Color() { r= g = b = 0; } .) 如果可以的话，使用位移操作&gt;&gt;和&lt;&lt;来代替整数乘除法 小心使用表查找函数 许多人都鼓励将复杂的函数（比如：三角函数）转化为使用预编译的查找表。对于射线追踪功能来说，这通常导致了不必要的内存查找，这很昂贵（并不断增长），并且这和计算一个三角函数并从内存中获取值一样快（尤其你考虑到三角查找打乱了cpu的cache存取）。 在其他情况下，查找表会很有用。对于GPU编程通常优先使用表查找而不是复杂函数。 对大多数类，优先使用+= 、 -= 、 *= 和 /=，而不是使用+ 、 – 、 * 、 和?/ 这些简单操作需要创建一个匿名临时中间变量。 例如：Vector v = Vector(1,0,0) + Vector(0,1,0) + Vector(0,0,1);?创建了五个匿名临时Vector: Vector(1,0,0), Vector(0,1,0), Vector(0,0,1), Vector(1,0,0) + Vector(0,1,0), 和 Vector(1,0,0) + Vector(0,1,0) + Vector(0,0,1). 对上述代码进行简单转换：Vector v(1,0,0); v+= Vector(0,1,0); v+= Vector(0,0,1);仅仅创建了两个临时Vector: Vector(0,1,0) 和 Vector(0,0,1)。这节约了6次函数调用（3次构造函数和3次析构函数）。 对于基本数据类型，优先使用+?、?-?、??、?和?/，而不是+=?、?-=?、?= 和 /= 推迟定义本地变量 定义一个对象变量通常需要调用一次函数（构造函数）。 如果一个变量只在某些情况下需要（例如在一个if声明语句内），仅在其需要的时候定义，这样，构造函数仅在其被使用的时候调用。 对于对象，使用前缀操作符（++obj），而不是后缀操作符（obj++） 这在你的射线追踪算法中可能不是一个问题 使用后缀操作符需要执行一次对象拷贝（这也导致了额外的构造和析构函数调用），而前缀的构造函数不需要一个临时的拷贝。 小心使用模板 对不同的是实例实现进行不同的优化。 标准模板库已经经过良好的优化，不过我建议你在实现一个交互式射线追踪算法时避免使用它。 使用自己的实现，你知道它如何使用算法，所以你知道如何最有效的实现它。 最重要的是，我的经历告诉我：调试STL库非常低效。通常这也不是一个问题，除非你使用debug版本做性能分析。你会发现STL的构造函数，迭代器和其他一些操作，占用了你15%的运行时间，这会导致你分析性能输出更加费劲。 避免在计算时进行动态内存分配 动态内存对于存储场景和运行期间其他数据都很有用。 但是，在许多（大多数）的系统动态内存分配需要获取控制访问分配器的锁。对于多线程应用程序，现实中使用动态内存由于额外的处理器导致了性能下降，因为需要等待分配器锁和释放内存。 即便对于单线程应用，在堆上分配内存也比在栈上分配内存开销大得多。操作系统还需要执行一些操作来计算并找到适合尺寸的内存块。 找到你系统内存cache的信息并利用它们 如果一个是数据结构正好适合一个cache行，处理整个类从内存中只需要做一次获取操作。 确保所有的数据结构都是cache行大小对齐（如果你的数据结构和一个cache行大小都是128字节，仍有可能因为你的结构体中的一个字节在一个cache行中，而其他127字节在另外一个cahce行中）。 避免不需要的数据初始化 如果你需要初始化一大段的内存，考虑使用memset。 尽早结束循环和尽早返回函数调用 考虑一个射线和三角形交叉，通常的情况是射线会越过三角，所以这里可以优化。 如果你决定将射线和三角面板交叉。如果射线和面板交叉t值是负数，你可以立即返回。这允许你跳过射线三角交叉一大半的质心坐标计算。这是一个大的节约，一旦你知道这个交叉不存在，你就应该立即返回交叉计算函数。 同样的，一些循环也应该尽早结束。例如，当设置阴影射线，对于近处的交叉通常都是不必须的，一旦有类似的的交叉，交叉计算就应该尽早返回。（这里的交叉含义不太明白，可能是专业词汇，译者注） 在稿纸上简化你的方程式 许多方程式中，通常都可以或者在某些条件中取消计算。 编译器不能发现这些简化，但是你可以。取消一个内部循环的一些昂贵操作可以抵消你在其他地方的好几天的优化工作。 整数、定点数、32位浮点数和64位双精度数字的数学运算差异，没有你想象的那么大 在现代CPU，浮点数运算和整数运算差不多拥有同样的效率。在计算密集型应用（比如射线追踪），这意味这可以忽略整数和浮点数计算的开销差异。这也就是说，你不必要对算数进行整数处理优化。 双精度浮点数运算也不比单精度浮点数运算更慢，尤其是在64位机器上。我在同一台机器测试射线追踪算法全部使用double比全部使用floats运行有时候更快，反过来测试也看到了一样的现象（这里的原文是：I have seen ray tracers run faster using all doubles than all floats on the same machine. I have also seen the reverse）。 不断改进你的数学计算，以消除昂贵的操作 sqrt()经常可以被优化掉，尤其是在比较两个值的平方根是否一致时。 如果你重复地需要处理 除x 操作，考虑计算1/x的值，乘以它。这在向量规范化（3次除法）运算中赢得了大的改进，不过我最近发现也有点难以确定的。不过，这仍然有所改进，如果你要进行三次或更多除法运算。 如果你在执行一个循环，那些在循环中执行不发生变化的部分，确保提取到循环外部。 考虑看看你的计算值是否可以在循环中修改得到（而不每次都重新开始循环计算）。","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[]},{"title":"x == x","slug":"x-x","date":"2016-11-19T12:40:58.000Z","updated":"2020-05-24T10:27:47.715Z","comments":true,"path":"72da8a18/","link":"","permalink":"https://lxb.wiki/72da8a18/","excerpt":"","text":"C的表达式 x == x，何时为假呢？即下面的代码： if (x == x) { printf(&quot;Equal\\n&quot;); } else { printf(&quot;Not equal\\n&quot;); }什么时候输出为”Not equal”呢？ #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;string.h&gt;&lt;/string.h&gt;&lt;/stdio.h&gt;&lt;/stdlib.h&gt; int main(void) { float x = 0xffffffff; if (x == x) { printf(&quot;Equal\\n&quot;); } else { printf(&quot;Not equal\\n&quot;); } if (x &amp;gt;= 0) { printf(&quot;x(%f) &amp;gt;= 0\\n&quot;, x); } else if (x &amp;lt; 0) { printf(&quot;x(%f) &amp;lt; 0\\n&quot;, x); } int a = 0xffffffff; memcpy(&amp;amp;x, &amp;amp;a, sizeof(x)); if (x == x) { printf(&quot;Equal\\n&quot;); } else { printf(&quot;Not equal\\n&quot;); } if (x &amp;gt;= 0) { printf(&quot;x(%f) &amp;gt;= 0\\n&quot;, x); } else if (x &amp;lt; 0) { printf(&quot;x(%f) &amp;lt; 0\\n&quot;, x); } else { printf(&quot;Surprise x(%f)!!!\\n&quot;, x); } return 0; }编译gcc -g -Wall test.c，看执行结果： $ ./a.out Equal x(4294967296.000000) &amp;gt;= 0 Not equal Surprise x(-nan)!!!最后两行输出是不是有点surprise啊。 下面先简单解释一下： 1. 当float x = 0xffffffff：这时将整数赋给一个浮点数，由于float和int的size都是4，而浮点数的存储格式与整数不同，其需要将某些位作为小数位，所以float的范围要小于int的范围。因此这里涉及到了整数转换浮点的规定。因为这个转换其实很少用到，我也就不查了。但是总之，这个转换是合法的。但是最终的值很可能不是你想要的结果——尤其是当浮点的范围小于整数的范围时。 2. 即使整数转换成浮点，数值再不是期望值，但它也一定是一个合法的浮点数值。所以第一个x == x，一定为true，且x不是大于0，就是小于0。这时x存的并不是0xffffffff。 3. 当使用memcpy将0xff填充到x的地址时，这时x存的保证为0xffffffff。但是这个不是一个合法的float的值。因此奇怪的现象发生了，x并不等于x。原因则是与cpu的浮点指令相关. 4. 作为一个非法的float值，当它与其它任何数值比较时，都会返回false。这也就造成了，后面惊奇的结果，x既不大于等于0，也不小于0。 总结一下：一般来说，浮点类型很少被使用，也不应该在程序中鼓励使用。不仅其效率比整数低，且由于浮点类型特殊的存储格式，很容易造成一些意想不到的错误。如果真的无法避免时，一定要小心小心再小心。特别要注意今天的主题，这种非法的浮点值，会导致任何比较判断都失败。而判断这种浮点值的方法也很简单，如果x != x，那么该浮点即为非法浮点值。","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[]},{"title":"C++在重载operator=为带模板的函数的时候的陷阱","slug":"C++在重载operator=为带模板的函数的时候的陷阱","date":"2016-10-29T12:20:32.000Z","updated":"2019-10-03T07:57:15.275Z","comments":true,"path":"a25c4e07/","link":"","permalink":"https://lxb.wiki/a25c4e07/","excerpt":"","text":"原文地址 https://segmentfault.com/a/1190000004467381 最近被一个语法问题缠了半天，终于找到了原因。不仔细思考一下写的时候真的很容易忽略。先看代码： template class A { public: const T t = 0; template A&amp; operator=(const A&amp; a) { return *this; } }; int main() { A a, b; b = a; // error } 这会带来一个编译错误，然而横睇掂睇都看不出问题。于是我就试了一下这样的代码：A c; b = c;居然通过了编译。F**k，这个模板居然胳膊肘往外拐。 其实我在写这个代码的时候忽略了一点，就是default assignment operator，它是你在定义类的时候编译器默认给你加上去的，行为是对所有成员变量赋值。它的声明是A&amp; operator=(const A&amp; a);，跟我们自己定义的放在一起： template A&amp; operator=(const A&amp; a) { return *this; } A&amp; operator=(const A&amp; a) /= delete/; 恰好构成了模板特化，这就糟了。一旦构成了特化，OtherT可以匹配的类型就会除去int，用A赋值时只能调用系统给我们定义的那个。然而它也不起作用，因为成员里面有常量（这样它就会被标记为= delete，留意delete并不会令OtherT可以匹配到int，反而令它匹配不到）。 知道了原因之后，解决就很方便了，只要重新定义这个默认赋值运算符就好： A&amp; operator=(const A&amp; a) { /…/ }","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[]},{"title":"Makefile学习笔记","slug":"Makefile学习笔记","date":"2016-10-09T14:21:25.000Z","updated":"2019-10-03T16:55:46.394Z","comments":true,"path":"31dc5dc1/","link":"","permalink":"https://lxb.wiki/31dc5dc1/","excerpt":"","text":"关于Makefile怎么写,参考http://blog.csdn.net/haoel/article/details/2886 一 关于编译和链接 一般来说，无论是C、C++、还是pas，首先要把源文件编译成中间代码文件，在Windows下是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做编译（compile）。然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常是你需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。 链接时，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。 总结一下，源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。你需要指定函数的Object File.二 Makefile的规则 三条: 1）如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。 2）如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。 3）如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。 target ... : prerequisites ... command ... ... target也就是一个目标文件，可以是Object File，也可以是执行文件,还可以是一个标签（Label）. prerequisites就是，要生成那个target所需要的东西(文件或是目标)。 command也就是make需要执行的命令。（任意的Shell命令） 这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在command中。说白一点就是说，prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。这就是Makefile的规则。也就是Makefile中最核心的内容。For example: edit : main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o main.o : main.c defs.h cc -c main.c kbd.o : kbd.c defs.h command.h cc -c kbd.c command.o : command.c defs.h command.h cc -c command.c display.o : display.c defs.h buffer.h cc -c display.c insert.o : insert.c defs.h buffer.h cc -c insert.c search.o : search.c defs.h buffer.h cc -c search.c files.o : files.c defs.h buffer.h command.h cc -c files.c utils.o : utils.c defs.h cc -c utils.c clean : rm edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o 在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个\\[Tab\\]键作为开头(在Makefile中的命令，必须要以\\[Tab\\]键开始)。make并不管命令是怎么工作的，它只管执行所定义的命令。make会比较targets文件和prerequisites文件的修改日期，如果prerequisites文件的日期要比targets文件的日期要新，或者target不存在的话，make就会执行后续定义的命令。 clean不是一个文件，它只不过是一个动作名字，有点像C语言中的lable一样，如果其冒号后什么也没有，那么make就不会自动去找文件的依赖性，也就不会自动执行其后所定义的命令。要执行其后的命令，就要在make命令后明显得指出这个lable的名字。这样的方法非常有用，我们可以在一个makefile中定义不用的编译或是和编译无关的命令，比如程序的打包，程序的备份，等等。三 Makefile里有什么 1 显示规则 2 隐晦规则 3 变量的定义 4 文件指示: 其包括了三个部分，一个是在一个Makefile中引用另一个Makefile，就像C语言中的include一样；另一个是指根据某些情况指定Makefile中的有效部分，就像C语言中的预编译#if一样；还有就是定义一个多行的命令。 5 注释: Makefile中只有行注释，和UNIX的Shell脚本一样，其注释是用“#”字符，这个就像C/C++中的“//”一样。如果你要在你的Makefile中使用“#”字符，可以用反斜框进行转义，如：“#”。 四 Makefile的文件名 默认情况下，make命令会在当前目录下按顺序找寻文件名为“GNUmakefile”、“makefile”、“Makefile”的文件，找到了就解释这个文件。在这三个文件名中，最好使用“Makefile”这个文件名，因为这个文件名第一个字符为大写，这样有一种显目的感觉。最好不要用“GNUmakefile”，这个文件是GNU的make识别的。有另外一些make只对全小写的“makefile”文件名敏感，但是基本上来说，大多数的make都支持“makefile”和“Makefile”这两种默认文件名。当然，也可以使用别的文件名来书写Makefile，比如：“Make.Linux”，“Make.Solaris”，“Make.AIX”等，如果要指定特定的Makefile，可以使用make的“-f”和“–file”参数，如：make -f Make.Linux或make –file Make.AIX。 五 引用其它的Makefile include的语法是： includefilename可以是当前操作系统Shell的文件模式（可以保含路径和通配符） 在include前面可以有一些空字符，但是绝不能是\\[Tab\\]键开始。include和可以用一个或多个空格隔开。例如,有这样几个Makefile：a.mk、b.mk、c.mk，还有一个文件叫foo.make，以及一个变量$(bar)，其包含了e.mk和f.mk，那么，下面的语句： include foo.make *.mk $(bar) 等价于： include foo.make a.mk b.mk c.mk e.mk f.mkmake命令开始时，会把找寻include所指出的其它Makefile，并把其内容安置在当前的位置。就好像C/C++的#include指令一样。如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，如果当前目录下没有找到，那么，make还会在下面的几个目录下找： 1、如果make执行时，有“-I”或“--include-dir”参数，那么make就会在这个参数所指定的目录下去寻找。 2、如果目录/include（一般是：/usr/local/bin或/usr/include）存在的话，make也会去找。如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成makefile的读取，make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。如果你想让make不理那些无法读取的文件，而继续执行，你可以在include前加一个减号“-”。如： -include 其表示，无论include过程中出现什么错误，都不要报错继续执行。和其它版本make兼容的相关命令是sinclude，其作用和这一个是一样的。六 环境变量 MAKEFILES 如果当前环境中定义了环境变量MAKEFILES，那么，make会把这个变量中的值做一个类似于include的动作。这个变量中的值是其它的Makefile，用空格分隔。只是，它和include不同的是，从这个环境变量中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。 但是在这里还是建议不要使用这个环境变量，因为只要这个变量一被定义，那么当使用make时，所有的Makefile都会受到它的影响，这绝不是想看到的。在这里提这个事，只是为了告诉大家，也许有时候你的Makefile出现了怪事，那么你可以看看当前环境中有没有定义这个变量。 当make嵌套调用时（参见前面的“嵌套调用”章节），上层Makefile中定义的变量会以系统环境变量的方式传递到下层的Makefile中。当然，默认情况下，只有通过命令行设置的变量会被传递。而定义在文件中的变量，如果要向下层Makefile传递，则需要使用exprot关键字来声明.七 关于命令 通常，make会把其要执行的命令行在命令执行前输出到屏幕上。当我们用“@”字符在命令行前，那么，这个命令将不被make显示出来，最具代表性的例子是，我们用这个功能来像屏幕显示一些信息。如： @echo 正在编译XXX模块......当make执行时，会输出“正在编译XXX模块……”字串，但不会输出命令，如果没有“@”，那么，make将输出： echo 正在编译XXX模块...... 正在编译XXX模块......如果make执行时，带入make参数“-n”或“–just-print”，那么其只是显示命令，但不会执行命令，这个功能很有利于调试Makefile，看看书写的命令执行起来是什么样子的或是什么顺序的,而make参数“-s”或“–slient”则是全面禁止命令的显示。 如果要让上一条命令的结果应用在下一条命令时，应该使用分号分隔这两条命令。比如第一条命令是cd，希望第二条命令在cd之后的基础上运行，那么就不能把这两条命令写在两行上，而应该把这两条命令写在一行上，用分号分隔。如： 示例一： exec: cd /home/hchen pwd 示例二： exec: cd /home/hchen; pwd当执行“make exec”时，第一个例子中的cd没有作用，pwd会打印出当前的Makefile目录，而第二个例子中，cd就起作用了，pwd会打印出“/home/hchen”。 每当命令运行完后，make会检测每个命令的返回码，如果命令返回成功，那么make会执行下一条命令，当规则中所有的命令成功返回后，这个规则就算是成功完成了。如果一个规则中的某个命令出错了（命令退出码非零），那么make就会终止执行当前规则，这将有可能终止所有规则的执行。 有些时候，命令的出错并不表示就是错误的。例如mkdir，如果目录不存在，那么mkdir就成功执行，万事大吉，如果目录存在，那么就出错了。之所以使用mkdir的意思就是一定要有这样的一个目录，只要这个目录存在了,就不希望mkdir出错而终止规则的运行。为了做到这一点，忽略命令的出错，可以在Makefile的命令行前加一个减号“-”（在Tab键之后），标记为不管命令出不出错都认为是成功的。如： clean: -rm -f *.o 还有一个全局的办法是，给make加上“-i”或是“–ignore-errors”参数，那么，Makefile中所有命令都会忽略错误。而如果一个规则是以“.IGNORE”作为目标的，那么这个规则中的所有命令将会忽略错误。还有一个要提一下的make的参数的是“-k”或是“–keep-going”，这个参数的意思是，如果某规则中的命令出错了，那么就终止该规则的执行，但继续执行其它规则。 在一些大的工程中，会把不同模块或是不同功能的源文件放在不同的目录中，这种情况可以在每个目录中都书写一个该目录的Makefile，例如，有一个子目录叫subdir，这个目录下有个Makefile文件，来指明了这个目录下文件的编译规则。那么总控的Makefile可以这样书写： subsystem: cd subdir &amp;&amp; $(MAKE)其等价于： subsystem: $(MAKE) -C subdir这两个例子的意思都是先进入“subdir”目录，然后执行make命令。总控Makefile的变量可以传递到下级的Makefile中，但是不会覆盖下层的Makefile中所定义的变量，除非指定了“-e”参数。 如果要传递变量到下级Makefile中，那么可以使用这样的声明： export如果不想让某些变量传递到下级Makefile中，那么可以这样声明： unexport如： 示例一： export variable = value 其等价于： variable = value export variable 其等价于： export variable := value 其等价于： variable := value export variable 示例二： export variable += value 其等价于： variable += value export variable如果要传递所有的变量，那么，只要一个export就行了,后面什么也不用跟，表示传递所有的变量。 八 变量 两种高级用法: 我们可以替换变量中的共有的部分，其格式是“$(var:a=b)”或是“${var:a=b}”，其意思是，把变量“var”中所有以“a”字串“结尾”的“a”替换成“b”字串。这里的“结尾”意思是“空格”或是“结束符”。示例： foo := a.o b.o c.o bar := $(foo:.o=.c)这个示例中，我们先定义了一个“$(foo)”变量，而第二行的意思是把“$(foo)”中所有的“.o”字串“结尾”全部替换成“.c”，所以“$(bar)”的值就是“a.c b.c c.c”。 第二种高级用法是——“把变量的值再当成变量”。先看一个例子： x = y y = z a := $($(x))在这个例子中，$(x)的值是“y”，所以$($(x))就是$(y)，于是$(a)的值就是“z”。（注意，是“x=y”，而不是“x=$(y)”） 我们还可以使用更多的层次： x = y y = z z = u a := $($($(x)))这里的$(a)的值是“u”. 还有一种设置变量值的方法是使用define关键字。使用define关键字设置变量的值可以有换行，这有利于定义一系列的命令.定义是以endef关键字结束,其工作方式和“=”操作符一样。变量的值可以包含函数、命令、文字，或是其它变量。命令需要以\\[Tab\\]键开头，define定义的命令也不例外.下面的这个示例展示了define的用法： define two-lines echo foo echo $(bar) endef 九 目标变量 前面所有的在Makefile中定义的变量都是“全局变量”，在整个文件，都可以访问这些变量。当然，“自动化变量”除外，如“$&lt;”等这种类量的自动化变量就属于“规则型变量”，这种变量的值依赖于规则的目标和依赖目标的定义。 当然，我们同样可以为某个目标设置局部变量，这种变量被称为“Target-specific Variable”，它可以和“全局变量”同名，因为它的作用范围只在这条规则以及连带规则中，所以其值也只在作用范围内有效,而不会影响规则链以外的全局变量的值。其语法是： : : overide可以是各种赋值表达式，如“=”、“:=”、“+=”或是“？=”。第二个语法是针对于make命令行带入的变量，或是系统环境变量。 这个特性非常的有用，当我们设置了这样一个变量，这个变量会作用到由这个目标所引发的所有的规则中去。如： prog : CFLAGS = -g prog : prog.o foo.o bar.o $(CC) $(CFLAGS) prog.o foo.o bar.o prog.o : prog.c $(CC) $(CFLAGS) prog.c foo.o : foo.c $(CC) $(CFLAGS) foo.c bar.o : bar.c $(CC) $(CFLAGS) bar.c在这个示例中，不管全局的$(CFLAGS)的值是什么，在prog目标，以及其所引发的所有规则中（prog.o foo.o bar.o的规则），$(CFLAGS)的值都是“-g”. 十 条件判断语法 条件表达式的语法为： else endif其中表示条件关键字，如“ifeq”。这个关键字有四个。 第一个是我们前面所见过的“ifeq” ifeq (, ) ifeq &apos;&apos; &apos;&apos; ifeq &quot;&quot; &quot;&quot; ifeq &quot;&quot; &apos;&apos; ifeq &apos;&apos; &quot;&quot;比较参数“arg1”和“arg2”的值是否相同。当然，参数中我们还可以使用make的函数。如： ifeq ($(strip $(foo)),) endif这个示例中使用了“strip”函数，如果这个函数的返回值是空（Empty），那么就生效。 第二个条件关键字是“ifneq”。 第三个条件关键字是“ifdef”。语法是： ifdef如果变量的值非空，那到表达式为真。否则，表达式为假。当然，同样可以是一个函数的返回值。注意，ifdef只是测试一个变量是否有值，其并不会把变量扩展到当前位置。还是来看两个例子： 示例一： bar = foo = $(bar) ifdef foo frobozz = yes else frobozz = no endif 示例二： foo = ifdef foo frobozz = yes else frobozz = no endif第一个例子中，“$(frobozz)”值是“yes”，第二个则是“no”。 第四个条件关键字是“ifndef”。 在这一行上，多余的空格是被允许的，但是不能以\\[Tab\\]键做为开始（不然就被认为是命令）。而注释符“#”同样也是安全的。“else”和“endif”也一样，只要不是以\\[Tab\\]键开始就行了。 特别注意的是，make是在读取Makefile时就计算条件表达式的值，并根据条件表达式的值来选择语句，所以，最好不要把自动化变量（如“$@”等）放入条件表达式中，因为自动化变量是在运行时才有的。十一 foreach 函数 foreach函数和别的函数非常的不一样。因为这个函数是用来做循环用的，它的语法是： $(foreach , &lt;list&gt;,&lt;text&gt;)这个函数的意思是，把参数 &lt;list&gt;中的单词逐一取出放到参数&lt;var&gt;所指定的变量中，然后再执行&lt;text&gt;所包含的表达式。每一次&lt;text&gt;会返回一个字符串，循环过程中，&lt;text&gt;的所返回的每个字符串会以空格分隔，最后当整个循环结束时，&lt;text&gt;所返回的每个字符串所组成的整个字符串（以空格分隔）将会是foreach函数的返回值。&amp;nbsp; &amp;nbsp;所以，&lt;var&gt;最好是一个变量名，&lt;/var&gt; &lt;list&gt;可以是一个表达式，而&lt;text&gt;中一般会使用&lt;var&gt;这个参数来依次枚举 &lt;list&gt;中的单词。举个例子：&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;names := a b c d files := $(foreach n,$(names),$(n).o) $(name)中的单词会被挨个取出，并存到变量“n”中，“$(n).o”每次根据“$(n)”计算出一个值，这些值以空格分隔，最后作为foreach函数的返回，所以，$(files)的值是“a.o b.o c.o d.o”。 十二 检查规则 有时候，我们不想让我们的makefile中的规则执行起来，只想检查一下命令，或是执行的序列。可以使用make命令的下述参数： “-n” “--just-print” “--dry-run”------------------ “-t” “--touch” 这个参数的意思就是把目标文件的时间更新，但不更改目标文件。也就是说，make假装编译目标，但不是真正的编译目标，只是把目标变成已编译过的状态。 “-W ” 这个参数需要指定一个文件。一般是是源文件（或依赖文件），make会根据规则推导来运行依赖于这个文件的命令，一般来说，可以和“-n”参数一同使用，来查看这个依赖文件所发生的规则命令。假定目标需要更新，如果和“-n”选项使用，那么这个参数会输出该目标更新时的运行动作。十三 make的其他参数 “-B” “–always-make” 认为所有的目标都需要更新（重编译）。 “-C ” “–directory= ” 指定读取makefile的目录。如果有多个“-C”参数，make的解释是后面的路径以前面的作为相对路径，并以最后的目录作为被指定目录。如：“make –C ~hchen/test –C prog”等价于“make –C ~hchen/test/prog”。 “-i” “–ignore-errors” 在执行时忽略所有的错误。 “-k” “–keep-going” 出错也不停止运行。如果生成一个目标失败了，那么依赖于其上的目标就不会被执行了。 十四 隐含规则 在使用Makefile时，有一些会经常使用，而且使用频率非常高的东西，比如，编译C/C++的源程序为中间目标文件（Unix下是\\[.o\\]文件，Windows下是\\[.obj\\]文件）,这些就是早先约定了的，不需要我们再写出来的规则。 “隐含规则”也就是一种惯例，make会按照这种“惯例”心照不喧地来运行，即使Makefile中没有书写这样的规则。例如，把\\[.c\\]文件编译成\\[.o\\]文件这一规则，根本就不用写出来，make会自动推导出这种规则，并生成需要的\\[.o\\]文件。 “隐含规则”会使用一些系统变量，我们可以改变这些系统变量的值来定制隐含规则的运行时的参数。如系统变量“CFLAGS”可以控制编译时的编译器参数。1 使用隐含规则 如果要使用隐含规则生成你需要的目标，你所需要做的就是不要写出这个目标的规则。那么，make会试图去自动推导产生这个目标的规则和命令，如果make可以自动推导生成这个目标的规则和命令，那么这个行为就是隐含规则的自动推导。当然，隐含规则是make事先约定好的一些东西。例如，我们有下面的一个Makefile： foo : foo.o bar.o cc –o foo foo.o bar.o $(CFLAGS) $(LDFLAGS)这个Makefile中并没有写下如何生成foo.o和bar.o这两目标的规则和命令。因为make的“隐含规则”功能会自动为我们自动去推导这两个目标的依赖目标和生成命令。 make会在自己的“隐含规则”库中寻找可以用的规则，如果找到，那么就会使用。如果找不到，那么就会报错。在上面的那个例子中，make调用的隐含规则是，把\\[.o\\]的目标的依赖文件置成\\[.c\\]，并使用C的编译命令“cc –c $(CFLAGS) \\[.c\\]”来生成\\[.o\\]的目标。也就是说，我们完全没有必要写下下面的两条规则： foo.o : foo.c cc –c foo.c $(CFLAGS) bar.o : bar.c cc –c bar.c $(CFLAGS)因为，这已经是“约定”好了的事了，make和我们约定好了用C编译器“cc”生成[.o]文件的规则，这就是隐含规则。 当然，如果我们为\\[.o\\]文件书写了自己的规则，那么make就不会自动推导并调用隐含规则，它会按照我们写好的规则忠实地执行。 还有，在make的“隐含规则库”中，每一条隐含规则都在库中有其顺序，越靠前的则是越被经常使用的，所以，这会导致我们有些时候即使我们显示地指定了目标依赖，make也不会管。如下面这条规则（没有命令）： foo.o : foo.p依赖文件“foo.p”（Pascal程序的源文件）有可能变得没有意义。如果目录下存在了“foo.c”文件，那么我们的隐含规则一样会生效，并会通过“foo.c”调用C的编译器生成foo.o文件。因为，在隐含规则中，Pascal的规则出现在C的规则之后，所以，make找到可以生成foo.o的C的规则就不再寻找下一条规则了。如果你确实不希望任何隐含规则推导，那么，你就不要只写出“依赖规则”，而不写命令。当然，我们也可以使用make的参数“-r”或“–no-builtin-rules”选项来取消所有的预设置的隐含规则。 1 编译C程序的隐含规则 “.o”的目标的依赖目标会自动推导为“.c”，并且其生成命令是“$(CC) –c $(CPPFLAGS) $(CFLAGS)” 2 编译C++程序的隐含规则 “.o”的目标的依赖目标会自动推导为“.cc”或是“.C”，并且其生成命令是“$(CXX) –c $(CPPFLAGS) $(CFLAGS)”。（建议使用“.cc”作为C++源文件的后缀，而不是“.C”） 3 链接Object文件的隐含规则 “”目标依赖于“.o”，通过运行C的编译器来运行链接程序生成（一般是“ld”），其生成命令是：“$(CC) $(LDFLAGS) .o $(LOADLIBES) $(LDLIBS)”。这个规则对于只有一个源文件的工程有效，同时也对多个Object文件（由不同的源文件生成）的也有效。例如如下规则： x : y.o z.o并且“x.c”、“y.c”和“z.c”都存在时，隐含规则将执行如下命令： cc -c x.c -o x.o cc -c y.c -o y.o cc -c z.c -o z.o cc x.o y.o z.o -o x rm -f x.o rm -f y.o rm -f z.o如果没有一个源文件（如上例中的x.c）和你的目标名字（如上例中的x）相关联，那么，你最好写出自己的生成规则，不然，隐含规则会报错的。","categories":[{"name":"C++","slug":"C","permalink":"https://lxb.wiki/categories/C/"}],"tags":[{"name":"makefile","slug":"makefile","permalink":"https://lxb.wiki/tags/makefile/"}]}]}